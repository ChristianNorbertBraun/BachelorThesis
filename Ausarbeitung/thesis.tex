\documentclass[12pt,oneside,a4paper,parskip]{scrbook}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[ngerman]{babel}
\usepackage{floatflt} 
\usepackage{subfigure}
\usepackage[pdftex]{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{color}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{nicefrac}
\usepackage{pdfpages}
\usepackage{float} 
\usepackage{pdflscape}
\usepackage{subfigure}
\usepackage{pdfpages}  
\usepackage[verbose]{placeins} 
\usepackage[nouppercase,headsepline,plainfootsepline]{scrpage2}
\usepackage{listings}		
\usepackage{xcolor}			
\usepackage{color}			
\usepackage{caption}		
\usepackage{subfigure}			
\usepackage{epstopdf}		
\usepackage{longtable}  
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\usetikzlibrary{pgfplots.dateplot}
\usepackage{pgfplotstable}
\usepackage{filecontents}
\usepackage{setspace}
\usepackage[nolist]{acronym}
\usepackage{booktabs}
\usepackage[style=numeric]{biblatex}
%\bibliography{literatur2}
\addbibresource{literatur.bib}

%%%%%%%%%%%%%%%%%%%
%% definitions
%%%%%%%%%%%%%%%%%%%
\def\BaAuthor{Christian Norbert Braun}
\def\BaTitle{Einsatz eines Distributed File Systems zur Skalierung eines Banking-Buchungssystems}
\def\BaSupervisorOne{Prof.\ Dr.\ Steffen Heinzl}
\def\BaSupervisorTwo{Prof.\ Dr.\ Peter Braun}
\def\BaDeadline{31.03.2017}

\hypersetup{
pdfauthor={\BaAuthor},
pdftitle={\BaTitle},
pdfsubject={Subject},
pdfkeywords={Keywords}
}

%%%%%%%%%%%%%%%%%%%
%% configs to include
%%%%%%%%%%%%%%%%%%%
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\definecolor{pblue}{rgb}{0.13,0.13,1}
\definecolor{pgreen}{rgb}{0,0.5,0}
\definecolor{pred}{rgb}{0.9,0,0}
\definecolor{pgrey}{rgb}{0.46,0.45,0.48}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape
  linewidth=\textwidth
}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\lstset{language=xml,
  morestring=[b]",
  morestring=[s]{>}{<},
  morecomment=[s]{<?}{?>},
  stringstyle=\color{black},
  numbers=left,
  numberstyle=\scriptsize,
  stepnumber=1,
  numbersep=8pt,
  identifierstyle=\color{darkblue},
  keywordstyle=\color{cyan},
  backgroundcolor=\color{background},
  morekeywords={xmlns,version,type}% list your attributes here
}

\lstset{language=Java,
  showspaces=false,
  showtabs=false,
  tabsize=4,
  breaklines=true,
  keepspaces=true,      
  numbers=left,
  numberstyle=\scriptsize,
  stepnumber=1,
  numbersep=8pt,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{pgreen},
  keywordstyle=\color{pblue},
  stringstyle=\color{pred},
  basicstyle=\ttfamily,
  backgroundcolor=\color{background},
%  moredelim=[il][\textcolor{pgrey}]{$$},
%  moredelim=[is][\textcolor{pgrey}]{\%\%}{\%\%}
}




\begin{document}

\acrodefplural{dfs}[DFS]{Distributed File Systems}

\begin{acronym}
  \acro{dfs}[DFS]{Distributed File System}
  \acro{hdfs}[HDFS]{Hadoop Distributed File System}
  \acro{api}[API]{Application Programming Interface}
  \acro{bbs}[Buchungssystem]{Banking Buchungssystem}
\end{acronym}


%%%%%%%%%%%%%%%%%%%
%% Titelseite
%%%%%%%%%%%%%%%%%%%


\frontmatter
\titlehead{%  {\centering Seitenkopf}
  {Hochschule für angewandte Wissenschaften Würzburg-Schweinfurt\\
   Fakultät Informatik und Wirtschaftsinformatik}}
\subject{Bachelorarbeit}
\title{\BaTitle\\[15mm]}
\subtitle{\normalsize{vorgelegt an der Hochschule f\"{u}r angewandte Wissenschaften W\"{u}rzburg-Schweinfurt in der Fakult\"{a}t Informatik und Wirtschaftsinformatik zum Abschluss eines Studiums im Studiengang Informatik}}
\author{\BaAuthor}
\date{\normalsize{Eingereicht am: \BaDeadline}}
\publishers{
  \normalsize{Erstpr\"{u}fer: \BaSupervisorOne}\\
  \normalsize{Zweitpr\"{u}fer: \BaSupervisorTwo}\\
}

%\uppertitleback{ }
%\lowertitleback{ }

\maketitle


%%%%%%%%%%%%%%%%%%%
%% abstract
%%%%%%%%%%%%%%%%%%%

\section*{Zusammenfassung}
\addcontentsline{toc}{chapter}{Zusammenfassung}

TODO

\section*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

TODO

\newpage
\chapter*{Danksagung}
\addcontentsline{toc}{chapter}{Danksagung}
Danke an FH und adorsys. An Francis und an Prof.Heinzl. Eventuell auch an Korrekturleser.

%%%%%%%%%%%%%%%%%%%
%% Inhaltsverzeichnis
%%%%%%%%%%%%%%%%%%%
\tableofcontents										



%%%%%%%%%%%%%%%%%%%
%% Main part of the thesis
%%%%%%%%%%%%%%%%%%%
\mainmatter


\chapter{Einführung}\label{ch:intro}
Banken sind Big Data. Jeden Tag fließen unzählige Zahlungsprozesse und Kundendaten durch die Systeme deutscher und internationaler Banken. Alleine im Jahr 2012 schätzte man die Menge der gespeicherten Daten auf etwa 1,9 Petabyte pro Bank \cite{datanami}. Ein Ende des Datenwachstums ist nicht in Sicht. Durch den Einsatz von Mobile- und Online-Banking steigt nicht nur die Anzahl der Vertriebswege sondern auch der ausgeführten Transaktionen \cite{DBBigData}. Kunden rufen mobil ihren Kontostand ab, überweisen online ihre Rechnungen und bezahlen den Einkauf im Shoppingcenter mit der Kreditkarte. Dieses vertriebswegübergreifende Nutzen der Bankprodukte hat nicht nur einen Anstieg der Lese- und Schreibzugriffe auf die Systeme der Banken zur Folge, sondern fordert auch auf allen Vertriebswegen eine ähnlich gutes Nutzererlebnis \cite{bankwirtschaft}.

\section{Motivation}
Die weitläufig eingesetzten Kernbankensysteme sind veraltet und haben Probleme den Anforderungen und der Unmenge an Daten Herr zu werden. Ein Ausbau wäre möglich, ist jedoch aufgrund der benötigten Hard- und Software teuer oder liefert keine langfristige Skalierbarkeit und Ausfallsicherheit \cite{herzKernbankensystem}. Auf dieses Problem stießen vor den Banken schon Firmen wie Google, Facebook oder Yahoo. Die Lösung war in allen drei Fällen der Einsatz eines \acp{dfs}. So entwickelte Google das Google File System \cite{GFS} zur Skalierung ihrer Websuche, Facebook Haystack \cite{haystack} zum Speichern und Lesen von Bildern und Yahoo das \ac{hdfs} \cite{hdfs}. Diese Systeme laufen auf Standard-Hardware und sind daher einfach und günstig skalierbar. Außerdem überzeugen sie auch durch eine hohe Ausfallsicherheit und Verfügbarkeit. Auch Banken könnten durch die Möglichkeiten eines \acp{dfs} profitieren. Was bei Google und Co. funktioniert, birgt auch für Banken eine Chance langfristig mit den aufstrebenden FinTechs zu konkurrieren und das volle Potential ihrer Daten auszunutzen\cite{wiki:fintech}.

\section{Zielsetzung}

Banken führen Änderungen der IT Struktur in der Regel erst dann durch, wenn die zu übernehmende Technologie lang erprobt und sich als zuverlässig erwiesen hat. Doch der Wandel der Kunden im Umgang mit den Bankprodukten und die wachsende Datenmenge zwingt die Finanz Branche zu einem Umdenken \cite{bigdataBigStorage}. Im Rahmen dieser Arbeit sollen die Auswirkungen erarbeitet werden, die der Einsatz eines \acp{dfs} als Persistenzschicht eines Banking Buchungssystems (Buchungssystem) bewirken kann. Da nicht alle Prozesse, die ein Buchungssystem abbildet, die gleichen Anforderungen haben, gilt es diejenigen herauszufinden, welche durch ein \ac{dfs} realisierbar sind. Insbesondere sollen die Möglichkeiten einer verbesserten Skalierbarkeit und Ausfallsicherheit diskutiert werden. Aktuelle Buchungssysteme sind behebig und der Unterhalt für die Banken teuer \cite{bankingsCosts}. Durch ein auf Standard-Hardware optimiertes und leicht skalierbares System könnten Ressourcen akquiriert werden, wenn sie wirklich gebraucht werden und abgeschaltet werden, wenn sie nicht mehr nötig sind. So sollen die Kosten um ein Buchungssystem zu betreiben verringert werden. Im Idealfall profitieren davon nicht nur die Banken sondern auch deren Kunden. Zusätzlich zu den wirtschaftlichen Verbesserungen sollen auch die Entwicklungen in der Ausfallsicherheit und Verfügbarkeit in dieser Arbeit kenntlich gemacht werden.

\section{Umfeld}
Unterstützend und beratend bei der Entwicklung dieser Arbeit tritt das IT Consulting Unternehmen adorsys GmbH \& Co. KG auf. adorsys mit ihrem Hauptsitz in Nürnberg entwickelt mittlerweile seit mehr als 10 Jahren individuelle Software für Banken und Versicherungen. Zu den Kunden zählen neben der Teambank auch ERGO Direkt und Schwäbisch Hall. Auch die Entwicklung eines Open Source Kernbankensystems durch adorsys war zwischenzeitig geplant. Alles in Allem ist adorsys ein Partner mit Expertise im Finanzsektor und bei der Architektur von komplexen Systemen.

\section{Aufbau der Arbeit}
Im folgenden Kapitel wird zunächst genauer auf die Vorgehensweise der Recherche und Entwicklung der Arbeit eingegangen. So werden benötigte Metriken zum Messen der Performanz, Ausfallsicherheit und Skalierbarkeit erarbeitet. Außerdem wird der Rahmen der beispielhaften Implementierung und Bewertung der Lösung weiter abgesteckt.

Im Kapitel \nameref{buchungssystem} werden die grundlegenden Bestandteile und Aufgaben eines Buchungssystems erläutert. Besonders die Probleme und Entwicklungschancen sollen analysiert und ausgearbeitet werden.

Im darauf folgenden Kapitel soll auf Basis der vorher erarbeiteten Probleme die Funktionsweise eines \acp{dfs} näher erläutert und die Vor- und Nachteile analysiert werden. Auch auf typische Anwendungsgebiete und Grenzen wird näher eingegangen.

Nachdem nun die Begriffe des Buchungssystems und \acp{dfs} grob abgesteckt sind, können diese im Konzept-Kapitel verheiratet werden. Hier werden die Vorgänge zum Lesen und Schreiben von Buchungen sowie die Bedeutung von Transaktionen in einem Buchunugssystems betrachtet. Außerdem werden zusätzliche Maßnahmen zur Skalierung und Ausfallsicherheit besprochen.

Das Kapitel zur Implementierung beschreibt die schrittweise Umsetzung des Konzepts und die dafür benötigten Technologien. Auch die konkrete Auswahl der Programmiersprache und des \acp{dfs} wird hier getätigt.

Im Kapitel zur Evaluierung wird die Zielsetzung der Arbeit mit den Funktionen der Testimplementierung verglichen und das Ergebnis bewertet. Das Letzte Kapitel zeigt Möglichkeiten und Erweiterungen der Lösung auf.

\chapter{Vorgehensweise}
Die in der Einführung beschriebene Zielsetzung kann sich leider nur auf sehr wenige wissenschaftliche Quellen stützen. Gerade die benötigte Information zu den Banken ist rar gesät und kann meist nur aus Artikeln von News-Seiten oder Blogs entnommen werden. Angaben zur Datenmenge von Banken oder den Betriebskosten eines Kernbankensystems können daher nur geschätzt werden. Die Entwicklung des Konzepts und die darauf folgende Bewertung begründet sich also mehr auf Analysen und Annahmen als auf empirisch bewiesene Tatsachen. Alle technologiebezogenen Annahmen werden jedoch durch wissenschaftliche Arbeiten, welche sich mit den Grenzen und Möglichkeiten eben dieser Technologie beschäftigen, untermauert.

\section{Analyse der Ist-Situation}
Bevor eine Bewertung der Ist-Situation in irgendeiner Form durchgeführt werden kann, muss zunächst die Bedeutung eines Buchungssystems innerhalb des Kernbankensystems und der Bank verstanden werden. Dabei gilt es, nicht nur die technische Architektur herauszuarbeiten, sondern auch die abzubildenden Prozesse zu erfassen. Dies soll vor allem im Hinblick auf die Anforderungen geschehen, welche sich durch die neuen Vertriebswege wie Online- und Mobile-Banking ergeben haben. Wie sieht der Ablauf zum Erstellen einer Buchung aus? Was mussten Buchungssysteme damals und was müssen sie heute leisten? Entstehen überhaupt Probleme durch die zunehmende Anzahl an Anfragen an die Systeme der Banken? Welche Aufgaben haben welche Teile des Buchungssystems? Durch Fragen wie diese sollen die minimalen Anforderungen eines heutigen Buchungssystems erarbeitet und priorisiert werden. Außerdem soll erforscht werden zu welchen Bedingungen diese Anforderungen von den aktuellen Systemen abgebildet werden und wo die eingesetzte Persistenzschicht die Möglichkeiten des Systems ausbremst.

\section{Einsatzbereiche eines DFS}
Nachdem die Bedeutung des Buchungssystems bekannt ist, werden die Bestandteile herangezogen, bei denen die aktuell eingesetzte Persistenzschicht am schlechtesten auf die Anforderungen passt. Schlecht bedeutet hierbei, dass entweder zu viel oder zu wenig Funktionalität bereit gestellt wird oder, dass der Einsatz der Technologie massive Nachteile mit sich bringt. Danach soll die Funktionsweise mehrerer \ac{dfs} verstanden und deren Möglichkeiten mit den minimal benötigten Anforderungen abgeglichen werden. So lässt sich herausarbeiten, ob ein \ac{dfs} allein überhaupt zur Realisierung eines Bestandteiles des Buchungssystems geeignet ist. Zusätzlich sollen die Vor- und Nachteile eines \ac{dfs} mit den Vor- und Nachteilen der vorher eingesetzten Persistenzschicht verglichen werden. Je nachdem welche Technologie hier besser abschneidet, lässt sich absehen ob sich eine Investition in ein \ac{dfs} lohnt oder nicht.

\section{Entwicklung des Konzepts}
An dieser Stelle ist bereits klar wie ein Buchungssystem funktioniert und welche Anforderungen es zu erfüllen hat. Außerdem sind die Teile des Systems bekannt, welche Raum für Verbesserungen durch ein \ac{dfs} bieten. Auch die generelle Funktionalität von einem \ac{dfs} wurde erarbeitet und herausgefunden welches der gegenwärtigen \ac{dfs} am besten zur Realisierung einzelner Bestandteile eines Buchungssystems geeignet ist. Dieses Wissen soll als Grundlage dienen, ein Konzept zu entwickeln, das noch detaillierter auf die Bedürfnisse des Buchungssystems eingeht. Dabei dient die Funktionsweise des am besten passenden \acp{dfs} als Richtlinie, welches aber noch speziell für den Anwendungsfall eines Banking Buchungssystems angepasst werden soll. Ob bereits ein \ac{dfs} existiert, dass exakt so funktioniert spielt hier zunächst keine Rolle. Es soll vielmehr ein System entwickelt werden, das neben seinen Aufgaben auch die wirtschaftlichen, leistungsbezogenen und auf die Skalierung bezogenen Anforderungen bestmöglich erfüllt. Auf Basis dieses Systems soll das Erstellen und Lesen einer Buchung detailliert erklärt werden. Dieser Prozess kann dann den entsprechenden Schritten eines konventionellen Buchungssystems gegenüber gestellt werden.

\section{Beispielhafte Implementierung}
Um das Ergebnis des Konzepts hinsichtlich der Performanz und Umsetzbarkeit zu testen, soll es beispielhaft implementiert werden.
Dazu muss zum einen die Programmiersprache für das Backend als auch ein konkretes \ac{dfs} ausgewählt werden. Falls es kein \ac{dfs} geben sollte, welches den aus dem Konzept hevorgehenden Anforderungen gerecht wird, muss auf das am besten passende ausgewichen werden. Die tatsächliche Entwicklung eines \acp{dfs} ist äußerst komplex und sprengt bei weiten den Rahmen dieser Arbeit. Das führt dazu, dass das Konzept gegebenenfalls auf ein bereits existierendes \ac{dfs} angepasst werden muss. Des Weiteren steht die Entwicklung einer Schnittstelle zum \acp{dfs} und des Backends an. Alle Schritte sollen eine ausreichende Testabdeckung vorweisen und entsprechend dokumentiert werden.

\section{Bewertung der Lösung}
Die Bewertung der im Rahmen dieser Arbeit entwickelten Lösung kann nur auf Basis der tatsächlichen Implementierung erfolgen. Die Auswirkung von den Teilen des Konzepts, welche sich technisch nicht umsetzen lassen, können nur erahnt werden und sind deshalb nicht zu berücksichtigen. Zur Bewertung der entwickelten Lösung werden die fünf folgenden Kriterien herangezogen.

\begin{enumerate}
  \item \textbf{Integrierbarkeit:} Beschreibt wie hoch der Aufwand geschätzt wird das System in ein Buchungssystem zu integrieren.
  \item \textbf{Skalierbarkeit:} Wie gut lässt sich das System skalieren? Welche Teile können zum Flaschenhals werden? Zusätzlich zu diesen Fragen soll es auch darum gehen, ob eine lastbezogene Skalierung möglich ist und wenn ja, wie viel Aufwand dazu betrieben werden muss.
  \item \textbf{Performanz:} Die Leistung soll durch die Anzahl der beantworteten Anfragen bei maximaler Auslastung bewerten werden. Zusätzlich muss das System nach dem Performanz-Test einen korrekten Stand beinhalten.
  \item \textbf{Ausfallsicherheit:} Bei der Ausfallsicherheit soll betrachtet werden, wie viele Teile des Systems versagen können, bevor es zu einem inkonsistenten Zustand oder einem Ausfall führt.
  \item \textbf{Wirtschaftlichkeit:} Zu guter Letzt soll geschätzt werden was der Betrieb des Systems kostet. Dazu wird sowohl die erforderliche Hardware und deren Kosten betrachtet, als auch der durchschnittliche Preis um das System bei einem externen Anbieter zu hosten. Auch die Kosten für eine mögliche Skalierung und die Gewährleistung einer annehmbaren Ausfallsicherheit werden berücksichtigt.
\end{enumerate}

Der Einfluss und die Möglichkeiten eines \acp{dfs} auf ein Buchungssystem sollen möglichst klar aus diesen Kriterien hervorgehen.
Daher sollen auch aktuell bei Banken eingesetzte Buchungssysteme so gut wie möglich anhand dieser Kriterien bewertet werden. Im direkten Vergleich zeigt sich am besten was der Einsatz eines \acp{dfs} letztendlich für ein Buchungssystem leistet.

\chapter{Wesen und Probleme eines Buchungssystems}
\label{bookingSystem}
Die Anwendungssysteme der Banken sind alt und über lange Zeit gewachsen. Wo 1970 die ersten spartenbezogenen Programme nur bei Kredit-, Einlagegeschäften und
Wertpapierabwicklung unterstützten, bilden jetzt die Systeme der Banken nahezu alle Geschäftsprozesse ab \cite[16]{ITidF}. Um die Bedeutung des Buchungssystems innerhalb dieser komplexen Anwendungsstruktur zu verstehen, soll im Folgenden zunächst auf die Entstehung und Architektur der Anwendungsysteme von Banken eingegangen werden. Danach werden die Aufgaben und Bestandteile eines Buchungssystems erarbeitet und die daraus entstehenden Anforderungen aufgezeigt. Im letzten Schritt sollen die Probleme der aktuellen Systeme besonders hinsichtlich der neuen Vertriebswege behandelt werden. 
\label{buchungssystem}

\section{IT-Systeme der Banken}
Seit den ersten Anwendungssystemen der Banken stand das Konto und die Kontoführung im Zentrum. Das Hinzufügen neuer Funktionen und Anforderungen erfolgte über das anhängen neuer Module an eben diesen Konto bezogenen Kern. Die so angedockten Abwicklungssysteme waren zum Beispiel für die Abwicklung des Inlands- und Auslandszahlungsverkehr, des Kreditwesens oder der Einlagen verantwortlich. Nachdem die Banken auch Unterstützung bei den Geschäftsprozessen forderten, legte sich um die Abwicklungssysteme ein weiterer Ring. Dieser stellte Dienste für Kundenberater und Sachbearbeiter der Banken zur Verfügung \cite[18-20]{ITidF}\cite{SuPdIiB}. Diese Vorgehensweise führte zu einer Silo- oder auch Spartenarchitektur die in Abbildung \ref{zwiebel} dargestellt wird. Systeme dieser Art wurden ursprünglich von den Banken selbst entwickelt. Die stark heterogenen Teilsysteme und die unentwirrbaren Abhängigkeiten zwischen ihnen stellte sich jedoch als nicht weiter tragbar und wartbar heraus \cite{bankEnzy}\cite{SuPdIiB}\cite[52]{ITidF}. Besserung versprach der Einsatz von hoch standardisierten Teilsystemen oder Gesamtbankenlösungen von Drittanbietern. Die Standardisierung erlaubt über Parameter eine eingeschränkte Anpassung der Systeme an die Bedürfnisse der Banken. So können die benötigten Systeme von unterschiedlichen Anbietern eingekauft und verbunden werden. Im Gegensatz dazu steht die Gesamtbankenlösung, die weiterhin versucht allen Anforderungen und Funktionen einer Bank gerecht zu werden. Diese Art von Systemen ist aber aus ähnlichen Gründen, wie die ursprüngliche Anwendungsstruktur der Banken nicht besonders erfolgreich \cite[S. 56 ff.]{ITidF}. 

\begin{figure}
   \makebox[\textwidth]{\includegraphics[width=\paperwidth]{img/3/zwiebelstrktur.png}}
  \caption[Historische Anwendungsstruktur von Banken]{Historisch gewachsene Anwendungsstruktur von Banken. Entnommen aus \cite{SuPdIiB}}
  \label{zwiebel}
\end{figure}

Heutzutage sind bei Genossenschaftsbanken häufig die Produkte von Fiducia GAD \cite{fiducia}, bei kleineren Privatbanken Systeme von FIS Kordoba \cite{kordoba} oder SAP \cite{SAP} und bei den Sparkassen die Lösung der Finanz Informatik One System Plus (OSPlus) \cite{finanzinformatik} im Einsatz. Die Entwicklung eigener Systeme können sich nur noch wenige große Banken wie die Deutsche Bank erlauben. Bis auf die Lösung der Finanz Informatik OSPlus und FIS Kordoba handelt es sich in Deutschland in der Regel um Systeme die keine Gesamtbankenlösung bieten sondern mehr oder weniger Teile der Geschäfts- und Kontoprozesse der Banken abdecken und mit anderen Systemen kombiniert werden können \cite{einfuehrungKernbanksystem}\cite[56-58]{ITidF}. Auch wenn die aktuellen Systeme deutlich besser standardisiert und dadurch wartbarer als früher sind, ist die Struktur immer noch ähnlich oder baut im Kern sogar noch auf den Ursprüngen der Banken IT auf. Die Architektur lässt sich in vier Schichten aufteilen, die von oben nach unten folgendermaßen beschrieben sind \cite[104]{ITidF}:

\begin{enumerate}
\item \textbf{Visualisierungsschicht:} Die Visualisierungsschicht ist die Schnittstelle zum Benutzer und wird auf dessen Endgerät ausgeführt. Sie bezieht einerseits Daten von der Darstellungsschicht und zeigt diese ansprechend an. Andererseits leitet sie die Eingaben des Nutzers an die Darstellungsschicht weiter.
\item \textbf{Darstellungsschicht:} Diese Schicht ist verantwortlich für eine fehlerfreie Kommunikation zwischen Visualisierungsschicht und Anwendungsschicht. Daten werden von einer Schicht empfangen und in das gewünschte Format der anderen Schicht überführt.
\item \textbf{Anwendungsschicht:} Die Anwendungsschicht kümmert sich um die eigentliche Geschäftslogik für einen bestimmten Teilbereich. Sie stellt Schnittstellen für Dienste wie teilweise auch die neuen Vertriebswege Online- und Mobile-banking zur Verfügung und führt aufwendige Datenmanipulationen auf der Datenschicht durch. Aufgrund der teilweise sehr alten und komplexen Systeme finden sich hier auch noch Bereiche die noch in COBOL oder sogar Assembler geschrieben sind.
\item \textbf{Datenhaltungsschicht:} Die Datenhaltungsschicht ist für die Verwaltung der Stammdaten sowie der von der Anwendungsschicht gesendeten und angefragten Informationen zuständig. Auch Operationen für einfache Manipulationen der Daten werden von ihr bereitgestellt. In der Finanzindustrie gelten relationale Datenbanken als Standard für die Persistenzschicht der Datenhaltungsschicht \cite[105]{ITidF}\cite{MarkstudieKernbankensysteme}
\end{enumerate}

Die Datenhaltungsschicht als Basis der Architektur entspricht je nach Aufteilung und Definition entweder allein oder in Kombination mit einzelnen Anwendungsschichten dem Buchungssystem. Ähnlich wie in Abbildung \ref{zwiebel} bildet es auch heute noch das Zentrum der IT Landschaft von Banken. Als ein solches wird es auch häufig als Kernbankensystem bezeichnet. Diese Bezeichnung verdeutlicht auch, dass die Aufgabe des Buchungssystems die Erfüllung von Kernaufgaben ist und allein nur eine begrenzte Funktionalität bereitstellt. Die Unterstützung weitergehender Bank Prozesse muss durch das Aufsetzen weiterer Anwendungsschichten erfolgen \cite[58]{ITidF}. Da in der Literatur und Wirtschaft der Begriff Kernbankensystem auch häufig als eine Gesamtbankenlösung verstanden wird, wird in dieser Arbeit immer von Buchungssystem gesprochen sofern es um die Kernfunktionalität von Banken geht \cite{vergleichCoreBanking}.  


\section{Das Buchungssystem und seine Aufgaben}
Das Buchungssystem steht demnach im Zentrum jedes kontenbasierten Geschäftsvorfalls der Banken \cite{bankEnzy}. Daher muss es auch alle Kernfunktionen eben dieser erfüllen und unterstützen können. Dazu gehören die Zahlungsverkehrs-, Investitions- und Kreditfunktion aber auch die Verwaltung von Kundenstammdaten sowie die grundlegende Kontoführung. Für Investitions- und Kreditfunktion können auch die Begriffe Passiv- und Aktivgeschäfte genutzt werden \cite[12, 86]{DdF}\cite{einfuehrungKernbanksystem}. Im Folgenden soll der Inhalt der einzelnen Funktionen anhand von \cite[69-88]{DdF} und \cite[91-153]{bankwirtschaft} beschrieben werden.

\begin{itemize}
  \item \textbf{Aktivgeschäfte:} Das Aktivgeschäft erhält seinen Namen, da alle Kreditgeschäfte also Forderungen an Kunden in der Bilanz auf der Aktivseite abgebildet werden. Durch das Kreditgeschäft erzielen Banken einen Großteil ihrer Zinserträge. Jedoch ist damit auch ein hohes Risikopotential verbunden. Als Aktivgeschäft zählen unter Anderem alle klassischen Kreditgeschäfte.
  Zum Beispiel der Kontokorrentkredit oder auch Dispositionskredit, räumt Privatpersonen eine Überziehungsmöglichkeit des Kontos ein. Aber auch längerfristige Kreditgeschäfte wie der Hypothekarkredit, Baukredit oder Investitionskredit gehören zu den Aktivgeschäften. Bei den Kreditgeschäften gilt in der Regel, dass von der Bank zum Kunden ein vertraglich festgelegter Betrag fließt, dessen Rückzahlung zusätzlich Zinsen vom Schuldner auf ein Konto der Bank zu erbringen ist.
  \item \textbf{Passivgeschäfte:} Als Passivgeschäft werden alle Einlagegeschäfte der Banken bezeichnet. Im Gegensatz zu den Aktivgeschäften erscheinen sie in der Bilanz auf der Passiv Seite. Aktiv- und Passivgeschäfte stehen in einer engen Verbindung. Häufig finanzieren Kreditinstitute ihre Aktivgeschäfte durch die Einlagen der Kunden. Zu den Passivgeschäften gehören Sichteinlagen, und Termineinlagen. Sichteinlagen sind täglich fällige Gelder. In der Regel handelt es sich hierbei um die Einzahlungen auf ein Girokonto. Ihr Zweck ist hauptsächlich der bargeldlose Zahlungsverkehr. Termineinlagen hingegen entziehen den Kunden den Zugriff auf die Einlage für einen festgelegten Zeitraum welcher in der Regel auf eine Dauer von mindestens 30 Tage und maximal 5 Jahren angelegt wird. Es handelt sich also um Einlagen, welche der Kunde über einen gewissen Zeitraum nicht benötigt. Dafür werden Termineinlagen höher verzinst als Sichteinlagen. Werden Termineinlagen als Kündigungsgelder vereinbart, so gibt es keine Laufzeitfrist und die Auszahlung erfolgt nach Einreichen einer Kündigung und abgelaufener Kündigungsfrist. Bei Passivgeschäften fließt also ein festgelegter Betrag vom Kunden zur Bank, welche über die Einlage in einem festgelegten Rahmen verfügen darf.
  \item \textbf{Zahlungsverkehr:} Der Zahlungsverkehr im Bankengeschäft beschreibt die bare und unbare Übertragung von Zahlungsmitteln im Inland und Ausland. Zur Übertragung können Überweisungen, Kartenzahlungen oder Lastschriften genutzt werden. Die besondere Bedeutung des Zahlungsverkehrs liegt in der Unumgänglichkeit für die Bankkunden. Auch wenn Kunden keine Kredite nehmen oder keine Einlagen tätigen, so müssen dennoch immer Zahlungen über die Systeme der Banken ausgeführt werden. Gerade die zunehmende Digitalisierung wird einen großen Einfluss auf die Entwicklung des bargeldlosen Zahlungsverkehrs haben. 
\end{itemize}

Das Buchungssystem muss all diese Inhalte führen können und Kontobewegungen nachvollziehbar ablegen. Im Folgenden werden alle Kontobewegungen als Buchungen bezeichnet. Es dient also neben den Anwendungssystemen auch den gesetzlichen Rechnungsabschlüssen und Bilanzen als Grundlage \cite{bankEnzy}\cite{MarkstudieKernbankensysteme}. In der Regel wird diese Nachvollziehbarkeit durch die Doppelte Buchführung gewährleistet. Das heißt, dass eine Buchung immer in den Konten beider beteiligten Parteien auftaucht. In Summe müssen alle Kredit- und Debitbeträge Null ergeben\cite{accounting}.

\section{Anforderungen}
Um die oben genannten Aufgaben Jetzt und auch noch in Zukunft zuverlässig zu erfüllen, müssen Buchungssysteme gewissen Anforderungen gerecht werden. Diese lassen sich in interne und externe Anforderungen unterteilen. Interne Anforderungen beschreiben die Bedingungen, die eine Bank nachkommen muss um die geschäftsinternen Prozesse reibungslos und wirtschaftlich durchzuführen. Beispiele dafür sind eine wohldefinierte Schnittstelle des Buchungssystems um bei Bedarf neue Geschäftsprozesse anzudocken oder ein detaillierte Analyse des Kundenverhaltens durchführen zu können. Auch die Reduktion der IT Wartungskosten gilt als eine interne Anforderung an Buchungssysteme. Externe Anforderungen beziehen sich auf Bedingungen, die Banken von außen auferlegt werden. Dazu gehören gesetzliche Richtlinien, sowie Veränderungen von Angebot und Nachfrage im Markt \cite{capgemini}. So wird durch den Kunden aber auch dem Gesetzgeber das zuverlässige Ablegen einer jeden Kontobewegung beziehungsweise Buchung gefordert. Das Buchungssystem muss folglich immer erreichbar sein und eine hohe Ausfallsicherheit gewährleisten. Da aber neben der Ausfallsicherheit auch die Geschwindigkeit der Abarbeitung bankfachlicher Prozesse relevant ist, spielt die Performanz der Buchungssysteme auch eine tragende Rolle. Banken legen deswegen häufig ihre Buchungssysteme redundant an und verbinden diese über ein sicheres und performantes Datennetz \cite{bankEnzy}\cite[97-99]{ITidF}. Die so nebeneinander gestellten Systeme erlauben auch eine Lastverteilung bei der Bearbeitung mehrerer Anfragen. Gerade in Hinblick auf die neuen Vertriebswege und dem steigenden bargeldlosen Zahlungsverkehr ist ein System, dass sich skalieren lässt unabdingbar \cite{bankEnzy}\cite{capgemini}. Die Abbildungen \ref{online-banking} und \ref{bargeldlos} zeigen dieses Wachstum auf.


\begin{filecontents}{date.dat}
date       value
2006-01-01  34
2007-01-01  34
2008-01-01  36
2009-01-01  37
2010-01-01  35
2011-01-01  44
2012-01-01  44
2013-01-01  45
2014-01-01  54
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahre},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anteil der Nutzer in Prozent},
date ZERO=2005-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date.dat};
\end{axis}
\end{tikzpicture}
\caption[Nutzer von Online-Banking in Deutschland]{Nutzer von Online-Banking in Deutschland. Nachempfunden nach \cite{onlinebanking}.}
\label{online-banking}
\end{center}
\end{figure}


\begin{filecontents}{date2.dat}
date       value
2011-01-01  90.61 
2012-01-01  94.38
2013-01-01  99.52
2014-01-01  103.34
2015-01-01  112.13
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahre},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anzahl der Transaktionen in Mrd.},
date ZERO=20010-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date2.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionen im bargeldlosen Zahlungsverkehr in der EU]{Anzahl der Transaktionen im bargeldlosen Zahlungsverkehr in der EU. Nachempfunden nach \cite{bargeldlos}.}
\label{bargeldlos}
\end{center}
\end{figure}


Die Folge dieser Entwicklung ist nicht nur eine steigende Anzahl an Zugriffen auf die Systeme der Banken, sondern auch eine stetig zunehmende Datenmenge. Nicht umsonst wurde in der Einleitung dieser Arbeit die Verbindung zwischen Big Data und Banken geknüpft. Wo ursprünglich das alleinige Speichern aller Buchungsdaten ausreichend war fordern neue Dienste und Richtlinien nun auch den schnellen und gezielten Zugriff auf Kundendaten, Konten und Buchungen \cite{bigdataBigStorage}. In diesem Zuge ist die Second Payment Service Directive (PSD2) zu nennen. Dabei handelt es sich um eine EU Richtlinie die im Januar 2016 in Kraft getreten ist. Demnach sind Banken verpflichtet auf Wunsch des Kunden alle seine Konten, und durchgeführten Buchungen einem Drittanbieter über eine sichere Verbindung zur Verfügung zu stellen. Auch das Autorisieren von Bezahlungen muss über diese Verbindung möglich sein. Die Europäische Aufsichtsbehörde hat am 23. Februar 2017 die Regulatory Technical Standards (RTS) veröffentlicht, welche festlegen sollen, wie diese Verbindung realisiert werden muss und wie die Schnittstelle für Drittanbieter auszusehen hat \cite{rts}. Jetzt haben die Banken bis zum vierten Quartal 2018 Zeit die Bestimmung in ihre Systeme zu integrieren \cite{eu-psd2}\cite{psd2dk}. Die PSD2 stellt eine enorme Herausforderung für die Buchungssysteme der Banken dar. Wo vorher nur Zugriffe von bankeigenen Systemen möglich waren können jetzt beliebige Drittanbieter Kontodaten und Kontobewegungen abfragen. Als Konsequenz können deutlich steigende Lese- und moderat steigende Schreibzugriffe erwartet werden. Die Vorteile die sich für den Endnutzer ergeben, stellen die Banken hingegen vor ein Problem. Für die Bankkunden sind alle Bankprodukte bankenübergreifend in einer einzigen Anwendung verfügbar. Drittanbieter können Kauf- und Sparverhalten der Kunden analysieren und sinnvolle Hinweise zur Kontoführung geben. Zusätzlich sind Kunden nicht mehr an die eine Schnittstelle ihrer Bank gebunden, um Buchungen durchzuführen, sondern können ihrer Finanzen mit der besten am Markt erhältlichen Anwendung verwalten. Die Banken müssen jedoch zum einen mit der größeren Belastung der Buchungssysteme umgehen und zum Anderen ihre Anwendungen und Infrastruktur umstrukturieren um trotz Konkurrenz die Kunden auf ihrer Plattform halten zu können \cite{psd2vid}.
Die Anforderungen an ein Buchungssystem belaufen sich demnach auf eine wohl definierte Schnittstelle sowohl für bankeigene Prozesse als auch für Drittanbieter, ein gutes Kosten zu Nutzen Verhältnis der Systeme, sowie auf Skalierbarkeit und Ausfallsicherheit welche den Forderungen durch PSD2 und neuen Vertriebswegen wie Mobile- und Online-Banking gerecht wird.

\section{Probleme}
Wie bereits erwähnt sind die IT-Landschaften der Banken und insbesondere die Buchungssysteme alt und über einen langen Zeitraum gewachsen. Neue Funktionalität wurde über das Hinzufügen neuer Schichten realisiert, wobei der Kern gleich blieb. Da aber zur Zeit der Entwicklung der Buchungssysteme die heutigen Anforderungen noch nicht absehbar waren, können diese auch nicht zufriedenstellend erfüllt werden \cite[23-27]{ITidF}\cite{bankEnzy}. Auch die Banken selbst sind sich bewusst, dass etwas getan werden muss, um weiter auf dem Markt relevant zu bleiben \cite{capgemini}.
Besonders die Skalierung der Systeme könnte sich für die Banken als Problem herausstellen. Durch die PSD2 und die neuen Vertriebswege werden eine steigende Datenmenge sowie Lese- und Schreibzugriffe eine hohe Belastung der Buchungssysteme zur Folge haben \cite{bigdataBigStorage}. Experten gehen davon aus, dass sich die Menge der Daten bis 2020 versiebenfacht \cite{versiebenfacht}. Buchungssysteme können aber nicht beliebig skaliert werden um die Anforderungen zu meistern. Die eingesetzten relationalen Datenbanken scheinen für viele Aufgaben essentiell, sind aber durch ihre Architektur und grundlegenden Konzepte nicht für das Speichern und Verwalten beliebig vieler Einträge geeignet \cite{rdbmsBigData}. Die Grundlage für relationale Datenbanken bildet das ACID (Atomicity, Consistency, Isolation, Durability) Prinzip. Atomarität, Konsistenzerhaltung, Isolation und Dauerhaftigkeit beschreiben eine mächtige Möglichkeit, Transaktionen innerhalb eines Systems abzubilden. Auch bei nebenläufigen Prozessen kann so immer ein konsistenter Stand gewährleistet und im Fehlerfall wieder hergestellt werden. Diese Möglichkeiten gehen jedoch auf Kosten der Performanz. Um die Einhaltung der ACID Prinzipien zu garantieren, müssen alle an einer Transaktion beteiligten Einträge mit einem exklusiven Lock versehen werden. Bei einer stark verteilten Datenbank erfordert dieser Vorgang ein eigenes verteiltes Commit-Protokoll auch Zwei-Phasen-Commit genannt \cite{dbarchitecture}. Alle an einem Commit beteiligten Datenbanksysteme müssen den Commit bestätigen. Denn entweder wird die Transaktion auf allen Systemen oder auf keinem ausgeführt. Ist ein Datenbanksystem nicht verfügbar, kann die Transaktion nicht durchgeführt werden. Bei zwei Datenbanksystemen mit jeweils 99,9 \% Verfügbarkeit wird diese durch die Abhängigkeit des Zwei-Phasen-Commits auf 99,8 \% Verfügbarkeit des Gesamtsystems reduziert \cite{BASE}. Wie sich der Transaktionsdurchsatz einer relationalen Datenbank bei einem steigenden Anteil an konkurrierenden Anfragen verhält ist in Abbildung \ref{salt} zu erkennen. Hierbei wurden Transaktionen mit jeweils fünf Update-Operationen auf Tabellen mit unterschiedlich vielen Einträgen ausgeführt. Je nach Anzahl der Reihen in den Tabellen kam es so zu mehr oder weniger konkurrierenden Zugriffen \cite{salt}.
\begin{filecontents}{date3.dat}
date  value
0     70000
1     65000
2     10000
3     2000
4     300
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xtick=data,
ymin=0, ymax=80000,
scaled ticks=false,
xticklabels={0,0{,}0001,0{,}001,0{,}01,0{,}1},
xlabel={Konkurrierender Zugriff (1/\#Reihen)},
ylabel={Transaktionsdurchsatz (Transaktionen/Sekunde)},
]
\addplot table[x=date,y=value] {date3.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionsdurchsatz im Verhältnis zum konkurrierenden Zugriff]{Transaktionsdurchsatz im Verhältnis zum konkurrierenden Zugriff auf Ressourcen in einem ACID basierten System. Nachempfunden nach \cite{salt}.}
\label{salt}
\end{center}
\end{figure}
Der sinkende Transaktionsdurchsatz ist eine Folge der nach ACID-Prinzipien durchgeführten Transaktionen. Hierbei werden Datenbankeinträge erst wieder freigegeben, nachdem die gesamte Transaktion durchgeführt oder abgebrochen wurde. Je nachdem wie das Locking implementiert oder welche Daten angefordert werden, kann eine Transaktion ein Lock für einzelne Tabellenzeilen, die ganze Tabelle, ganze Datenblöcke oder sogar für das ganze Datenbanksystem anfordern. Es ist also durchaus möglich, dass eine Kontotransaktion für einen Kunden auch die Einträge anderer Kunden mit beansprucht \cite{locking}\cite{dbarchitecture}.

Aber nicht nur bei rein schreibenden Transaktionen fällt die Performanz relationaler Datenbanken enorm ab. Die Abbildung \ref{salt2} zeigt den Transaktionsdurchsatz auf eine Tabelle mit 100 Einträgen bei Transaktionen die entweder fünf Einträge verändern oder lesen, während der Anteil der lesenden Transaktionen immer weiter zunimmt. Wie zu sehen ist führt schon ein zehn prozentiger Anteil an Schreibzugriffen zur Halbierung des Transaktionsdurchsatzes.
\begin{filecontents}{date4.dat}
date  value
0     100000
5     90000
10    50000
15    12000
20     8000
50     5000
100    1000
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xmin=0, xmax=100,
ymin=0, ymax=100000,
scaled ticks=false,
xlabel={Anzahl Schreibzugriffe in Prozent},
ylabel={Transaktionsdurchsatz (Transaktionen/Sekunde)},
]
\addplot table[x=date,y=value] {date4.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionsdurchsatz im Verhältnis zum Anteil der Schreibzugriffe]{Transaktionsdurchsatz im Verhältnis zum Anteil der Schreibzugriffe in einem ACID basierten System. Nachempfunden nach \cite{salt}.}
\label{salt2}
\end{center}
\end{figure}

Zusätzlich ist bei einem sehr hohen Durchsatz von Transaktionen nicht zu erwarten, dass zwei Transaktionen mit ähnlichen Daten auch im Speicher immer nah zusammen liegen. Bezogen auf die kontenbezogenen Daten, kann ein hoher Durchsatz von schreibenden Transaktionen zu einer Datenfragmentierung führen. Das bedeutet, dass es unwahrscheinlich ist, dass alle Umsätze eines Kunden innerhalb der Umsatztabelle der Bank auch im Speicher nahe zusammen liegen. Auch das könnte einen Einfluss auf die Lesegeschwindigkeit haben. Relationale Datenbanken werden wenn, dann in der Regel vertikal Skaliert. Horizontale Skalierung ist zwar zum Beispiel über Sharding möglich, ist jedoch meist aufwendig zu implementieren und hat bei verteilten Anfragen negative Auswirkungen auf die Performanz der Datenbanksysteme. Vertikale Skalierung beschreibt die Skalierung von Systemen durch das Aufrüsten von Hardware Komponenten. Also zum Beispiel das Einsetzen eines größeren Hauptspeichers oder eines schnelleren Prozessors. Horizontale Skalierung hingegen wird durch die Verteilung der Anwendung auf mehrere Maschinen erreicht. Die Nachteile an der vertikalen Skalierung sind jedoch die sehr limitierten Möglichkeiten und die höheren Kosten im Vergleich zur horizontalen Skalierung \cite{sharding}\cite{rdbmssuck}. 

Die Kosten der IT-Systeme sind jedoch auch jetzt schon ein Problem für die Banken. Eine exakte Ermittlung der Ausgaben für die IT der Banken stellt sich aber als schwierig heraus, da die Zahlen in der Regel nicht öffentlich gemacht werden. Die Schätzungen der IT Kosten beziehen sich daher auf die in der Gewinn- und Verlustrechnung öffentlich gemachten Aufwendungen für den Verwaltungsaufwand der Banken. Experten schätzen das 15 - 20 \% von diesem für die IT anfallen. Demnach lagen die Kosten für die Anwendungssysteme der Deutschen Bank im Jahre 2004 bei rund 2,6 Mrd. Euro, und die der HypoVereinsbank bei mehr als 900 Mio. Euro. Die zehn größten Banken in Deutschland haben in 2004 etwa 6,5 Mrd. Euro für die Instandhaltung und Entwicklung ihrer IT ausgegeben. 
Etwa 24 \% davon laufen alleine in die Buchungsdienste. Gerechnet auf die Girokonten ergibt das Jahresausgaben von etwa 14 Euro pro Girokonto \cite[29-39]{ITidF}. Diese Zahl lässt sich auch durch den Umsatz und die betreuten Konten von Kernbanksystem Anbietern validieren. So betreute die Fiducia IT 2005 insgesamt 52,6 Mio. Kundenkonten und erzielte einen Jahresumsatz von etwa 728,6 Mio. Euro. Das entspricht einem Preis von 13,80 Euro pro Konto. Die Finanz Informatik im Jahr 2005 erzielte bei 58 Mio. Kundenkonten einen Umsatz von 723 Mio. Euro, was einen Kontopreis von etwa 12,50 Euro entspricht. Dabei fließen nur etwa 20 \% in die Entwicklung neuer Dienste. Die restlichen Kosten fallen für den reinen Betrieb und Instandhaltung an. Langfristig sind diese Ausgaben für Banken nicht tragbar \cite[75-91]{ITidF}\cite{SuPdIiB}\cite[41-42]{DdF}\cite{bankingsCosts}. 


\chapter{Grundlagen eines DFS}
Änderungen an der Skalierung der Buchungssystemen und dem insgesamt hohen Kostenfaktor der Anwendungssysteme von Banken, werden auch durch die ACID basierten Datenbanksysteme erschwert. Auch wenn die Funktionalität die ACID mit sich bringt nur für einen Bruchteil der Prozesse benötigt wird, wird so das gesamte System ausgebremst und in den Skalierungsmöglichkeiten eingeschränkt \cite{salt}.
Systeme die hingegen die Skalierbarkeit und Verfügbarkeit gegenüber der Konsistenz priorisieren funktionieren nach dem BASE (Basically Available, Soft State, Eventual consistency) Prinzip. Häufig weisen diese Systeme niedrigere Betriebskosten im Gegensatz zu den ACID Alternativen auf \cite{clusterBASE}.
In diesem Kapitel soll zunächst die Bedeutung von BASE geklärt werden. Ein besonderer Fokus liegt dabei auf den Unterschieden der Konsistenz zu traditionellen ACID Systemen. Dann soll es um die Funktionsweise von DFS gehen. Diese funktionieren nach dem BASE Prinzip und haben Verfügbarkeit, Ausfallsicherheit und Kosteneffizienz zu ihrer Königsdisziplin gemacht. Zum besseren Verständnis werden zwei populäre Vertreter der DFS näher erläutert. Im letzten Teil des Kapitels werden die Vorteile und Grenzen der DFS nochmal genauer betrachtet und die Anforderungen an ein Buchungssystem mit den Möglichkeiten eines DFS abgeglichen.

\section{BASE}
ACID Systeme sind weit verbreitet und bekannt. Sie bringen eine sehr starke Semantik, verursachen aber in verteilten Systemen hohe Komplexität und Kosten. Konsistenz ist das oberste Ziel von ACID basierten Systemen. Die Verfügbarkeit der Systeme wird nicht garantiert. Im Gegenteil, es wird sogar bevorzugt keine Antwort zu geben, als eine falsche \cite{clusterBASE}. Die Skalierung solcher Systeme ist schwierig. Ist ein Teilsystem nicht erreichbar, leidet die Verfügbarkeit des gesamten Systems darunter \cite{BASE}. In einer idealen Welt wären Systeme gleichermaßen Skalierbar, Konsistent und Verfügbar. Solche Systeme kann es jedoch nach dem CAP Theorem (Consistency, Availability, Partition tolerance) nicht geben. Demnach können verteilte Systeme nur zwei von den drei Eigenschaften, Konsistenz, Verfügbarkeit und Partitionstoleranz erfüllen. Da Partitionstoleranz jedoch für jede Art der Skalierung benötigt wird, kann die Wahl nur zwischen Verfügbarkeit und Konsistenz fallen. Die Vorstellung Konsistenz gegen Verfügbarkeit zu tauschen erscheint häufig bedenklich. Es ist aber wichtig zu verstehen, dass eine Entscheidung für zwei Eigenschaften des CAP Theorem nie einen hundertprozentigen Ausschluss der dritten bewirkt.
Das heißt hoch konsistente Systeme verzichten nicht komplett auf die Verfügbarkeit und hoch verfügbare Systeme können einen gewissen Grad an Konsistenz bieten \cite{cap}. Die Herausforderung ist ein für die Anwendung möglichst passendes Verhältnis zwischen Konsistenz und Verfügbarkeit zu finden. So wie die Verfügbarkeit eines Systems in Prozent angegeben werden kann, gibt es auch Freiraum in der Gestaltung der Konsistenz. Die Autoren Paolo Viotti und Marko Vukolic unterscheiden in ihrem Artikel \textit{Consistency in Non-Transactional Distributed Storage Systems} insgesamt 50 Konsistenzarten \cite{consistency}. Demnach war in den 80iger Jahren nur die strong consistency bekannt. Sie fordert, dass Datenoperationen immer direkt zwischen der Anfrage vom und der Antwort zum Client durchgeführt werden müssen. Zusätzlich muss jeder Lesezugriff den tatsächlich letzten geschriebenen Wert zurückliefern. Am anderen Ende des Konsistenzspektrums befinden sich die weak und eventual consistency. Bei weak consistency, müssen Lesezugriffe nicht immer den tatsächlich letzten geschrieben Wert zurückliefern. Auch die Reihenfolge in der die Schreibeoperationen ausgeführt werden ist nicht vorgegeben. Mehrfache Lesezugriffe nacheinander müssen nicht immer das gleiche Ergebnis liefern. Eventual consistency ist etwas stärker als weak consistency. Auch hier liefert ein Lesezugriff nicht immer die letzten geschrieben Werte zurück. Wird aber für eine Weile kein Schreibzugriff durchgeführt, werden immer die gleichen Werte gelesen. Es tritt also irgendwann ein konsistenter Stand ein. Eventual consistency aus der Sicht einzelner Datenreplikas in einem verteilten System lässt sich mit drei Eigenschaften beschreiben:
\begin{itemize}
  \item \textbf{Eventual delivery:} Wenn ein Datenreplika eine Schreiboperation erreicht, wird diese Schreiboperation irgendwann bei allen Datenreplikas durchgeführt.
  \item \textbf{Convergence:} Alle Datenreplikas, welche die Schreiboperationen erhalten haben, werden irgendwann identisch sein.
  \item \textbf{Termination:} Alle Operationen werden ausgeführt und beendet.
\end{itemize}
Systeme die auf dem BASE Prinzip aufsetzen tauschen eine strong consistency gegen die eventual consistency ein und erreichen so eine hohe Verfügbarkeit. Wo ACID pessimistisch mit Schreibzugriffen umgeht und Konsistenz immer am Ende einer Transaktion fordert, ist BASE optimistisch und garantiert nur, dass irgendwann Konsistenz eintreten wird. Dieser Ansatz stützt sich darauf, dass dem Nutzer eine immer verfügbare Anwendung wichtiger ist, als das sie immer den korrekte Stand anzeigt. Vor allem wenn sich die Dauer des inkonsistenten Zustands auf wenige Sekunden beläuft. Durch diesen kleinen Abstrich bei der Konsistenz schaffen BASE Systeme aber viel Raum für Skalierung und Performance Steigerungen. Kommunikation zwischen den Teilsystemen die in einem ACID System zwingend nötig ist, kann in einem BASE System vernachlässigt oder auf einen besser passenden Zeitpunkt verschoben werden \cite{clusterBASE}. Client Anfragen können so nahezu immer und schnell beantwortet werden. Zusätzlich belastet das Starten mehrere parallel laufender Systeme das Netzwerk nicht so stark wie bei vergleichbaren ACID Systemen.

\section{Funktionsweise}
DFS basieren auf dem oben erklärten BASE Prinzip. Sie lockern die Konsistenz auf und ermöglichen dadurch gute Skalierbarkeit, Verfügbarkeit und Kosteneffizienz. Deswegen haben DFS einen besonderen Stellenwert in der Speicherung und Verarbeitung von Big Data und werden von vielen Firmen in diesem Bereich eingesetzt. Beispiele hierfür sind das Google File System, welches von Google selbst eingesetzt wird, Facebooks Haystack, das von der deutschen Telekom, CERN und Cisco verwendete DFS Ceph \cite{ceph} und das von Yahoo entwickelte und weit verbreitete Hadoop Distributed File System (HDFS).

Um sehr große Datenmengen verwalten zu können bedienen sich die meisten DFS dem sogenannten Object Storage \cite{cephPaper}. Traditionelle Filesysteme arbeiten mit dem File Storage bei dem die Dateien und die dazugehörigen Metadaten getrennt abgespeichert werden. Gerade bei sehr vielen Dateien wird die Verwaltung der Metadaten hier zu einem Flaschenhals \cite{filestorage}. Beim Object Storage hingegen werden Datei und die dazugehörigen Metadaten gemeinsam abgespeichert. Die Kombination aus Dateiinhalt und den Metadaten wird auch Objekt genannt. Anstelle auf der Datei zu arbeiten werden alle Operationen auf Ebene der Objekte durchgeführt. Lesen, Schreiben und Löschen funktioniert ähnlich wie bei einem traditionellen Filesystem, jedoch kann in einem Object Storage der Nutzer nicht bestimmen wie und wo das Objekt tatsächlich abgelegt wird. Der Zugriff auf einen Object Storage erfolgt auch meistens über eine abstrakte Schnittstelle wie zum Beispiel http. Die Speicherverwaltung fällt demnach komplett in den Aufgabenbereich der sogenannten Object Storage Devices (OSD). Mehrere OSDs können so einfach nebeneinander gestellt werden und durch eine http Schnittstelle wie ein einziges OSD angesprochen werden. Dafür ist lediglich eine zusätzliche Verwaltung der einzelnen OSDs notwendig. Auf diese Weise lässt sich ein Object Storage extrem gut skalieren und liefert eine nahezu endlose Speicherkapazität. Dadurch, dass der Nutzer einen Cluster an OSDs wie einen einzigen ansprechen kann, können die einzelnen OSDs sogar über Ländergrenzen hinweg verteilt sein. So kann zusätzlich Ausfallsicherheit gewährleistet werden \cite{osvideo}\cite{objectstorage}\cite{objectBasedStorage}. In der Regel bestehen Object Storage Systeme aus den OSDs selbst und einem Server, welcher diese OSDs verwaltet.

Nachdem die Grundlage eines DFS besprochen wurde, soll die konkrete Funktionsweise zweier DFS erklärt werden. Da das später entwickelte Konzept auf den Prinzipien der beiden DFS aufbaut, werden die Annahmen der Systeme sowie ein Lese- und Schreibzugriff auf das DFS ausführlich beschrieben. Die beiden betrachteten DFS sind das Google File System (GFS) und Haystack.
Das GFS wurde 2003 im Artikel \textit{The Google File System} vorgestellt \cite{GFS}. Demnach wurde es von Google entwickelt um die Analyse und Verwaltung ihrer extrem schnell wachsenden Datenmenge zu ermöglichen. Wie bei allen DFS wurde das GFS ganz im Sinne guter Skalierbarkeit, Verfügbarkeit, Verlässlichkeit und Leistung konzipiert. Aber auch andere Annahmen wurden bei dem Design des GFS berücksichtigt. So sind Ausfälle oder korrupte Daten bei einem System in dieser Größenordnung eher die Regel als die Ausnahme. Dateien sollen nachdem sie einmal geschrieben wurden nur noch gelesen werden. Ein Anhängen von Daten an bereits bestehenden Dateien soll aber möglich sein. Die zu verwaltenden Dateien sind in der Regel sehr groß, von einigen Megabyte bis zu mehreren Gigabyte. Demnach muss das System mit Lese- und Schreibzugriffen auf große Datenmengen zurecht kommen. Da kleine Dateien eher die Ausnahme sind, wird dem Lesen und Schreiben dieser keine besondere Aufmerksamkeit geschenkt. Das GFS soll auch mit den Anfragen von sehr vielen Clients umgehen können. Daraus folgt, dass der konkurrierende Zugriff auf eine Datei besonders berücksichtigt werden muss.  

Das GFS stellt den Clients ähnliche Operationen wie gewöhnliche File Systeme zur Verfügung. Dateien können geöffnet, geschlossen, gelesen und geschrieben werden und sind in Pfaden und Ordnern strukturiert. Zusätzlich können Daten an bestehende Dateien angehängt und Snapshots also Kopien von Dateien erzeugt werden. Das GFS selbst besteht dabei aus zwei Hauptkomponenten. Einem einzigen Master und mehreren Chunkservern. Dateien werden in 64 Megabyte große Chunks mit jeweils 64 Byte Metadaten aufgeteilt und auf den Chunkservern platziert. Der Master verwaltet dabei den Namensraum für die einzelnen Dateien und Chunks. Außerdem kennt er für jede Datei die dazugehören Chunks mit ihren Positionen und den Positionen ihrer Replikas. Für den schnellen Zugriff werden diese Informationen immer im Hauptspeicher des Masters vorgehalten. Um beim Starten des Masters oder im Fehlerfall die Zuweisungen von Datei zu Chunk wiederherstellen zu können, werden alle Änderungen an der Struktur und des Namensraums persistent in einem Operation Log gespeichert. Dieser agiert als Timeline und weißt jeder Änderung einen eindeutigen Zeitstempel zu. Ein einzelner Master ermöglicht ein sehr ausgeklügelte Chunk Platzierung auf den Chunkservern und vereinfacht das gesamte Design des Systems. Auf der anderen Seite, kann er aber auch zum Flaschenhals werden. Deswegen ist es sehr wichtig die Anfragen der Clients an den Master so gering wie möglich zu halten. Es werden keine Schreib- und Lesezugriffe auf Dateien direkt durch den Master durchgeführt.

\begin{figure}[htb]
  \centering
  \includegraphics[width=15cm]{img/4/ReadGFS.png}
  \caption[Lesezugriff auf einen Chunk in GFS]{Lesezugriff auf einen Chunk in GFS. Nachempfunden nach \cite{GFS}.}
  \label{readgfs}
\end{figure}

Der Ablauf eines Lesezugriffes ist in Abbildung \ref{readgfs} zu sehen. Der Client weiß, welchen Teil einer Datei er lesen möchte. Aus der bekannten Chunkgröße kann so der Index des benötigten Chunks errechnet werden. Auf eine Anfrage an den Master mit dem Dateinamen und dem Chunkindex antwortet dieser mit der Position des Chunks und all seiner Replikas. Der Client kann nun einen Chunk, der ihm am nächsten liegt auswählen und die Anfrage an den entsprechenden Chunkserver stellen. Dabei kann er auch übertragen welche Bytes er innerhalb des Chunks erhalten möchte. Der Chunkserver antwortet dem Client darauf hin mit den angefragten Daten. Da sich die Position der einzelnen Chunks in der Regel nicht ändert, können die Clients diese Information cachen und bei weiteren Lesezugriffen darauf zurückgreifen. Auch das entlastet den Master weiter.

\begin{figure}[htb]
  \centering
  \includegraphics[width=15cm]{img/4/writeGFS.png}
  \caption[Schreiben eines Chunks in GFS]{Ablauf eines Schreibvorgangs in GFS. Nachempfunden nach \cite{GFS}.}
  \label{writeGFS}
\end{figure}

Ein Schreibzugriff hingegen ist in Abbildung \ref{writeGFS} zu sehen. Um zu gewährleisten, dass alle Replikas eines Chunks irgendwann identisch sind, ist es wichtig, dass die Schreiboperationen in der gleichen Reihenfolge durchgeführt werden. GFS realisiert dies durch den Einsatz von Leases. Wenn ein Client eine Datei schreiben möchte, fordert er vom Master ein Lease auf einen Chunk an. Ein Lease hat eine Dauer von 60 Sekunden, kann aber bei Bedarf auch verlängert werden. Der Master gewährt dem Client ein Lease und sendet zusätzlich die Positionen des zu beschreibenden Chunks und seiner Replikas. In GFS gibt es von jedem Chunk standardmäßig drei Replikas. Einer dieser Chunks wird für die Dauer des Leases als Primary gekennzeichnet, die Anderen werden zu Secondaries. Darauf hin beginnt der Client die zu speichernden Daten zu den Replikas zu senden. Dabei ist die Reihenfolge in der dies geschieht egal. Es können auch erst die Secondaries und dann der Primary angesprochen werden. Sobald ein Replika Daten erhält, kann dieses die Daten zum nächsten weitersenden. Die Daten werden zunächst nur im Hauptspeicher gehalten und nicht persistent abgespeichert. Erst wenn die Daten bei allen Replikas angekommen sind, sendet der Client einen Schreibauftrag an den Primary Chunk, der die vorangegangenen Daten identifiziert. Der Primary Chunk verteilt diesen Schreibauftrag dann an alle Replikas welche daraufhin die Daten persistent ablegen. Jeder Secondary bestätigt dem Primary, dass die Daten erfolgreich geschrieben wurden oder teilt einen Fehler mit. Der Primary antwortet letztendlich dem Client und zeigt entweder den Erfolg der Operation an oder sendet die aufgetretenen Fehler. Möchte der Client während der Gültigkeit des Leases ein weiteres mal Daten schreiben, so muss keine Anfrage mehr an den Master getätigt werden und die Daten können direkt geschrieben werden. Dieser Prozess ermöglicht auch einen einfach Umgang mit einem zweiten Client der auf den gleichen Chunk schreiben möchte. Wenn der zweite Client nach einem Lease für den Chunk anfragt, erhält er als Antwort das gleiche Lease und demnach den gleichen Primary wie der erste Client. Auch der zweite Client beginnt die Daten in beliebiger Reihenfolge auf die Replikas zu streamen. Danach teilt er dem Primary den Schreibauftrag für die geschriebenen Daten mit. Da die Daten erst zum Ende des Leases persistent geschrieben werden, kann der Primary durch Kommunikation mit den Replikas sicherstellen, dass die Daten beider Clients in der richtigen Reihenfolge in den Replikas landen. So kann das Problem mehrerer konkurrierender Zugriffe leicht gelöst werden. Jedoch kann es bis zu 60 Sekunden dauern, bis ein Client den tatsächlich als letztes geschriebenen Wert eines Chunks auslesen kann.

Das Anfertigen von Replikas für jeden Chunk hat neben der Ausfallsicherheit auch noch performanztechnische Vorteile. Clients können immer das Replika lesen, das ihnen am nächsten ist. Wollen mehrere Clients die gleichen Daten lesen, können die Lesezugriffe auf alle bestehenden Replikas verteilt werden. Das GFS nutzt ausgeklügelte Mechanismen, um korrupte Replikas zu erkennen und zu ersetzen. Dabei werden Chunks mit besonders viel gelesenen Daten oder vielen korrupten Replikas am höchsten priorisiert. In regelmäßigen Heartbeat-Nachrichten teilen die Chunkserver dem Master mit, in welchem Zustand sich die einzelnen Chunks befinden. Ein weiteres Problem ist der einsame Master. Dieser stellt einen Single Point of Failure da. Um diesem entgegenzuwirken werden parallel zum Master mehrere Shadow Masters betrieben. Diese hinken dem Master immer einige Operationen hinterher, können aber über den Operation Log immer auf den korrekten Stand gelangen. Inkonsistenz herrscht innerhalb des GFS also nur wenn ein Chunk gelesen wird auf dem gerade ein Lease angefordert wurde und bis der Shadow Master, im Falle eines Ausfall des Masters, die letzten Operationen aus dem Operation Log durchgeführt hat.

Ähnlich wie Google entwickelte auch Facebook ein eigenes DFS. Ihr Ziel war es die unzähligen Bilder die Tag für Tag auf ihr soziales Netzwerk geladen werden zu speichern und zu verwalten. 2010 beschrieben einige Entwickler von Facebook in dem Artikel \textit{Finding a Needle in Haystack: Facebook's Photo Storage} ihr DFS Haystack \cite{haystack}. 2010 verwaltete dieses bereits etwa 20 Petabytes an Daten. Das System ist in der Lage mehr als eine Millionen Lesezugriffe in der Sekunde zu bedienen und einer Milliarde hochgeladenen Fotos in der Woche Herr zu werden. Neben Skalierbarkeit, Ausfallsicherheit, Verfügbarkeit und Performance wurde bei Haystack ein besonderes Augenmerk auf die Reduzierung der Metadaten pro Datei gelegt. Für die Speicherung von vier Bildern benötigt Haystack gerade einmal 40 Bytes an Metadaten. Gewöhnliche File Systeme würden dafür 536 Byte anlegen. Dadurch sollen alle Metadaten immer im Hauptspeicher gehalten werden und einen schnellen Zugriff ermöglichen. Um dies zu erreichen, wurde auf ein POSIX konformes File System verzichtet. In Haystack gibt es keine Pfade oder Ordner für die Metadaten gehalten werden müssen. Bilder werden eindeutig über Schlüssel identifiziert. Ansonsten wurde Haystack unter ähnlichen Annahmen wie das GFS entwickelt. Ausfälle sind an der Tagesordnung, es müssen sehr viele Daten gespeichert werden und das System wird von sehr vielen Clients genutzt. Die gewöhnliche Dateigröße unterscheidet sich hingegen von den Anforderungen des GFS. Die hochgeladenen Bilder sind meist nur einige Kilobyte groß und sind damit vergleichsweise klein. In Haystack werden deshalb mehrere Bilder in einer großen Datei zusammengefasst. Diese Datei wird Volume genannt. Zu Beginn werden leere Volumes mit einer festen Größe angelegt und nach und nach mit Bildern aufgefüllt. Zehn Terabyte an Speicher können so in 100 Volumes mit eine Größe von jeweils 100 Gigabyte aufgeteilt werden.

\begin{figure}
  \centering
  \includegraphics[width=5cm]{img/4/volume.png}
  \caption[Aufbau eines Volumes]{ Aufbau eines Volumes. Nachempfunden nach \cite{haystack}.}
  \label{volume}
\end{figure}
Haystack besteht aus drei Systemelementen. Dem Haystack Directory, Haystack Store und dem Haystack Cache. Im Folgenden wird auf diese ohne den Präfix Haystack Bezug genommen.
\begin{itemize}
  \item \textbf{Directory:} Die Aufgabe des Directorys ist die Verwaltung aller Volumes. Um die Replikation der Daten zu vereinfachen, fasst das Directory mehrere physikalische Volumes in ein logisches Volume zusammen. Beim Schreibvorgang wird nur noch ein logisches Volume referenziert und in alle darin enthaltenen physikalischen Volumes geschrieben. Zusätzlich kann das Directory Volumes auf read-only setzen, wenn diese ihre Kapazität erreicht haben oder zum Teil korrupt sind.
  \item \textbf{Cache:} Um viele Anfragen direkt aus dem Hauptspeicher beantworten zu können, werden alle Bilder die das System verlassen im Cache gespeichert. Dadurch werden vor allem kurz nach dem Hochladen eines neuen Fotos die Maschinen entlastet. Denn neue Bilder werden häufiger gelesen als Bilder, die weiter in der Vergangenheit hochgeladen wurden. 
  \item \textbf{Store:} Der Store speichert die tatsächlichen Bilddaten. Er besteht ähnlich wie die Chunkserver beim GFS aus mehreren Store Machines. Dabei kümmert sich eine Store Machine um mehrere Volumes welche jeweils mehrere Millionen Bilder enthalten können. Der Aufbau eines Volumes ist in Abbildung \ref{volume} dargestellt. Demnach steht am Anfang immer ein Superblock, welcher Informationen zur Anzahl der freien Blöcke und der Größe des Dateisystems beinhaltet \cite{wiki:superblock}. Gefolgt wird dieser von mehreren Needles, die jeweils ein Bild repräsentieren. Der Zugriff auf ein Bild erfolgt über Angabe des Volume-Schlüssels, einem Offset und der Größe der zu lesenden Daten. All diese Daten werden von der Store Machine im Speicher gehalten. Zusätzlich wird diese Information in einem Volume Index File gesichert. Das verhindert, dass beim Starten einer Store Machine alle Volumes gelesen werden müssen um die benötigten Informationen in den Speicher zu schieben. 
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{img/4/ReadHaystack.png}
  \caption[Ablauf beim Lesen eines Bildes.]{ Ablauf beim Lesen eines Bildes. Nachempfunden nach \cite{haystack}.}
  \label{readHaystack}
\end{figure}

In Abbildung \ref{readHaystack} sind die Schritte zum Lesen eines Bildes in Haystack schematisch dargestellt. Zunächst fragt der Client das Directory nach dem gewünschten Bild. Dieses wird durch einen eindeutigen Schlüssel identifiziert der die Form \textit{[logisches Volume ID, Bild ID]} aufweist. Über das logische Volume kann das Directory sehr leicht ein physikalisches Volume heraussuchen und die Adresse einer Store Machine, die dieses beherbergt an den Client senden. Der Client sendet jetzt erneut den Schlüssel des Bildes aber diesmal an die Adresse der Store Machine. Diese Anfrage trifft den Cache. Wurde das Bild zuvor bereits einmal ausgeliefert, kann dieser direkt mit den Bilddaten antworten. Befindet sich das Bild nicht im Cache so wird die Anfrage zur Store Machine weitergeleitet. Diese sucht mithilfe des Schlüssels für das Bild und des logischen Volumes das physikalische Volume, die Größe des Bildes und den benötigten Offset heraus. Damit kann das Bild aus dem physikalischen Volume gelesen und über den Cache zum Client geschickt werden. 

Wie in Haystack ein Bild gespeichert wird, ist in Abbildung \ref{writeHaystack} zu sehen. Im ersten Schritt fragt der Client das Directory nach einem beschreibbaren logischen Volume. Dieses antwortet mit der ID des logischen Volumes und den Adressen der Store Machines, die die im logischen Volume enthaltenen physikalischen Volumes beinhalten. Darauf hin vergibt der Client einen eindeutigen Schlüssel für das hochzuladende Bild. Der Schlüssel entspricht der oben erklärten Form. Gemeinsam mit dem Schlüssel übertragt der Client daraufhin die Bilddaten an alle vom Directory erhaltenen Store Machines. Diese hängen das Bild in den entsprechenden Volumes an und aktualisieren die Volume-Information in ihrem Hauptspeicher. Da Bilder in Volumes immer nur an die bestehenden Bilder angehängt werden können, ist eine Modifikation eines bereits gespeicherten Bildes nicht möglich. Um ein solches Bild zu überschreiben, sendet der Client ein neues Bild mit dem gleichen Schlüssel wie das zu überschreibende Bild. Bei der nächsten Anfrage nach diesem Bild wird die Store Machine immer das Bild zurückgeben, das den größten Offset innerhalb des Volumes besitzt, da dieses immer das aktuellste ist. Der Speicher der für das ungültig gewordene Bild benötigt wird, bleibt weiterhin belegt. Um die Netzwerkbandbreite und die Leistung der Festplatten optimal auszunutzen, bemüht sich Haystack immer mehrere Bilder auf einmal zu einem Volume hinzuzufügen. Dies wird ermöglicht, da Nutzer von Facebook in der Regel Bilder in Alben auf die Plattform laden.

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{img/4/WriteHaystack.png}
  \caption[Ablauf beim Schreiben eines Bildes in Haystack.]{ Ablauf beim Schreiben eines Bildes. Nachempfunden nach \cite{haystack}.}
  \label{writeHaystack}
\end{figure}

Damit der Speicher der durch überschriebene oder korrupte Bilder belegt wird nicht verloren ist, werden Volumes regelmäßig verdichtet. Dazu wird das Volume Bild für Bild kopiert und korrupte oder überschriebene Bilder übersprungen. Daraufhin wird das ursprüngliche Volume durch das neue ersetzt. Um auch bei korrupten Bildern eine gleichmäßige Replikation zu garantieren, kommunizieren Store Machines und Directory gelegentlich miteinander. Dabei teilen die Store Machines den Inhalt und den Zustand ihrer Volumes mit. Sind einige Bilddaten korrumpiert, kann das Directory dem betroffenen logischen Volume ein neues physikalisches Volume zuordnen und die Replikation starten. Inkonsistenzen treten bei Haystack nur auf, wenn ein Bild angefordert wird, während es noch auf die einzelnen Store Machines verteilt wird, oder wenn ein korruptes Bild angefragt wird, bevor dieser Fehler von der zuständigen Store Machine erkannt wurde. Werden Bilder überschrieben, kann es auch passieren das Clients noch das ursprüngliche Bild aus dem Cache erhalten bis dieses ungültig wird.

Haystack und das GFS ähneln sich in den Grundstruktur des Systeme sehr. Die eigentlichen Daten werden auf extra dafür vorgesehene OSDs gespeichert und ein Master kümmert sich um die Verwaltung dieser. Beide DFS ermöglichen einen effizienten Zugriff auf Daten der im besten Fall nur eine einzige Leseoperation von der Festplatte benötigt. Durch das Verteilen der Anfragen auf mehrere OSDs sind beide Systeme nahezu beliebig Skalierbar und liefern eine enorme Ausfallsicherheit. Dennoch gibt es nicht nur bei den gespeicherten Daten Unterschiede. Während das GFS sehr große Dateien verwaltet und diese sogar in Chunks aufteilt, bemüht sich Haystack darum mehrere Bilder in einer einzigen großen Datei, den Volumes zusammenzufassen. Demnach sind nicht nur die Schreibe- sondern auch die Lesezugriffe bei Haystack deutlich kleiner als bei dem GFS. Der Größe der Dateien ist auch die Tatsache geschuldet, dass sich ein Cache im GFS nicht lohnt. Ein weiterer Unterschied ist in der Anordnung der Dateien zu finden. GFS liefert ein fast POSIX konformes Dateisystem. Haystack hingegen verzichtet auf Pfade und Ordner komplett. Dadurch kann die Menge an Metadaten weiter reduziert werden. Dieser Schritt ist notwendig, da Haystack kleinere und damit mehr eigenständige Dateien verwaltet. 

\section{DFS zur Bewältigung der Anforderungen eines Buchungssystems}
Die Frage ist, ob sich ein DFS eignet um die Anforderungen und Probleme eines Buchungssystems zu bewältigen.
Im Kapitel \ref{bookingSystem} wurden die aktuellen und kommenden Herausforderungen, denen sich ein Buchungssystem stellen muss erörtert. Ob ein DFS diesen gewachsen ist, soll sich mit dem in diesem Kapitel erarbeiteten Informationen zeigen \cite{largeHadoop}.


\begin{itemize}
  \item \textbf{Wohldefinierte Schnittstelle:} Die Entwicklung einer wohl definierten Schnittstelle sowohl für bankeigene Prozesse als auch für Drittanbieter ist unabhängig von der genutzten Persistenzschicht. Die Realisierung eines Buchungssystems mit einem DFS als Grundlage sollte demnach für diese Anforderung kein Problem darstellen. Gegebenenfalls könnten einfache Anfragen sogar direkt die Schnittstelle des DFS nutzen und die Systeme weiter entlasten.

  \item \textbf{Skalierbarkeit:} Die grundlegende Systemarchitektur vieler OSDs und eines Masters, der diese verwaltet, gewährleistet eine einfache horizontale Skalierung. Speicherkapazität kann nahezu nach Belieben durch das Hinzufügen weiterer OSDs erhöht werden. Auch die Verarbeitung von Lese- und Schreibzugriffe profitiert von dieser Art der Skalierung. Dadurch, dass diese Anfragen nie direkt durch den Master gehen, wird die Last im Netzwerk sowie auf die OSDs verteilt.

  \item \textbf{Ausfallsicherheit:} Die in einem DFS abgelegten Daten werden immer auf mehreren OSDs abgelegt. Ausgeklügelte Algorithmen zur Platzierung der Replikas ermöglichen eine sehr hohe Fehlertoleranz und Ausfallsicherheit. Die geographische Verteilung der Replikas ermöglicht sogar eine Datenwiederherstellung, wenn ganze Rechenzentren versagen.

  \item \textbf{Verfügbarkeit:} Verfügbarkeit ist eine Kernkompetenz eines DFS. Wie alle anderen BASE basierten Systeme, lockern DFS die Konsistenz, um Anfragen zu beantworten, auch wenn Teile des Systems unerreichbar sind.

  \item \textbf{Kosteneffizienz:} DFS sind entwickelt worden um auf Standard-Hardware zu laufen. Die einfache horizontale Skalierung hält die Betriebskosten auch für große Systeme niedrig \cite{rdbmssuck}.

  \item \textbf{Leistung:} Viele Lesezugriffe auf ein DFS benötigen nur einen einzigen Zugriff auf die Festplatte. Informationen zu den Dateien und wo sie sich befinden werden in der Regel im Hauptspeicher vorgehalten. Replikas von Dateien und Chunks ermöglichen den Client den Zugriff auf die im am nächsten liegende Datei sowie eine Lastverteilung. 
\end{itemize}

Diese Punkte erfüllen DFS nahezu mühelos. Das Gewährleisten eines konsistenten Zustands bringt ein DFS hingegen ins Schwitzen. Bevor ein Zahlungsverkehr autorisiert wird, sollte sichergestellt sein, dass der Schuldner über die benötigten Zahlungsmittel verfügt. Für den Halter eines Girokontos ist es auch von großer Bedeutung, dass der in einer Webanwendung angezeigte Kontostand dem tatsächlichen entspricht. Auch die geringe Datenmenge, die bei einer einzelnen Buchung gespeichert werden muss ist nicht optimal. Informationen zu Gläubiger, Schuldner, Betrag, Währung und Beschreibung der Buchung kann in wenigen hundert Bytes abgebildet werden. DFS sind in der Regel auf die Speicherung größerer Dateien ausgelegt. Der Artikel \textit{An optimized approach for storing and accessing small files on cloud storage} zeigt an dem HDFS die Probleme der Verwaltung vieler kleiner Dateien auf \cite{hdfsSmallFiles}. Das HDFS wurde stark von dem GFS inspiriert und unterscheidet sich nur geringfügig \cite{hdfsGfs}. Für jede Datei wird ein eigener Eintrag im Master erstellt. Dieser soll für den effizienten Zugriff immer im Hauptspeicher liegen. Viele kleine Dateien benötigten demnach sehr viel mehr Hauptspeicher des Masters, als eine sehr große Datei mit der gleichen Menge an Inhalt. Auch die Beantwortung von Lesezugriffen auf viele kleine Dateien kann die Leistung eines DFS in die Knie zwingen. Abbildung \ref{hdfsSmallFiles} zeigt, wie lange das Herunterladen von 320 Megabyte dauert, je nachdem auf wie viele Dateien die Datenmenge verteilt ist. 5120 64 Kilobyte große Dateien herunterzuladen dauert vier und ein viertel mal so lange wie der Download von 40 acht Megabyte großen Dateien. Diese Verzögerung kommt vor allem durch die für jede Datei benötigte Anfrage an den Master \cite{hdfsSmallFiles}. Das Ablegen vieler kleiner Dateien benötigt demnach eine gesonderte Behandlung.



\begin{filecontents}{date5.dat}
date  value
1   15
2   9
3   6
4   5
5   4
6   3.8
7   3.7
8   3.6
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
xtick=data,
xmin=0, xmax=9,
ymin=0, ymax=16,
xticklabels={64KB,128KB,256KB,512KB,1MB,2MB,4MB,8MB},
xticklabel style = {font=\tiny},
xlabel={Dateigröße},
ylabel={Zeit zum Herunterladen in Sekunden},
]
\addplot table[x=date,y=value] {date5.dat};
\end{axis}
\end{tikzpicture}
\caption[Zeit zum Herunterladen unterschiedlicher Dateigrößen in HDFS]{Zeit zum Herunterladen unterschiedlicher Dateigrößen in HDFS bei gleichem Datendurchsatz (320 MB). Nachempfunden nach \cite{hdfsSmallFiles}.}
\label{hdfsSmallFiles}
\end{center}
\end{figure}

\chapter{Konzept}
Die Erhaltung eines konsistenten Zustandes sowie die geringe Datenmenge einer einzelnen Buchung scheinen nicht mit den Möglichkeiten eines DFS vereinbar. Wenn der Kontostand eines Kontos nicht sicher bestimmt werden kann, kann das Durchführen einer Buchung den Kontostand unter Null bringen. Das darf aber zum Beispiel bei einem Girokonto nicht passieren. Aber gelten diese Bedingungen denn für alle Kontoarten? In diesem Kapitel soll ein Konzept entwickelt werden, dass die Stärken eines DFS für ein Buchungssystem zugänglich macht. Dabei wird die Relevanz der strong consistency in Frage gestellt und eine Lösung für das Speichern der kleinen Datenmenge einer Buchung erarbeitet. Zu Letzt soll ein Designvorschlag für das Buchungssystem gemacht werden, der eine extreme Skalierung sowie ausreichende Konsistenz liefert.

\section{Konsistenz und Kontoarten}
\label{konsistenzKonten}
Strong consistency in einem Buchungssystem scheint unabdingbar. Wenn man sich aber wieder auf die Bedeutung der eventual consistency besinnt, wird klar, dass ein DFS auch irgendwann Konsistent ist. Und zwar immer wenn eine bestimmte Zeit keine Schreibzugriffe getätigt wurden. 
Inkonsistenzen treten also immer nur im Rahmen weniger Sekunden bis zu einer Minute nach dem Durchführen einer Buchung auf. Gibt es demnach keine Konten für die diese Konsistenz ausreichen würde? Problematisch sind alle Konten, bei denen regelmäßig Geld ein- und ausgeht und ein vorgegebener Saldo nicht unterschritten werden darf. Bei einem Girokonto zum Beispiel darf der Kontostand nicht unter Null gehen. Durch einen Dispositionskredit kann der Kontostand zwar negativ werden, aber auch nur einen bestimmten Betrag erreichen. Mit einer eventual consistency kann dieses Verhalten aber nicht garantiert werden. Wenn zum Zeitpunkt einer Buchung kein konsistenter Zustand vorliegt, kann nicht festgestellt werden ob das Konto noch über genügend Zahlungsmittel verfügt. Würden Buchungen immer nur zu bekannten Zeitpunkten eingehen, könnte man sicher gehen, dass zuvor ein konsistenter Zustand hergestellt wird. Auch Konten, die beliebig überzogen werden können, könnten abgebildet werden. Hier spielt es keine Rolle, ob ein bestimmter Betrag unterschritten wurde und Buchungen können einfach durchgeführt werden. Würde ein Girokonto auf diese Art funktionieren läge die Verantwortung ein bestimmtes Saldo des Kontos nicht zu unterschreiten beim Bankkunden mit dem Risiko einer starken Verschuldung. Ganz zu schweigen von der Gefahr, dass ein unberechtigter Zugriff auf ein solches Konto fatal für den Kunden sowie das Vertrauen in die Bank wäre.

Konten die eine starke Konsistenz also nicht zwingend benötigen, sollten nicht einfach von außerhalb der Bank zugänglich sein. Auch Konten die lediglich Zahlungen empfangen und selbst keine Zahlungen durchführen würden mit einer gelockerten Konsistenz zurecht kommen. Wo kein Geld abgehoben wird, kann der Kontostand auch nicht sinken. Aber auch wenige kontrollierte Abbuchungen sind mit eventual consistency realisierbar. Wenn Abbuchungen nur in sehr großen Abständen oder einmalig erfolgen, kann sichergestellt werden, dass zum Zeitpunkt der Buchung der Zustand konsistent ist. All diese Punkte treffen auf die Konten zu, die komplett in der Verantwortung der Bank liegen. Von den Passivgeschäften lässt sich die Termineinlage sehr gut mit einem DFS abbilden. In der Regel handelt es sich hierbei um eine einmalige Einzahlung des Bankkunden, bei der bekannt ist, wann der Betrag plus Zinsen wieder ausgezahlt wird. Für die Dauer der Termineinlage liegt die Verantwortung bei der Bank. Diese ist aber durchaus in der Lage Buchungen nur zu vorbestimmten Zeitpunkten durchzuführen und sicherzustellen, dass das Kontosaldo nicht unter Null fällt. Besonders gut eignen sich jedoch die Aktivgeschäfte für die Abbildung ohne strong consistency. Bei allen Formen des Ratenkredites, gibt es auf Seiten der Bank ein Konto, auf welches der Kreditnehmer seine Raten einzahlen muss. Dieses Konto erfährt nur einen Geldzufluss und zum Ende des Kredits wird der Gesamtbetrag auf ein weiteres Konto der Bank gebucht. Für jede Termineinlage und für jeden Ratenkredit, wird also ein Konto angelegt, welches relativ wenige Kontobewegung erfährt und dessen Verwaltung in den Händen der Bank liegt. Wie Abbildung \ref{ratenkredite} zeigt ist die Anzahl der Konten die allein für die Ratenkredite angelegt werden beachtlich. Auch wenn jedes dieser Konten an sich die starke Konsistenz der relationalen Datenbanken nicht benötigt, liegen sie in den gleichen Systemen wie die Girokonten und beanspruchen Speicher und Leistung der relationalen Datenbanken. Alleine die Speicherung der Konten für Termineinlagen und Ratenkredite in einem DFS könnte den Banken Kosten sparen und Druck von den Buchungssystemen nehmen.

\begin{filecontents}{date6.dat}
date       value
2008-01-01  6909
2009-01-01  7611
2010-01-01  7272
2011-01-01  7183
2012-01-01  7697
2013-01-01  7737
2014-01-01  7434
2015-01-01  7442
\end{filecontents}


\begin{figure}[htb]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahre},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anzahl der Kreditverträge in Tausend},
date ZERO=2008-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date6.dat};
\end{axis}
\end{tikzpicture}
\caption[Anzahl der Ratenkreditverträge in Deutschland]{Anzahl der Ratenkreditverträge in Deutschland. Nachempfunden nach \cite{ratenkredite}.}
\label{ratenkredite}
\end{center}
\end{figure}

\section{Annahmen}
Das hier entwickelte Konzept soll aber von der Skalierbarkeit und Leistung auch in der Lage sein alle anderen Kontoarten abzudecken. Eine Lösung um die benötigte Konsistenz zu gewährleisten soll im weiteren Verlauf des Kapitels erarbeitet werden. Zunächst sollen einige Annahmen getroffen werden, nach denen ein DFS und das darauf aufgesetzte Buchungssystem entwickelt wird. Die wichtigste Aufgabe eines DFS in einem Buchungssystem ist das persistente Abspeichern aller Buchungen die für alle Konten eingehen. Eine Buchung besteht im Rahmen dieser Arbeit aus dem Namen des Empfängers und des Senders, dem IBAN und dem BIC des Empfängers, dem zu übermittelnden Geldbetrag sowie der Währung, einem Ausführungsdatum und einem Freitextfeld für eine Beschreibung der Buchung. Die Datenmenge einer Buchung sollte daher ein Kilobyte nicht überschreiten. Das DFS muss mit sehr vielen von diesen Buchungen umgehen können, auch wenn mehrere für ein Konto nahezu parallel eintreffen. Zum Beispiel wenn eine Überweisung und ein Dauerauftrag auf den gleichen Zeitpunkt fallen. Anfragen werden durch die Schnittstellen zu Online- und Mobile-Banking sowie der PSD2 von einer Vielzahl an Clients gestellt, wobei zu erwarten ist, dass die Mehrheit davon Lesezugriffe sind. Dabei wird selten nur eine konkrete Buchung ausgelesen und häufiger alle Buchungen eines gewissen Zeitraums für ein Konto angefordert. Beispielsweise alle Buchungen im letzten Monat oder alle Buchungen im letzten halben Jahr. Damit die Bank ihren Funktionen nachgehen und dem Kunden ein gutes Nutzererlebnis bieten kann, muss eine hohe Verfügbarkeit des Buchungssystems gewährleistet sein. Können keine Buchungen durchgeführt werden ist damit direkt ein Gewinnverlust und Imageschaden der Bank verknüpft. Auch wenn Teile des Systems nicht erreichbar sind, muss das Buchungssystem weiter funktionieren können. Bei Systemen, die mehrere Milliarden Buchungen im Jahr bearbeiten, sind Ausfälle und Fehler an der Tagesordnung. 
GFS eignet sich nicht für das persistente Abspeichern der Buchungen, da es auf größere Dateien ausgelegt ist. Für jede Datei wird ein Chunk angelegt, welcher 64 Byte im Hauptspeicher des Masters belegt. Wenn der Master über 16 Gigabyte Hauptspeicher verfügt und für jede Buchung ein Chunk angelegt werden muss, könnten so gerade einmal 250 Millionen Buchungen abgelegt werden bevor die Grenzen des Masters erreicht wären. Nach Abbildung \ref{bargeldlos} wäre das nur ein Bruchteil der Buchungen die im Jahr 2011 durchgeführt wurden. Haystack passt etwas besser auf die Anforderungen eines Buchungssystems an ein DFS. Die in Haystack verwalteten Dateien sind häufig auch nur wenige Kilobyte groß. Jedoch bietet Haystack keine Möglichkeit mehrere Bilder auf einmal auszulesen. Jeder Zugriff erfordert den exakten Schlüssel des Bildes. Ein Auslesen aller Bilder die in einem gewissen Zeitraum hochgeladen wurden ist nicht möglich. Auch der Cache ist für ein Buchungssystem nicht relevant. Buchungen werden immer nur vom Besitzer des entsprechenden Kontos angefragt. Das eine Buchung mehrmals nacheinander benötigt wird, ist deshalb unwahrscheinlich.

\section{DFS für kleine Daten}
\label{oneOnlyDFS}
Ein DFS, das für ein Buchungssystem geeignet ist muss in der Lage sein viele kleine Dateien beziehungsweise Buchungen abzulegen und auszulesen. Wobei besonders das Auslesen mehrerer Buchungen am Stück relevant ist. Auch die Zeit in der das System inkonsistent ist, soll möglichst gering ausfallen. Wie bereits beschrieben eignen sich sowohl GFS als auch Haystack nicht optimal für diesen Anwendungszweck. Durch den gezielten Einsatz einiger Techniken der beiden DFS sowie von Ansätzen aus mehreren Artikeln, ist die Entwicklung eines DFS, welches all diese Anforderungen erfüllt jedoch durchaus möglich \cite{hdfsSmallFiles}\cite{smallfilesObjectStorage}\cite{hadoopSmallFiles2}\cite{smallFilesHDFS3}\cite{smallFilePrefetching}.

Der generelle Aufbau des DFS besteht wieder aus mehreren OSDs, die die Daten verwalten und einem Master, der wiederum die OSDs verwaltet. Viele kleine Dateien sollen aufgrund ihres logischen Zusammenhangs gruppiert und zusammengefügt werden. Dateien hängen logisch zusammen, wenn sie sich ein gemeinsames Überthema teilen. In einem traditionellen File System ist das häufig der Fall wenn Dateien im gleichen Ordner liegen. Beispiele sind alle Bilder die in einer Stadt aufgenommen wurden, oder alle Musikstücke eines Künstlers oder eben auch alle Buchungen eines Kontos die in einem gewissen Zeitraum getätigt wurden. Dateien die einen solchen logischen Zusammenhang besitzen, werden häufig gemeinsam angefragt. Daher sollen hier mehrere Buchungen in eine gemeinsame Datei eingefügt werden. Diese Datei ist vergleichbar mit einem Volume bei Haystack. Um Verwirrung zu vermeiden wird sie im folgenden jedoch Bucket genannt. Ein Bucket hat immer eine feste Größe und wird gefüllt durch das Einfügen von Daten. Da das DFS mit sehr kleinen Daten umgehen soll, ist eine Größe von 16 Megabyte pro Bucket sinnvoll. Geht man von etwa einem Kilobyte an Daten pro Buchung aus, so kann ein Bucket 16000 Buchungen halten. Um einen Bucket innerhalb eines Jahres zu Füllen müssten über 43 Buchungen am Tag eingehen. Eine Vielzahl an Konten lässt sich also mit nur einem einzigen Bucket abdecken. Aber auch Konten mit sehr vielen Buchungen wie zum Beispiel von einem Online Händler können auf nur wenige Buckets verteilt werden. Die Verwaltung von mehreren kleinen Einträgen in einer großen Datei ist zwingend erforderlich, um den Hauptspeicher des Masters zu entlasten. Dieser kennt immer nur die Adresse des Buckets und nicht den gesamten Inhalt. Ein weiterer Vorteil der Gruppierung von logisch zusammenhängenden Daten in einer großen Datei ist, dass Lesezugriffe nur einen einzigen Zugriff auf die Festplatte benötigen um  in den Hauptspeicher zu laden und die Anfrage des Clients zu beantworten. Nur wie kann der Master wissen in welchem Bucket sich welche Daten befinden? Alle abgelegten Daten erhalten einen streng aufsteigenden, acht byte großen Schlüssel. Dieser Schlüssel gibt den Zeitpunkt der Erstellung bis auf Nanosekunden genau wieder. Die Adresse eines Buckets beinhaltet immer den Schlüssel und somit den Zeitpunkt der ältesten Daten die innerhalb des Buckets abgelegt wurden. Werden weitere Daten in diesen Bucket eingefügt, so werden diese streng aufsteigend eingegliedert. Bevor ein neuer Bucket angelegt wird, muss immer erst der vorherige komplett aufgefüllt werden. Werden nun Daten mit einem Schlüssel angefordert, muss nur der Bucket gefunden werden, dessen Adresse entweder gleich dem gegebenen Schlüssel ist, oder am nächsten vor dem gegebenen Schlüssel liegt. Wenn bekannt ist, in welchem Bucket sich die benötigten Daten befinden, kann dieser gelesen und der entsprechende Abschnitt zurückgeliefert werden. Damit aber nicht bei jeder Anfrage der komplette Bucket durchlaufen werden muss, wird wie bei Haystack ein Bucket Index File angelegt. Im Gegensatz zu Haystack wird darin aber nicht jeder einzelne Eintrag festgehalten sondern nur einige Checkpoints abgespeichert. Dies geschieht immer wenn ein gewisser Füllstand erreicht wurde. Standardmäßig soll ein 16 Megabyte großer Bucket mindestens 16 mal unterteilt werden. Der Aufbau eines Buckets und des dazugehörigen Bucket Index Files ist schematisch in Abbildung \ref{bucket} zu sehen. Der Inhalt des Bucket Index Files wird von den OSDs im Hauptspeicher gehalten. Das Einfügen nur weniger Checkpoints ermöglicht entgegen der Indizierung jedes einzelnen Eintrags, dass sehr viele Buckets auf einem OSD gehalten und effizient verwaltet werden können. Ein Eintrag im Bucket Index File speichert den acht Byte großen Schlüssel, plus drei Byte für den Offset innerhalb des Buckets. Die Größe der Daten muss nicht mit abgelegt werden. Jeder Eintrag im Bucket beginnt mit seinem Schlüssel gefolgt von zwei Byte für die Größe der abgelegten Daten. Beim Suchen bestimmter Daten mittels eines Schlüssels wird immer von dem Checkpoint gestartet, der dem Schlüssel am nächsten liegt und von dort aus Eintrag für Eintrag gelesen. Für den Zugriff auf beliebige Einträge müssen bei einer durchschnittlichen Dateigröße von einem Kilobyte also bis zu 1000 Einträge gelesen werden, bis der richtige gefunden ist. Dieser Suchvorgang fällt aber beim Lesen mehrerer Einträge am Stück nicht mehr besonders ins Gewicht.

\begin{figure}
  \centering
  \includegraphics[width=10cm]{img/5/bucket.png}
  \caption[Aufbau eines Buckets und dem Bucket Index Files]{ Aufbau eines Buckets (links) und eines Bucket Index Files (rechts).}
  \label{bucket}
\end{figure}

Um Buckets effizient zu Nutzen muss dem DFS aber auch eine Möglichkeit geboten werden logisch zusammenhängende Daten zu erkennen. Die einfachste Möglichkeit wäre die Verwendung von Ordnern und Pfaden. Alle Dateien die sich innerhalb eines Ordners befinden werden gemeinsam in einen Bucket abgelegt. Die Verwaltung von Ordnern und Pfaden beansprucht aber Logik und Hauptspeicher des Masters. Die Möglichkeit Dateien innerhalb der Ordner zu verschieben, Ordner umzubenennen oder neue Unterordner anzulegen ist aber vielleicht gar nicht nötig. Eine Technik die den logischen Zusammenhang in den Schlüssel von Daten fest kodiert nennt sich Namespace Flattening. Im Falle von Buchungen für Konten bietet es sich an, im ersten Teil des Schlüssels BIC und IBAN aneinanderzuhängen, gefolgt von dem zweiten Teil des Schlüssels, der einem acht Byte Timestamp entspricht. Alle Daten, die beim Ablegen einen identischen ersten Teil des Schlüssels besitzen, landen im gleichen Bucket. Zusätzlich kann nur nach dem ersten Teil des Schlüssels gefragt werden, um alle Buckets zu erhalten, die auf den Schlüssel bezogen logisch zusammenhängende Daten beinhalten. Durch das Namespace Flattening kann wichtiger Hauptspeicher auf dem Master gespart werden. Dieser benötigt mit diesem Ansatz ingesamt 65 Byte Hauptspeicher für einen Bucket. Für BIC und IBAN 44 Byte, für die Adresse der drei Replikas des Buckets jeweils vier Bytes und für den zweiten Teil des Schlüssels, dem Timestamp acht Bytes \cite{bic}\cite{iban}.

Die oben beschriebenen Ansätze ermöglichen das Abspeichern vieler kleiner Daten und das effiziente Auslesen mehrerer zusammenhängenden Daten. Ein weiteres Problem, dass sehr viele kleine Dateien mit sich bringen, ist die Last der Anfragen auf den Master. Für jeden Lesezugriff muss der Master nach der Position der Datei gefragt werden. Zum Großteil kann dieses Problem bereits durch die Gruppierung mehrerer Daten in eine große Datei mit streng aufsteigenden Schlüssel behoben werden. Wenn aber Lesezugriffe stattfinden, die über die Grenzen von Buckets hinausgehen, ist der Master aktuell immer ein weiteres mal involviert. Dieser Zugriff auf den Master kann jedoch durch das sogenannte Prefetching verhindert werden. Prefetching bezeichnet gewöhnlich das clientseitige Anfragen und Speichern von Daten, bevor sie eigentlich benötigt werden. Das setzt aber voraus, dass der Client genau weiß, welche Daten er überhaupt anfragen könnte. Das ist in einem Buchungssystem aber nicht der Fall. Der Client ist sich nicht bewusst, welche Buchungen in welchen Buckets liegen. Daher soll das hier entwickelte DFS ein serverseitiges Prefetching implementieren. Bei jeder Anfrage nach einzelnen Buchungen oder einem zeitlich zusammenhängenden Block an Buchungen, soll der Master nicht nur die Adresse des Buckets mit den angefragten Daten zurückliefern, sondern auch die Adresse der nächsten Buckets zur Verfügung stellen. Für weitere Lesezugriffe muss dann nicht mehr der Master gefragt werden. Da BIC und IBAN hart in den Schlüssel für Daten einkodiert werden, werden sich die Adressen der Buckets nur ändern, wenn diese korrupt werden sollten. Solange das nicht der Fall ist, kann der Client diese Information auch im Cache behalten.

Nachdem die generelle Struktur des DFS erklärt wurde, werden nun die Schritte die zum Schreiben und Lesen von Daten nötig sind aufgezeigt.
Das Schreiben von Daten erfolgt nahezu identisch wie bei dem GFS. Der Client fragt mit dem ersten Teil des Schlüssels also BIC und IBAN beim Master nach einem beschreibbaren Bucket. Jeder Bucket besitzt standardmäßig drei Replikas. Der Master gewährt einen Lease auf einen Bucket und sendet die Adresse der Replikas zum Client. Einer dieser Replikas wird Primary die anderen Secondaries. Der Client beginnt in beliebiger Reihenfolge auf die Replikas zu streamen und sendet erst danach den Schreibauftrag zum Primary. Es wird immer zunächst der zweite Teil des Schlüssels und die Größe der Datei geschrieben, gefolgt von den eigentlichen Nutzdaten. Wird eine gewisse Füllmenge des Buckets erreicht, wird der geschriebene Eintrag im Bucket Index File als Checkpoint abgelegt und im Hauptspeicher des zuständigen OSDs hinzugefügt. Da es sich hier um kleine Datenmengen handelt, wird die Dauer eines Leases auf zehn Sekunden festgelegt. Durch diese Art des Schreibvorganges ergeben sich die gleichen Vorteile wie beim GFS. Die Reihenfolge in der Daten geschrieben werden ist immer in allen Replikas der Buckets die gleiche, und der Zugriff durch einen zweiten Client stellt kein Problem für die Konsistenz dar.

Ein Lesezugriff läuft sehr ähnlich wie bei Haystack ab. Neben dem Lesen eines konkreten Eintrages über einen Schlüssel, soll aber auch das Lesen mehrerer Einträge die nach einem bestimmten Zeitpunkt abgelegt wurden möglich sein. Beim lesen eines einzelnen Eintrags über einen Schlüssel, sendet der Client den Schlüssel an den Master. Dieser findet über den ersten Teil des Schlüssels heraus in welchen Buckets sich die Daten potentiell befinden können. Über den zweiten Teil des Schlüssels kann der genaue Bucket identifiziert werden. Der Master antwortet dem Client mit der Adresse des Buckets und seinen Replikas für die angefragten Daten und zusätzlich mit der Adresse der zwei am nächsten anliegenden Buckets. Der Client kann jetzt mit dem Schlüssel beim entsprechenden OSD anfragen. Das OSD sucht in seinem Hauptspeicher für den zuständigen Bucket nach dem Checkpoint, der gleich ist oder am nächsten vor dem gegebenen Schlüssel liegt. Von diesen Checkpoint aus beginnt die Suche nach dem Eintrag. Der gefundene Eintrag wird zum Client zurückgesendet. 
Um alle Einträge, die in einem gewissen Zeitraum passiert sind zu lesen, stellt der Client seine Anfrage an den Master mit einen bestimmten BIC und IBAN und einem Datum von und gegebenenfalls ein Datum bis zudem alle Einträge zurückgeliefert werden sollen. Der Master sucht auch hier wieder anhand der Schlüssel der Buckets die entsprechenden Adressen heraus und sendet sie gemeinsam mit den nächsten weiteren Buckets an den Client. Dieser fragt wieder mit BIC, IBAN und den Daten das zuständige OSD an. Der Suchvorgang wird wieder in Gang gesetzt doch anstelle beim passenden Eintrag zu stoppen, werden alle Einträge, deren Schlüssel nach dem gegeben Datum erstellt wurde als Block zurückgeliefert. Die Daten werden entweder bis zum Ende des Buckets oder bis zum gegebenen Enddatum zum Client gestreamt.
Hat ein Bucket nicht alle benötigten Daten enthalten, kennt der Client über das Prefetching ja bereits die anliegenden Buckets und kann weitere Daten anfragen.

Ein DFS nach dem hier beschrieben Design ermöglicht das Ablegen sehr vieler kleiner Datenschnippsel, sofern sie in irgendeiner Form einen logischen Zusammenhang erfüllen. Außerdem ist das Auslesen der so zusammenhängenden Daten sehr performant und kann häufig mit nur einem Festplattenzugriff realisiert werden. Ein OSD benötigt für einen 16 Megabyte großen Bucket gerade einmal 176 Byte Hauptspeicher. Diese kommen aus den elf Bytes pro Checkpoint und den 16 Einträgen pro Bucket zustande. Wenn die OSDs über 16 Gigabyte Hauptspeicher verfügen, können sie so über 90 Millionen Buckets verwalten bei den jeder Bucket potentiell ein Konto komplett abbildet. Die Adresse der Buckets benötigt auf dem Master sogar nur 65 Byte Hauptspeicher. Wenn auch der Master über 16 Gigabyte Hauptspeicher verfügt, ermöglicht er den schnellen Zugriff auf über 245 Millionen Buckets mit beinahe vier Billionen Buchungen. Das entspricht einem Festplattenspeicher von etwa vier Petabyte. Dieses DFS sollte in der Lage sein alle Buchungen die bei einer Bank eingehen effizient und schnell zu speichern und auszuliefern. Inkonsistenzen treten nur während der Dauer eines Leases auf, welches zunächst auf zehn Sekunden limitiert wurde. Ist Konsistenz dringend erforderlich, kann die Ausgabe von Leases gestoppt werden und nach zehn Sekunden tritt ein Konsistenter Zustand ein. Generell wäre ein solches DFS für die Abbildung der Kontoarten die nach Kapitel \ref{konsistenzKonten} eine gelockerte Konsistenz ertragen könnten bereits völlig ausreichend. Problematisch ist jedoch, dass zum Feststellen des Kontostandes immer alle Buchungen eines Kontos gelesen und ausgewertet werden müssten. Dieses Problem und die noch zu schwache Konsistenz um alle Kontoarten zu realisieren lässt sich aber durch ein geschicktes Design der Anwendungsschicht des Buchungssystem lösen.

\section{Design des Buchungssystems}
Die Anwendungsschicht des Buchungssystems stellt eine Schnittstelle für Clients und Anwendungen, die das Buchungssystem nutzen zur Verfügung. Sie ist die einzige Möglichkeit um Daten in das DFS zu schreiben und Daten auszulesen. Daher ist die Anwendungsschicht des Buchungssystems für die Validierung von eingehenden Buchungen, sowie der Bereitstellung von erweiterten Datenmanipulationen zuständig. Für Termineinlagen und Ratenkrediten, würde es ausreichen, wenn Anfragen an das Buchungssystem direkt vom DFS beantwortet werden. Die nur wenigen Kontobewegungen bei diesen Kontoarten erzeugen nur eine sehr geringe Menge an Daten und Konsistenz kann bei Auszahlung oder Ende des Kredites leicht sicher gestellt werden. Konten bei denen beliebig viele und zu beliebigen Zeitpunkten Buchungen eingehen, haben aber mit der Inkonsistenz des DFS noch ein Problem. Dabei ist eigentlich egal ob zum Zeitpunkt zu dem eine Buchung eingeht bereits alle anderen Buchungen korrekt ausgelesen werden können. Es ist nur relevant, dass der Kontostand bereits alle vorangegangenen Buchungen berücksichtigt. Die einzige Information die für Girokonto und Co. also immer Konsistent sein muss, ist der tatsächliche Kontostand. Dieser lässt sich mit einem einzigen Eintrag pro Konto effizient erfassen. Bei jeder Buchung die dem Konto Zahlungsmittel zu- oder abführt, wird dieser Eintrag entsprechend angepasst. Um die benötigte Konsistenz zu gewährleisten, eignet sich für die Verwaltung der Kontostände ein ACID basierte relationale Datenbank. Diese Datenbank hält nur eine einzige Tabelle die nur so viele Einträge hat, wie Konten im Buchungssystem. Diese Datenmenge ist für ein relationales Datenbanksystem leicht zu bewältigen. Ein Eintrag in dieser Tabelle besteht aus BIC, IBAN, dem aktuellen Kontostand und dem Datum der letzten schreibenden Buchung. Bei jeder neuen Buchung wird die Betrag und das Datum für das Konto entsprechend angepasst. Erst danach wird die Buchung in das DFS abgelegt. Schlägt der erste oder der zweite Schritt fehl, muss sichergestellt werden, dass weder im DFS noch in der Kontostand Datenbank die Buchung vermerkt wird und der Client einen Fehler vom Buchungssystem erhält. Die zusätzliche Kontostands Datenbank ermöglicht es festzustellen ob ein Konto über die benötigten Zahlungsmittel für eine Buchung verfügt. Außerdem ist über das Datum feststellbar, ob bereits alle Buchungen korrekt geschrieben wurden oder noch Buchungen verarbeitet werden. Um den Kontostand auszulesen, müssen auch nicht mehr alle Buchungen verrechnet werden, sondern es genügt ein Zugriff auf die Kontostands Datenbank.

Kombiniert man diese Idee eines Buchungssystem mit dem in Kapitel \ref{oneOnlyDFS} entwickelten DFS so läuft das Schreiben einer Buchung nun wie folgt ab. Die Buchung wird über eine Schnittstelle an das Buchungssystem geliefert. Dieses führt über die Kontostand Datenbank einige Validierungen aus. Es wird geprüft ob das Konto existiert, und ob es über die nötigen Zahlungsmittel verfügt um die Buchung zu erfüllen. Ist dem so, so wird der Eintrag in der Kontostands Datenbank entsprechend angepasst und die Buchung weiter an das DFS gegeben. Dieses führt ebenfalls die Schritte zum Abspeichern der Buchung durch. Erst wenn die Buchung sowohl von der Kontostands Datenbank als auch vom DFS korrekt erfasst wurden, wird dem Client der Erfolg der Operation übermittelt.

Beim Lesen von Buchungen für ein Konto wird zunächst ermittelt ob auch der Kontostand übertragen werden soll. Wenn dem so ist, wird er aus der Kontostands Datenbank ausgelesen. Darauf hin wird die Anfrage an das DFS weitergeleitet und die Antwort mit dem Kontostand kombiniert und zum Client ausgeliefert. Zusätzlich kann der Client informiert werden, ob aktuell noch Buchungen ausstehen, die sich aktuell in der Verarbeitung befinden.

\section{Skalierbarkeit}
Der Sinn dieses Konzepts für ein DFS und des darauf aufsetzenden Buchungssystem, war die Erfüllung einer besonders einfachen und beliebig großen Skalierbarkeit. Besonders Lesezugriffe sollen immer schnell beantwortet werden können. Da der Fokus vor allem auf diesen Punkten lag, hat das Konzept nicht den Anspruch in allen Aspekten vollständig zu sein. 

Das hier entwickelte Buchungssystem und das dazugehörige DFS lässt sich auf mehrere Arten skalieren. Die Anwendungsschicht ist zunächst einmal komplett unabhängig vom DFS und der Kontostand Datenbank. Demnach lassen sich beliebig viele Instanzen davon parallel hochfahren. Je nach Anzahl der Anfragen, kann dies sogar lastbezogen geschehen. Wenn sehr viele Anfragen auf das Buchungssystem einprasseln, lassen sich mehrere Anwendungsschichten hochfahren, wenn der die Anzahl der Anfragen wieder sinkt, können die zusätzlichen Systeme einfach wieder entfernt werden. Die Skalierung der Kontostands Datenbank kann entweder über Replikation erfolgen, also mehrere Systeme oder über eine klare Aufteilung der Zuständigkeit auf mehrere Datenbanksysteme. So dass ein Datenbanksystem immer nur für einen bestimmten BIC Bereich zuständig ist und nur Anfragen beantwortet, die diesen Bereich betreffen. Das DFS lässt sich wie bereits beschrieben, einfach durch das hinzufügen weiterer OSDs oder das aufstocken des Hauptspeichers des Masters skalieren. Dabei ist zu erwähnen, dass die Skalierung des DFS, sowie die Skalierung der Anwendungsschicht der Buchungssysteme komplett unabhängig von einander statt finden kann. Die Anwendungsschicht braucht keine Anpassungen vorzunehmen um die gesteigerte Kapazität des DFS zu nutzen. Demnach wäre es auch denkbar das DFS auf Systemen von Drittanbietern auszulagern. Sollte die Möglichkeiten des Skalierung des DFS an ihre Grenzen stoßen, kann ähnlich wie bei der Kontostands Datenbank die Zuständigkeit für die Buchungen anhand des BICs auf mehreren DFS aufgeteilt werden.

\chapter{Implementierung}
\section{SeaweedFS}
Einsatz von SeaweedFS und wieso?
Wie wird er hier konkret Konfiguriert
\section{golang}
Wieso habe ich golang verwendet? Bezug zu SeaweedFS 
\section{Schnittstelle zu SeaweedFS}
Wieso wurde eine neue Schnittstelle geschrieben?
Worauf war zu achten? Nutzung des Filers (Distributed Filer)
\section{Bibliothek zur Abbildung von Buchungen}
Einführen der fehlenden Abstraktionsschicht für die spätere Anwendung
\section{RESTful Webservice}
Implementierung einer API zum leichten Anlegen und Lesen von Buchungen. Besonderes Augenmerk auf Modularisierung.

\chapter{Evaluierung}
Mal sehen was hier später steht.

\chapter{Ausblick}
Ich bin sehr gespannt.

\begin{lstlisting}[label=lst:java,
				   language=java,
				   firstnumber=1,
				   caption=Beispiel für einen Quelltext]				   

public void foo() {				   
	// Kommentar
}
\end{lstlisting}

\chapter{Zusammenfassung}


\backmatter
%%%%%%%%%%%%%%%%%%%
%% create figure list
%%%%%%%%%%%%%%%%%%%

\listoffigures
\addcontentsline{toc}{chapter}{Verzeichnisse}			

%%%%%%%%%%%%%%%%%%%
%% create tables list
%%%%%%%%%%%%%%%%%%%
\listoftables

%%%%%%%%%%%%%%%%%%%
%% create listings list
%%%%%%%%%%%%%%%%%%%
%\lstlistoflistings
%\addcontentsline{toc}{chapter}{Listings}				

\printbibliography
\addcontentsline{toc}{chapter}{Literatur}				

%%%%%%%%%%%%%%%%%%%
%% declaration on oath
%%%%%%%%%%%%%%%%%%%

\addchap{Eidesstattliche Erklärung}

Hiermit versichere ich, dass ich die vorgelegte Bachelorarbeit selbstständig verfasst und noch nicht anderweitig zu Prüfungszwecken vorgelegt habe. Alle benutzten Quellen und Hilfsmittel sind angegeben, wörtliche und sinngemäße Zitate wurden als solche gekennzeichnet.

\vspace{20pt}
\begin{flushright}
$\overline{~~~~~~~~~~~~~~~~~\mbox{\BaAuthor, am \today}~~~~~~~~~~~~~~~~~}$
\end{flushright}
\end{document}


