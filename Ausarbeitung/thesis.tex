\documentclass[12pt,oneside,a4paper,parskip]{scrbook}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[ngerman]{babel}
\usepackage{floatflt} 
\usepackage{subfigure}
\usepackage[pdftex]{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{color}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{nicefrac}
\usepackage{pdfpages}
\usepackage{hyphenat}
\usepackage{float} 
\usepackage{pdflscape}
\usepackage{subfigure}
\usepackage{pdfpages}  
\usepackage[verbose]{placeins} 
\usepackage[nouppercase,headsepline,plainfootsepline]{scrpage2}
\usepackage{listings}	
\usepackage[table,xcdraw]{xcolor}	
\usepackage{color}			
\usepackage{caption}		
\usepackage{booktabs}
\usepackage{subfigure}			
\usepackage{epstopdf}		
\usepackage{longtable}  
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\usetikzlibrary{pgfplots.dateplot}
\usepackage{pgfplotstable}
\usepackage{filecontents}
\usepackage{setspace}
\usepackage[nolist]{acronym}
\usepackage{booktabs}
\usepackage[style=numeric]{biblatex}
\usepackage{afterpage}
%\bibliography{literatur2}
\addbibresource{literatur.bib}
\clubpenalty = 10000
\widowpenalty = 10000

\hyphenation{Dritt-an-bie-tern}
\hyphenation{Dritt-an-bie-ter}
\hyphenation{Da-tei-na-men}
\hyphenation{Back-end}
\hyphenation{Kon-to-stands-ta-be-lle}
\hyphenation{Account-Info}


\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \newpage}

%%%%%%%%%%%%%%%%%%%
%% definitions
%%%%%%%%%%%%%%%%%%%
\def\BaAuthor{Christian Norbert Braun}
\def\BaTitle{Einsatz eines Distributed File Systems zur Skalierung eines Banking-Buchungssystems}
\def\BaSupervisorOne{Prof.\ Dr.\ Steffen Heinzl}
\def\BaSupervisorTwo{Prof.\ Dr.\ Peter Braun}
\def\BaDeadline{31.03.2017}

\hypersetup{
pdfauthor={\BaAuthor},
pdftitle={\BaTitle},
pdfsubject={Subject},
pdfkeywords={Keywords}
}

%%%%%%%%%%%%%%%%%%%
%% configs to include
%%%%%%%%%%%%%%%%%%%
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{black}{rgb}{0,0,0}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\definecolor{pblue}{rgb}{0.13,0.13,1}
\definecolor{pgreen}{rgb}{0,0.5,0}
\definecolor{pred}{rgb}{0.9,0,0}
\definecolor{pgrey}{rgb}{0.46,0.45,0.48}

\lstset{
  basicstyle=\ttfamily\color{black},
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape
  linewidth=\textwidth
}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

%\lstset{language=xml,
%  morestring=[b]",
%  morestring=[s]{>}{<},
%  morecomment=[s]{<?}{?>},
%  stringstyle=\color{black},
%  numbers=left,
%  numberstyle=\scriptsize,
%  stepnumber=1,
%  numbersep=8pt,
%  identifierstyle=\color{darkblue},
%  keywordstyle=\color{cyan},
%  backgroundcolor=\color{background},
%  morekeywords={xmlns,version,type}% list your attributes here
%}

\lstset{language=Java,
  showspaces=false,
  showtabs=false,
  tabsize=4,
  breaklines=true,
  keepspaces=true,      
  numbers=left,
  numberstyle=\scriptsize,
  stepnumber=1,
  numbersep=8pt,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{pgreen},
  keywordstyle=\color{pblue},
  stringstyle=\color{pred},
  basicstyle=\ttfamily,
  backgroundcolor=\color{background},
%  moredelim=[il][\textcolor{pgrey}]{$$},
%  moredelim=[is][\textcolor{pgrey}]{\%\%}{\%\%}
}




\begin{document}

\acrodefplural{dfs}[DFS]{Distributed File Systems}

\begin{acronym}
  \acro{dfs}[DFS]{Distributed File System}
  \acro{hdfs}[HDFS]{Hadoop Distributed File System}
  \acro{api}[API]{Application Programming Interface}
  \acro{bbs}[Buchungssystem]{Banking Buchungssystem}
\end{acronym}


%%%%%%%%%%%%%%%%%%%
%% Titelseite
%%%%%%%%%%%%%%%%%%%


\frontmatter
\titlehead{%  {\centering Seitenkopf}
  {Hochschule für angewandte Wissenschaften Würzburg-Schweinfurt\\
   Fakultät Informatik und Wirtschaftsinformatik}}
\subject{Bachelorarbeit}
\title{\BaTitle\\[15mm]}
\subtitle{\normalsize{vorgelegt an der Hochschule f\"{u}r angewandte Wissenschaften W\"{u}rzburg-Schweinfurt in der Fakult\"{a}t Informatik und Wirtschaftsinformatik zum Abschluss eines Studiums im Studiengang Informatik}}
\author{\BaAuthor}
\date{\normalsize{Eingereicht am: \BaDeadline}}
\publishers{
  \normalsize{Erstpr\"{u}fer: \BaSupervisorOne}\\
  \normalsize{Zweitpr\"{u}fer: \BaSupervisorTwo}\\
}

%\uppertitleback{ }
%\lowertitleback{ }

\maketitle
\afterpage{\blankpage}

%%%%%%%%%%%%%%%%%%%
%% abstract
%%%%%%%%%%%%%%%%%%%

\chapter*{Überblick}

Die Systeme der Banken sind alt und über lange Zeit gewachsen. Seit jeher steht dabei das Konto und das Buchungssystem im Zentrum aller Bankprozesse. Neue Vertriebswege wie Online- und Mobile-Banking stellen jedoch Anforderung an die Buchungssysteme der Banken, die mit der aktuellen Technologie nur schwer zu meistern sind. Besonders die benötigte Skalierung, um tausende Anfragen zu beantworten, ist teuer und schwer umzusetzen. Systeme, die deutlich günstiger und leichter zu skalieren sind, sind Distributed File Systeme (DFS). Firmen wie Google, Facebook und Yahoo setzen sie schon länger zur Verwaltung großer Datenmengen ein. Diese Arbeit versucht, ein Konzept zu entwickeln, dass die Vorteile eines DFS für ein Banking-Buchungssystem zugänglich macht.


{\let\clearpage\relax\chapter*{Abstract}}

The IT systems of banks are old and have grown over a long period of time. Since then the account and the booking system formed the core of all banking processes. But new channels of distribution like online or mobile banking cause challenges which are hard to solve with the currently used technology. Especially the urgently needed scalability for processing thousands of requests is hard to manage and drives up the IT transaction costs. Systems capable of cheap and easy scaling are distributed file systems. Companies like Google, Facebook or Yahoo utilize them already to handle big amounts of data. In this thesis we introduce a concept which enables the banking booking system, to make use of the capabilities of a distributed file system and to drive .

\afterpage{\blankpage}

\chapter*{Danksagung}

Das Anfertigen dieser Arbeit war zeitaufwendig und anstrengend. Ohne die Hilfe einiger guter Freunde und Bekannte wäre das nicht möglich gewesen. Ich möchte mich hier bei Herrn Professor Doktor Steffen Heinzl bedanken. Sein Interesse an der Arbeit und seine konstruktive Kritik waren motivierend und haben Lust am Schreiben gemacht. Besonderer Dank geht auch an Herrn Francis Pouatcha von der adorsys GmbH \& Co. KG, der die Idee für das Thema dieser Arbeit hatte. Genauso wie ich hat er unzählige Stunden in das Lesen der Arbeit und das Entwickeln neuer Ideen gesteckt. Besonders seine unkonventionelle Herangehensweise zum Lösen von Problemen hat mich sehr geprägt. Zuletzt, möchte ich meinen tiefsten Dank für alle Korrekturleser und Unterstützer der Arbeit aussprechen. Eure Geduld und Hilfe, hat mich beruhigt und den ein oder anderen Rechtschreibfehler aufgespürt. Vielen Dank an meinen Bruder Herbert Braun, an Laura Vogel, Timo Schäfer und Tobias Fertig.


\afterpage{\blankpage}
%%%%%%%%%%%%%%%%%%%
%% Inhaltsverzeichnis
%%%%%%%%%%%%%%%%%%%
\tableofcontents										



%%%%%%%%%%%%%%%%%%%
%% Main part of the thesis
%%%%%%%%%%%%%%%%%%%

\mainmatter

\chapter{Einführung}\label{ch:intro}
Banken sind Big Data. Jeden Tag fließen unzählige Zahlungsprozesse und Kundendaten durch die Systeme deutscher und internationaler Banken. Allein im Jahr 2012 schätzte man die Menge der gespeicherten Daten auf etwa 1,9 Petabyte pro Bank \cite{datanami}. Ein Ende des Datenwachstums ist nicht in Sicht. Durch den Einsatz von Mobile- und Online-Banking steigt nicht nur die Anzahl der Vertriebswege, sondern auch der ausgeführten Transaktionen \cite{DBBigData}. Kunden rufen mobil ihren Kontostand ab, überweisen online ihre Rechnungen und bezahlen den Einkauf im Shoppingcenter mit der Kreditkarte. Dieses vertriebswegübergreifende Nutzen der Bankprodukte hat nicht nur einen Anstieg der Lese- und Schreibzugriffe auf die Systeme der Banken zur Folge, sondern fordert auch auf allen Vertriebswegen ein ähnlich gutes Nutzererlebnis \cite{bankwirtschaft}.

\section{Motivation}
Die weitläufig eingesetzten Kernbankensysteme sind veraltet und haben Probleme den Anforderungen und der Unmenge an Daten Herr zu werden. Ein Anpassung wäre möglich, ist jedoch aufgrund der benötigten Hard- und Software teuer oder liefert keine langfristige Skalierbarkeit und Ausfallsicherheit \cite{herzKernbankensystem}. Auf dieses Problem stießen vor den Banken schon Firmen wie Google, Facebook oder Yahoo. Die Lösung war in allen drei Fällen der Einsatz eines \acp{dfs}. So entwickelte Google das Google File System (GFS)\cite{GFS} zur Skalierung ihrer Websuche, Facebook Haystack \cite{haystack} zum Speichern und Lesen von Bildern und Yahoo das \ac{hdfs} \cite{hdfs}. Diese Systeme laufen auf Standard-Hardware und sind daher einfach und günstig skalierbar. Außerdem überzeugen sie durch eine hohe Ausfallsicherheit und Verfügbarkeit. Auch Banken könnten von den Möglichkeiten eines \acp{dfs} profitieren. Was bei Google und Co. funktioniert, birgt auch für Banken eine Chance, langfristig mit den aufstrebenden FinTechs zu konkurrieren und das volle Potential ihrer Daten auszunutzen \cite{wiki:fintech}.

\section{Zielsetzung}

Banken führen Änderungen der IT-Struktur in der Regel erst dann durch, wenn die zu übernehmende Technologie lang erprobt ist und sich als zuverlässig erwiesen hat. Doch der Wandel der Kunden im Umgang mit den Bankprodukten und die wachsende Datenmenge zwingt die Finanzbranche zu einem Umdenken \cite{bigdataBigStorage}. Im Rahmen dieser Arbeit sollen die Auswirkungen erarbeitet werden, die der Einsatz eines \acp{dfs} als Persistenzschicht eines Banking-Buchungssystems (Buchungssystem) bewirken kann. Da nicht alle Prozesse, die ein Buchungssystem abbildet, die gleichen Anforderungen haben, gilt es diejenigen herauszufinden, welche durch ein \ac{dfs} realisierbar sind. Insbesondere sollen die Möglichkeiten einer verbesserten Skalierbarkeit und Ausfallsicherheit diskutiert werden. Aktuelle Buchungssysteme sind behäbig und der Unterhalt für die Banken teuer \cite{bankingsCosts}. Durch ein auf Standard-Hardware optimiertes und leicht skalierbares System könnten Ressourcen akquiriert werden, wenn sie wirklich gebraucht werden, und abgeschaltet werden, wenn sie nicht mehr nötig sind. So sollen die Kosten, ein Buchungssystem zu betreiben, verringert werden. Im Idealfall profitieren davon nicht nur die Banken, sondern auch deren Kunden, durch günstigere Konten und eine bessere Buchungsverwaltung. Zusätzlich zu den wirtschaftlichen Verbesserungen sollen auch die Entwicklungen in der Ausfallsicherheit und Verfügbarkeit in dieser Arbeit kenntlich gemacht werden.

\section{Umfeld}
Unterstützend und beratend bei der Entwicklung dieser Arbeit tritt Herr Francis Pouatcha vom IT-Consulting-Unternehmen adorsys GmbH \& Co. KG auf. adorsys mit ihrem Hauptsitz in Nürnberg entwickelt mittlerweile seit mehr als 10 Jahren individuelle Software für Banken und Versicherungen. Zu den Kunden zählen neben der Teambank auch ERGO Direkt und Schwäbisch Hall. Auch die Entwicklung eines Open-Source-Kernbankensystems durch adorsys war zwischenzeitlich geplant. Alles in allem ist adorsys ein Partner mit Expertise im Finanzsektor und bei der Architektur von komplexen Systemen.

\section{Aufbau der Arbeit}
Im folgenden Kapitel wird zunächst genauer auf die Vorgehensweise der Recherche und Entwicklung der Arbeit eingegangen. So werden benötigte Metriken zum Messen der Performanz, Ausfallsicherheit und Skalierbarkeit erarbeitet. Außerdem wird der Rahmen der beispielhaften Implementierung und Bewertung der Lösung weiter abgesteckt.

Im Kapitel, \ref{buchungssystem} werden die grundlegenden Bestandteile und Aufgaben eines Buchungssystems erläutert. Besonders die Probleme und Entwicklungschancen sollen analysiert und ausgearbeitet werden.

Im darauf folgenden Kapitel \ref{basicdfs} soll auf Basis der vorher erarbeiteten Probleme die Funktionsweise eines \acp{dfs} näher erläutert und die Vor- und Nachteile analysiert werden. Auch auf typische Anwendungsgebiete und Grenzen wird näher eingegangen.

Nachdem nun die Begriffe des Buchungssystems und \acp{dfs} grob abgesteckt sind, können die beiden Systeme im Kapitel \ref{concept} kombiniert werden. Hier werden die Vorgänge zum Lesen und Schreiben von Buchungen sowie die Bedeutung von Transaktionen in einem Buchungssystem betrachtet. Außerdem werden zusätzliche Maßnahmen zur Skalierung und Ausfallsicherheit besprochen.

Das Kapitel \ref{implementation} beschreibt die schrittweise Umsetzung des Konzepts und die dafür benötigten Technologien. Auch die konkrete Auswahl der Programmiersprache und des \acp{dfs} wird hier getätigt.

Im Kapitel \ref{evaluation} wird die Zielsetzung der Arbeit mit den Funktionen der Testimplementierung verglichen und das Ergebnis bewertet. Das letzte Kapitel zeigt Möglichkeiten und Erweiterungen der Lösung auf, bevor in der Zusammenfassung ein abschließendes Fazit gebildet wird.

\afterpage{\blankpage}
\chapter{Vorgehensweise}
Die in der Einführung beschriebene Zielsetzung kann sich leider nur auf sehr wenige wissenschaftliche Quellen stützen. Gerade die benötigte Information zu den Banken ist rar gesät und kann meist nur aus Artikeln von News-Seiten oder Blogs entnommen werden. Angaben zur Datenmenge von Banken oder den Betriebskosten eines Kernbankensystems können daher nur geschätzt werden. Die Entwicklung des Konzepts und die darauf folgende Bewertung begründet sich also mehr auf Analysen und Annahmen als auf empirisch bewiesene Tatsachen. Alle technologiebezogenen Annahmen werden jedoch durch wissenschaftliche Arbeiten, welche sich mit den Grenzen und Möglichkeiten eben dieser Technologie beschäftigen, untermauert.

\section{Analyse der Ist-Situation}
Bevor eine Bewertung der Ist-Situation durchgeführt werden kann, muss zunächst die Bedeutung eines Buchungssystems innerhalb des Kernbankensystems und der Bank verstanden werden. Dabei gilt es, nicht nur die technische Architektur herauszuarbeiten, sondern auch die abzubildenden Prozesse zu erfassen. Dies soll vor allem im Hinblick auf die Anforderungen geschehen, welche sich durch die neuen Vertriebswege wie Online- und Mobile-Banking ergeben haben. Wie sieht der Ablauf zum Erstellen einer Buchung aus? Was mussten Buchungssysteme zur Zeit ihrer Entstehung und was müssen sie heute leisten? Entstehen überhaupt Probleme durch die zunehmende Anzahl an Anfragen an die Systeme der Banken? Welche Aufgaben haben welche Teile des Buchungssystems? Durch Fragen wie diese sollen die minimalen Anforderungen eines heutigen Buchungssystems erarbeitet und priorisiert werden. Außerdem soll erforscht werden, zu welchen Bedingungen diese Anforderungen von den aktuellen Systemen abgebildet werden und wo die eingesetzte Persistenzschicht die Möglichkeiten des Systems ausbremst.

\section{Einsatzbereiche eines DFS}
Nachdem die Bedeutung des Buchungssystems bekannt ist, werden die Bestandteile herangezogen, bei denen die aktuell eingesetzte Persistenzschicht am schlechtesten auf die Anforderungen passt. Schlecht bedeutet hierbei, dass entweder zu viel oder zu wenig Funktionalität bereit gestellt wird oder dass der Einsatz der Technologie massive Nachteile mit sich bringt. Danach soll die Funktionsweise mehrerer \ac{dfs} verstanden und deren Möglichkeiten mit den minimal benötigten Anforderungen abgeglichen werden. So lässt sich herausarbeiten, ob ein \ac{dfs} allein überhaupt zur Realisierung eines Bestandteiles des Buchungssystems geeignet ist. Zusätzlich sollen die Vor- und Nachteile eines \ac{dfs} mit den Vor- und Nachteilen der vorher eingesetzten Persistenzschicht verglichen werden. Je nachdem, welche Technologie hier besser abschneidet, lässt sich absehen, ob sich eine Investition in ein \ac{dfs} lohnt oder nicht.

\section{Entwicklung des Konzepts}
An dieser Stelle ist bereits klar, wie ein Buchungssystem funktioniert und welche Anforderungen es zu erfüllen hat. Außerdem sind die Teile des Systems bekannt, welche Raum für Verbesserungen durch ein \ac{dfs} bieten. Auch die generelle Funktionalität eines \ac{dfs} wurde erarbeitet und herausgefunden, welches der gegenwärtigen \ac{dfs} am besten zur Realisierung einzelner Bestandteile eines Buchungssystems geeignet ist. Dieses Wissen soll als Grundlage dienen, ein Konzept zu entwickeln, das noch detaillierter auf die Bedürfnisse des Buchungssystems eingeht. Dabei dient die Funktionsweise des am besten passenden \acp{dfs} als Richtlinie, welches aber noch speziell für den Anwendungsfall eines Banking-Buchungssystems angepasst werden soll. Ob bereits ein \ac{dfs} existiert, dass exakt so funktioniert, spielt hier zunächst keine Rolle. Es soll vielmehr ein System entwickelt werden, das neben seinen Aufgaben auch die wirtschaftlichen, leistungsbezogenen und auf die Skalierung bezogenen Anforderungen bestmöglich erfüllt. Auf Basis dieses Systems soll das Erstellen und Lesen einer Buchung detailliert erklärt werden. Dieser Prozess kann dann den entsprechenden Schritten eines konventionellen Buchungssystems gegenüber gestellt werden.

\section{Beispielhafte Implementierung}
Um das Ergebnis des Konzepts hinsichtlich der Performanz und Umsetzbarkeit zu testen, soll es beispielhaft implementiert werden.
Dazu muss sowohl die Programmiersprache für das Backend als auch ein konkretes \ac{dfs} ausgewählt werden. Falls es kein \ac{dfs} geben sollte, welches den aus dem Konzept hevorgehenden Anforderungen gerecht wird, muss auf das am besten passende ausgewichen werden. Die tatsächliche Entwicklung eines \acp{dfs} ist äußerst komplex und sprengt bei weitem den Rahmen dieser Arbeit. Das führt dazu, dass das Konzept gegebenenfalls auf ein bereits existierendes \ac{dfs} angepasst werden muss. Des Weiteren steht die Entwicklung einer Schnittstelle zum \acp{dfs} und des Backends an. Alle Schritte sollen eine ausreichende Testabdeckung vorweisen und entsprechend dokumentiert werden.

\section{Bewertung der Lösung}
Die Bewertung der im Rahmen dieser Arbeit entwickelten Lösung kann nur auf Basis der tatsächlichen Implementierung erfolgen. Die Auswirkung der Teile des Konzepts, welche sich technisch nicht umsetzen lassen, können nur erahnt werden und sind deshalb nicht zu berücksichtigen. Zur Bewertung der entwickelten Lösung werden die fünf folgenden Kriterien herangezogen.

\begin{enumerate}
  \item \textbf{Interoperabilität:} Beschreibt, wie hoch der Aufwand geschätzt wird, das System in ein bestehendes Buchungssystem zu integrieren.
  \item \textbf{Skalierbarkeit:} Wie gut lässt sich das System skalieren? Welche Teile können zum Flaschenhals werden? Zusätzlich zu diesen Fragen soll es auch darum gehen, ob eine lastbezogene Skalierung möglich ist und wenn ja, wie viel Aufwand dazu betrieben werden muss.
  \item \textbf{Performanz:} Die Leistung soll durch die Anzahl der beantworteten Anfragen bei maximaler Auslastung bewertet werden. Zusätzlich muss das System nach dem Performanz-Test einen korrekten Stand aufweisen.
  \item \textbf{Ausfallsicherheit:} Bei der Ausfallsicherheit soll betrachtet werden, wie viele Teile des Systems versagen können, bevor es zu einem inkonsistenten Zustand oder einem Ausfall führt.
  \item \textbf{Wirtschaftlichkeit:} Zu guter Letzt soll geschätzt werden, was der Betrieb des Systems kostet. Dazu werden sowohl die erforderliche Hardware und deren Kosten betrachtet, als auch der durchschnittliche Preis, um das System bei einem externen Anbieter zu hosten. Auch die Kosten für eine mögliche Skalierung und die Gewährleistung einer annehmbaren Ausfallsicherheit werden berücksichtigt.
\end{enumerate}

Der Einfluss eines \acp{dfs} auf ein Buchungssystem soll aus diesen Kriterien hervorgehen.
Daher soll auch die Technologie, der aktuell bei Banken eingesetzten Buchungssysteme anhand dieser Kriterien bewertet werden. Im direkten Vergleich zeigt sich am besten, was der Einsatz eines \acp{dfs} für ein Buchungssystem leistet.

\afterpage{\blankpage}
\chapter{Wesen und Probleme eines Buchungssystems}
\label{bookingSystem}
Die Anwendungssysteme der Banken sind alt und über lange Zeit gewachsen. Wo 1970 die ersten spartenbezogenen Programme nur bei Kredit-, Einlagegeschäften und
Wertpapierabwicklung unterstützten, bilden jetzt die Systeme der Banken nahezu alle Ge\-schäfts\-pro\-ze\-sse ab \cite[16]{ITidF}. Um die Bedeutung des Buchungssystems innerhalb dieser komplexen Anwendungsstruktur zu verstehen, soll im Folgenden zunächst auf die Entstehung und Architektur der Anwendungsysteme von Banken eingegangen werden. Danach werden die Aufgaben und Bestandteile eines Buchungssystems erarbeitet und die daraus entstehenden Anforderungen aufgezeigt. Im letzten Schritt sollen die Probleme der aktuellen Systeme besonders hinsichtlich der neuen Vertriebswege behandelt werden. 
\label{buchungssystem}

\section{IT-Systeme der Banken}
Seit den ersten Anwendungssystemen der Banken stand das Konto und die Kontoführung im Zentrum. Das Hinzufügen neuer Funktionen und Anforderungen erfolgte über das Anhängen neuer Module an eben diesen kontobezogenen Kern. Die so angedockten Abwicklungssysteme waren zum Beispiel für die Durchführung des Inlands- und Auslandszahlungsverkehr, des Kreditwesens oder der Einlagen verantwortlich. Nachdem die Banken auch Unterstützung bei den Geschäftsprozessen forderten, legte sich um die Abwicklungssysteme ein weiterer Ring. Dieser stellte Dienste für Kundenberater und Sachbearbeiter der Banken zur Verfügung \cite[18-20]{ITidF}\cite{SuPdIiB}. Diese Vorgehensweise führte zu einer Silo- oder auch Spartenarchitektur, die in Abbildung \ref{zwiebel} dargestellt wird. Systeme dieser Art wurden ursprünglich von den Banken selbst entwickelt. Die stark heterogenen Teilsysteme und die unentwirrbaren Abhängigkeiten zwischen ihnen stellten sich jedoch als nicht weiter tragbar und wartbar heraus \cite{bankEnzy}\cite{SuPdIiB}\cite[52]{ITidF}. Besserung versprach der Einsatz von hoch standardisierten Teilsystemen oder Gesamtbankenlösungen von Drittanbietern. Die Standardisierung erlaubt über Parameter eine eingeschränkte Anpassung der Systeme an die Bedürfnisse der Banken. So können die benötigten Systeme von unterschiedlichen Anbietern eingekauft und verbunden werden. Im Gegensatz dazu steht die Gesamtbankenlösung, die weiterhin versucht, allen Anforderungen und Funktionen einer Bank gerecht zu werden. Diese Art von Systemen ist aber aus ähnlichen Gründen, wie die ursprüngliche Anwendungsstruktur der Banken nicht besonders erfolgreich \cite[S. 56 ff.]{ITidF}. 

\begin{figure}
   \makebox[\textwidth]{\includegraphics[width=\paperwidth]{img/3/zwiebelstrktur.png}}
  \caption[Historische Anwendungsstruktur von Banken]{Historisch gewachsene Anwendungsstruktur von Banken. Entnommen aus \cite{SuPdIiB}.}
  \label{zwiebel}
\end{figure}

Heutzutage sind bei Genossenschaftsbanken häufig die Produkte von Fiducia GAD \cite{fiducia}, bei kleineren Privatbanken Systeme von FIS Kordoba \cite{kordoba} oder SAP \cite{SAP} und bei den Sparkassen die Lösung der Finanz Informatik One System Plus (OSPlus) \cite{finanzinformatik} im Einsatz. Die Entwicklung eigener Systeme können sich nur noch wenige große Banken, wie die Deutsche Bank erlauben. Bis auf die Lösung der Finanz Informatik OSPlus und FIS Kordoba handelt es sich in Deutschland in der Regel um Systeme, die keine Gesamtbankenlösung bieten, sondern mehr oder weniger Teile der Geschäfts- und Kontoprozesse der Banken abdecken und mit anderen Systemen kombiniert werden können \cite{einfuehrungKernbanksystem}\cite[56-58]{ITidF}. Auch wenn die aktuellen Systeme deutlich besser standardisiert und dadurch wartbarer als früher sind, ist die Struktur immer noch ähnlich oder baut im Kern sogar noch auf den Ursprüngen der Banken-IT auf. Die Architektur lässt sich in vier Schichten aufteilen, die von oben nach unten folgendermaßen beschrieben sind \cite[104]{ITidF}:

\begin{enumerate}
\item \textbf{Visualisierungsschicht:} Die Visualisierungsschicht ist die Schnittstelle zum Benutzer und wird auf dessen Endgerät ausgeführt. Sie bezieht einerseits Daten von der Darstellungsschicht und zeigt diese ansprechend an. Andererseits leitet sie die Eingaben des Nutzers an die Darstellungsschicht weiter.
\item \textbf{Darstellungsschicht:} Diese Schicht ist verantwortlich für eine fehlerfreie Kommunikation zwischen Visualisierungsschicht und Anwendungsschicht. Daten werden von einer Schicht empfangen und in das gewünschte Format der anderen Schicht überführt.
\item \textbf{Anwendungsschicht:} Die Anwendungsschicht kümmert sich um die eigentliche Geschäftslogik für einen bestimmten Teilbereich. Sie stellt Schnittstellen für Dienste wie teilweise auch die neuen Vertriebswege Online- und Mobile-Banking zur Verfügung und führt aufwendige Datenmanipulationen auf der Datenschicht durch. Aufgrund der teilweise sehr alten und komplexen Systeme finden sich hier auch Bereiche, die noch in COBOL oder sogar Assembler geschrieben sind.
\item \textbf{Datenhaltungsschicht:} Die Datenhaltungsschicht ist für die Verwaltung der Stammdaten sowie der von der Anwendungsschicht gesendeten und angefragten Informationen zuständig. Auch Operationen für einfache Manipulationen der Daten werden von ihr bereitgestellt. In der Finanzindustrie gelten relationale Datenbanken als Standard für die Persistenzschicht der Datenhaltungsschicht \cite[105]{ITidF}\cite{MarkstudieKernbankensysteme}
\end{enumerate}

Die Datenhaltungsschicht als Basis der Architektur entspricht je nach Aufteilung und Definition entweder allein oder in Kombination mit einzelnen Anwendungsschichten dem Buchungssystem. Ähnlich wie in Abbildung \ref{zwiebel} bildet es auch heute noch das Zentrum der IT-Landschaft von Banken. Als ein solches wird es auch häufig als Kernbankensystem bezeichnet. Diese Bezeichnung verdeutlicht auch, dass die Aufgabe des Buchungssystems die Erfüllung von Kernaufgaben ist und allein nur eine begrenzte Funktionalität bereitstellt. Die Unterstützung weitergehender Bankprozesse muss durch das Aufsetzen weiterer Anwendungsschichten erfolgen \cite[58]{ITidF}. Da in der Literatur und Wirtschaft der Begriff Kernbankensystem auch häufig als eine Gesamtbankenlösung verstanden wird, wird in dieser Arbeit immer von "Buchungssystem" gesprochen, sofern es um die Kernfunktionalität von Banken geht \cite{vergleichCoreBanking}.  


\section{Das Buchungssystem und seine Aufgaben}
Das Buchungssystem steht demnach im Zentrum jedes kontenbasierten Geschäftsvorfalls der Banken \cite{bankEnzy}. Daher muss es auch alle Kernfunktionen eben dieser erfüllen und unterstützen können. Dazu gehören die Zahlungsverkehrs-, Investitions- und Kreditfunktion, aber auch die Verwaltung von Kundenstammdaten sowie die grundlegende Kontoführung. Für Investitions- und Kreditfunktion können auch die Begriffe Passiv- und Aktivgeschäfte genutzt werden \cite[12, 86]{DdF}\cite{einfuehrungKernbanksystem}. Im Folgenden soll der Inhalt der einzelnen Funktionen anhand von \cite[69-88]{DdF} und \cite[91-153]{bankwirtschaft} beschrieben werden.

\begin{itemize}
  \item \textbf{Aktivgeschäfte:} Das Aktivgeschäft erhält seinen Namen, da alle Kreditgeschäfte, also Forderungen an Kunden in der Bilanz auf der Aktivseite abgebildet werden. Durch das Kreditgeschäft erzielen Banken einen Großteil ihrer Zinserträge. Jedoch ist damit auch ein hohes Risikopotential verbunden. Als Aktivgeschäft zählen unter anderem alle klassischen Kreditgeschäfte
  - zum Beispiel der Kontokorrentkredit oder auch Dispositionskredit, der Privatpersonen eine Überziehungsmöglichkeit des Kontos einräumt. Aber auch längerfristige Kreditgeschäfte wie der Hypothekarkredit, Baukredit oder Investitionskredit gehören zu den Aktivgeschäften. Bei den Kreditgeschäften gilt in der Regel, dass von der Bank zum Kunden ein vertraglich festgelegter Betrag fließt, dessen Rückzahlung zusätzlich Zinsen vom Schuldner auf ein Konto der Bank zu erbringen ist.
  \item \textbf{Passivgeschäfte:} Als Passivgeschäft werden alle Einlagegeschäfte der Banken bezeichnet. Im Gegensatz zu den Aktivgeschäften erscheinen sie in der Bilanz auf der Passivseite. Aktiv- und Passivgeschäfte stehen in einer engen Verbindung. Häufig finanzieren Kreditinstitute ihre Aktivgeschäfte durch die Einlagen der Kunden. Zu den Passivgeschäften gehören Sichteinlagen und Termineinlagen. Sichteinlagen sind täglich fällige Gelder. In der Regel handelt es sich hierbei um die Einzahlungen auf ein Girokonto. Ihr Zweck ist hauptsächlich der bargeldlose Zahlungsverkehr. Termineinlagen hingegen entziehen den Kunden den Zugriff auf die Einlage für einen festgelegten Zeitraum, der in der Regel auf eine Dauer von mindestens 30 Tagen und maximal fünf Jahren angelegt wird. Es handelt sich also um Einlagen, die der Kunde über einen gewissen Zeitraum nicht benötigt. Dafür werden Termineinlagen höher verzinst als Sichteinlagen. Werden Termineinlagen als Kündigungsgelder vereinbart, so gibt es keine Laufzeitfrist und die Auszahlung erfolgt nach Einreichen einer Kündigung und abgelaufener Kündigungsfrist. Bei Passivgeschäften fließt also ein festgelegter Betrag vom Kunden zur Bank, die über die Einlage in einem festgelegten Rahmen verfügen darf.
  \item \textbf{Zahlungsverkehr:} Der Zahlungsverkehr im Bankengeschäft beschreibt die bare und unbare Übertragung von Zahlungsmitteln im Inland und Ausland. Zur Übertragung können Überweisungen, Kartenzahlungen oder Lastschriften genutzt werden. Die besondere Bedeutung des Zahlungsverkehrs liegt in der Un\-um\-gäng\-lich\-keit für die Bankkunden. Auch wenn Kunden keine Kredite nehmen oder keine Einlagen tätigen, so müssen dennoch immer Zahlungen über die Systeme der Banken ausgeführt werden. Gerade die zunehmende Digitalisierung wird einen großen Einfluss auf die Entwicklung des bargeldlosen Zahlungsverkehrs haben. 
\end{itemize}

Das Buchungssystem muss all diese Inhalte führen können und Kontobewegungen nachvollziehbar ablegen. Im Folgenden werden alle Kontobewegungen als Buchungen bezeichnet. Es dient also neben den Anwendungssystemen auch den gesetzlichen Rechnungsabschlüssen und Bilanzen als Grundlage \cite{bankEnzy}\cite{MarkstudieKernbankensysteme}. In der Regel wird diese Nachvollziehbarkeit durch die doppelte Buchführung gewährleistet. Das heißt, dass eine Buchung immer in den Konten beider beteiligten Parteien auftaucht. In Summe müssen alle Kredit- und Debitbeträge null ergeben \cite{accounting}.

\section{Anforderungen an ein Buchungssystem}
Um die oben genannten Aufgaben jetzt und auch noch in Zukunft zuverlässig zu erfüllen, müssen Buchungssysteme gewissen Anforderungen gerecht werden. Diese lassen sich in interne und externe Anforderungen unterteilen. Interne Anforderungen beschreiben die Bedingungen, denen eine Bank nachkommen muss, um die geschäftsinternen Prozesse reibungslos und wirtschaftlich durchzuführen. Beispiele dafür sind eine wohldefinierte Schnittstelle des Buchungssystems, an der bei Bedarf neue Geschäftsprozesse angedockt oder eine detaillierte Analyse des Kundenverhaltens durchgeführt werden kann. Auch die Reduktion der IT-Wartungskosten gilt als eine interne Anforderung an Buchungssysteme. Externe Anforderungen beziehen sich auf Bedingungen, die Banken von außen auferlegt werden. Dazu gehören gesetzliche Richtlinien sowie Veränderungen von Angebot und Nachfrage im Markt \cite{capgemini}. So wird durch den Kunden, aber auch durch den Gesetzgeber das zuverlässige Ablegen einer jeden Kontobewegung beziehungsweise Buchung gefordert. Das Buchungssystem muss folglich immer erreichbar sein und eine hohe Ausfallsicherheit gewährleisten. Da aber neben der Ausfallsicherheit auch die Geschwindigkeit der Abarbeitung bankfachlicher Prozesse relevant ist, spielt die Performanz der Buchungssysteme auch eine tragende Rolle. Banken legen deswegen häufig ihre Buchungssysteme redundant an und verbinden diese über ein sicheres und performantes Datennetz \cite{bankEnzy}\cite[97-99]{ITidF}. Die so nebeneinander gestellten Systeme erlauben auch eine Lastverteilung bei der Bearbeitung mehrerer Anfragen. Gerade in Hinblick auf die neuen Vertriebswege und den steigenden bargeldlosen Zahlungsverkehr ist ein System, das sich skalieren lässt, unabdingbar \cite{bankEnzy}\cite{capgemini}. Die Abbildungen \ref{online-banking} und \ref{bargeldlos} zeigen dieses Wachstum auf.


\begin{filecontents}{date.dat}
date       value
2006-01-01  34
2007-01-01  34
2008-01-01  36
2009-01-01  37
2010-01-01  35
2011-01-01  44
2012-01-01  44
2013-01-01  45
2014-01-01  54
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahr},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anteil der Nutzer in Prozent},
date ZERO=2005-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date.dat};
\end{axis}
\end{tikzpicture}
\caption[Nutzer von Online-Banking in Deutschland]{Nutzer von Online-Banking in Deutschland. Nachempfunden nach \cite{onlinebanking}.}
\label{online-banking}
\end{center}
\end{figure}


\begin{filecontents}{date2.dat}
date       value
2011-01-01  90.61 
2012-01-01  94.38
2013-01-01  99.52
2014-01-01  103.34
2015-01-01  112.13
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahr},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anzahl der Transaktionen in Mrd.},
date ZERO=20010-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date2.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionen im bargeldlosen Zahlungsverkehr in der EU]{Anzahl der Transaktionen im bargeldlosen Zahlungsverkehr in der EU. Nachempfunden nach \cite{bargeldlos}.}
\label{bargeldlos}
\end{center}
\end{figure}


Die Folge dieser Entwicklung ist nicht nur eine steigende Anzahl an Zugriffen auf die Systeme der Banken, sondern auch eine stetig zunehmende Datenmenge. Nicht umsonst wurde in der Einleitung dieser Arbeit die Verbindung zwischen Big Data und Banken geknüpft. Wo ursprünglich das alleinige Speichern aller Buchungsdaten ausreichend war, fordern neue Dienste und Richtlinien nun auch den schnellen und gezielten Zugriff auf Kundendaten, Konten und Buchungen \cite{bigdataBigStorage}. In diesem Zuge ist die Second Payment Service Directive (PSD2) zu nennen. Dabei handelt es sich um eine EU-Richtlinie, die im Januar 2016 in Kraft getreten ist. Demnach sind Banken verpflichtet, auf Wunsch des Kunden alle seine Konten und durchgeführten Buchungen einem Drittanbieter über eine sichere Verbindung zur Verfügung zu stellen. Auch das Autorisieren von Bezahlungen muss über diese Verbindung möglich sein. Die Europäische Aufsichtsbehörde hat am 23. Februar 2017 die Regulatory Technical Standards (RTS) veröffentlicht, welche festlegen sollen, wie diese Verbindung realisiert werden muss und wie die Schnittstelle für Drittanbieter auszusehen hat \cite{rts}. Jetzt haben die Banken bis zum vierten Quartal 2018 Zeit, die Bestimmung in ihre Systeme zu integrieren \cite{eu-psd2}\cite{psd2dk}. Die PSD2 stellt eine enorme Herausforderung für die Buchungssysteme der Banken dar. Wo vorher nur Zugriffe von bankeigenen Systemen möglich waren, können jetzt beliebige Drittanbieter Kontodaten und Kontobewegungen abfragen. Als Konsequenz können deutlich steigende Lese- und moderat steigende Schreibzugriffe erwartet werden. Die Vorteile, die sich für den Endnutzer ergeben, stellen die Banken hingegen vor ein Problem. Für die Bankkunden sind alle Bankprodukte bankenübergreifend in einer einzigen Anwendung verfügbar. Drittanbieter können Kauf- und Sparverhalten der Kunden analysieren und sinnvolle Hinweise zur Kontoführung geben. Zusätzlich sind Kunden nicht mehr an die eine Schnittstelle ihrer Bank gebunden, um Buchungen durchzuführen, sondern können ihrer Finanzen mit der besten am Markt erhältlichen Anwendung verwalten. Die Banken müssen jedoch zum einen mit der größeren Belastung der Buchungssysteme umgehen und zum anderen ihre Anwendungen und Infrastruktur umstrukturieren, um trotz Konkurrenz die Kunden auf ihrer Plattform halten zu können \cite{psd2vid}.
Die Anforderungen an ein Buchungssystem belaufen sich demnach auf eine wohldefinierte Schnittstelle sowohl für bankeigene Prozesse als auch für Drittanbieter, ein gutes Kosten-Nutzen-Verhältnis der Systeme sowie auf Skalierbarkeit und Ausfallsicherheit, die den Forderungen durch PSD2 und neuen Vertriebswegen wie Mobile- und Online-Banking gerecht wird.

\section{Probleme aktueller Buchungssysteme}
\label{problems}
Wie bereits erwähnt, sind die IT-Landschaften der Banken und insbesondere die Buchungssysteme alt und über einen langen Zeitraum gewachsen. Neue Funktionalität wurde über das Hinzufügen neuer Schichten realisiert, wobei der Kern gleich blieb. Da aber zur Zeit der Entwicklung der Buchungssysteme die heutigen Anforderungen noch nicht absehbar waren, können diese auch nicht zufriedenstellend erfüllt werden \cite[23-27]{ITidF}\cite{bankEnzy}. Auch die Banken selbst sind sich bewusst, dass etwas getan werden muss, um weiter auf dem Markt relevant zu bleiben \cite{capgemini}.
Besonders die Skalierung der Systeme könnte sich für die Banken als Problem herausstellen. Durch die PSD2 und die neuen Vertriebswege werden eine steigende Datenmenge sowie Lese- und Schreibzugriffe eine hohe Belastung der Buchungssysteme zur Folge haben \cite{bigdataBigStorage}. Experten gehen davon aus, dass sich die Menge der Daten bis 2020 versiebenfacht \cite{versiebenfacht}. Buchungssysteme können aber nicht beliebig skaliert werden, um die Anforderungen zu meistern. Die eingesetzten relationalen Datenbanken scheinen für viele Aufgaben essentiell, sind aber durch ihre Architektur und grundlegenden Konzepte nicht für das Speichern und Verwalten beliebig vieler Einträge geeignet \cite{rdbmsBigData}. Die Grundlage für relationale Datenbanken bildet das ACID-Prinzip (Atomicity, Consistency, Isolation, Durability). Atomarität, Konsistenzerhaltung, Isolation und Dauerhaftigkeit beschreiben eine mächtige Möglichkeit, Transaktionen innerhalb eines Systems abzubilden. Auch bei nebenläufigen Prozessen kann so immer ein konsistenter Stand gewährleistet und im Fehlerfall wieder hergestellt werden. Diese Möglichkeiten gehen jedoch auf Kosten der Performanz. Um die Einhaltung des ACID-Prinzips zu garantieren, müssen alle an einer Transaktion beteiligten Einträge mit einem exklusiven Lock versehen werden. Bei einer stark verteilten Datenbank erfordert dieser Vorgang ein eigenes verteiltes Commit-Protokoll, auch Zwei-Phasen-Commit genannt \cite{dbarchitecture}. Alle an einem Commit beteiligten Datenbanksysteme müssen den Commit bestätigen. Denn entweder wird die Transaktion auf allen Systemen oder auf keinem ausgeführt. Ist ein Datenbanksystem nicht verfügbar, kann die Transaktion nicht durchgeführt werden. Bei zwei Datenbanksystemen mit jeweils 99,9 \% Verfügbarkeit wird die Verfügbarkeit des Gesamtsystems, durch die Abhängigkeit des Zwei-Phasen-Commits auf 99,8 \% reduziert \cite{BASE}. Wie sich der Transaktionsdurchsatz einer relationalen Datenbank bei einem steigenden Anteil an konkurrierenden Anfragen verhält, ist in Abbildung, \ref{salt} zu erkennen. Hierbei wurden Transaktionen mit jeweils fünf Update-Operationen auf Tabellen mit unterschiedlich vielen Einträgen ausgeführt. Je nach Anzahl der Reihen in den Tabellen kam es so zu mehr oder weniger konkurrierenden Zugriffen \cite{salt}.
\begin{filecontents}{date3.dat}
date  value
0     70000
1     65000
2     10000
3     2000
4     300
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xtick=data,
ymin=0, ymax=80000,
scaled ticks=false,
xticklabels={0,0{,}0001,0{,}001,0{,}01,0{,}1},
xlabel={Anteil konkurrierender Zugriff (1/Anzahl der Tabellenreihen)},
ylabel={Transaktionsdurchsatz (Transaktionen/Sekunde)},
]
\addplot table[x=date,y=value] {date3.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionsdurchsatz im Verhältnis zum konkurrierenden Zugriff]{Transaktionsdurchsatz im Verhältnis zum konkurrierenden Zugriff auf Ressourcen in einem ACID-basierten System. Nachempfunden nach \cite{salt}.}
\label{salt}
\end{center}
\end{figure}
Der sinkende Transaktionsdurchsatz ist eine Folge der nach ACID-Prinzipien durchgeführten Transaktionen. Hierbei werden Datenbankeinträge erst wieder freigegeben, nachdem die gesamte Transaktion durchgeführt oder abgebrochen wurde. Je nachdem, wie das Locking implementiert oder welche Daten angefordert werden, kann eine Transaktion ein Lock für einzelne Tabellenzeilen, die ganze Tabelle, ganze Datenblöcke oder sogar für das ganze Datenbanksystem anfordern. Es ist also durchaus möglich, dass eine Kontotransaktion für einen Kunden auch die Einträge anderer Kunden mit beansprucht \cite{locking}\cite{dbarchitecture}.

Aber nicht nur bei rein schreibenden Transaktionen fällt die Performanz relationaler Datenbanken enorm ab. Die Abbildung \ref{salt2} zeigt den Transaktionsdurchsatz auf eine Tabelle mit 100 Einträgen bei Transaktionen, die fünf Einträge entweder verändern oder lesen, während der Anteil der lesenden Transaktionen immer weiter zunimmt. Wie zu sehen ist, führt schon ein zehnprozentiger Anteil an Schreibzugriffen zur Halbierung des Transaktionsdurchsatzes.
\begin{filecontents}{date4.dat}
date  value
0     100000
5     90000
10    50000
15    12000
20     8000
50     5000
100    1000
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xmin=0, xmax=100,
ymin=0, ymax=100000,
scaled ticks=false,
xlabel={Anzahl Schreibzugriffe in Prozent},
ylabel={Transaktionsdurchsatz (Transaktionen/Sekunde)},
]
\addplot table[x=date,y=value] {date4.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionsdurchsatz im Verhältnis zum Anteil der Schreibzugriffe]{Transaktionsdurchsatz im Verhältnis zum Anteil der Schreibzugriffe in einem ACID-basierten System. Nachempfunden nach \cite{salt}.}
\label{salt2}
\end{center}
\end{figure}

Zusätzlich ist bei einem sehr hohen Durchsatz von Transaktionen nicht zu erwarten, dass zwei Transaktionen mit ähnlichen Daten auch im Speicher immer nah zusammen liegen. Bezogen auf die kontenbezogenen Daten, kann ein hoher Durchsatz von schreibenden Transaktionen zu einer Datenfragmentierung führen. Das bedeutet, dass es unwahrscheinlich ist, dass alle Umsätze eines Kunden innerhalb der Umsatztabelle der Bank auch im Speicher nahe zusammen liegen. Auch das könnte einen Einfluss auf die Lesegeschwindigkeit haben. Relationale Datenbanken werden in der Regel vertikal skaliert, wenn überhaupt. Horizontale Skalierung ist zwar zum Beispiel über Sharding möglich, ist jedoch meist aufwendig zu implementieren und hat bei verteilten Anfragen negative Auswirkungen auf die Performanz der Datenbanksysteme. Vertikale Skalierung beschreibt die Skalierung von Systemen durch das Aufrüsten von Hardware-Komponenten, also zum Beispiel das Einsetzen eines größeren Hauptspeichers oder eines schnelleren Prozessors. Horizontale Skalierung hingegen wird durch die Verteilung der Anwendung auf mehrere Maschinen erreicht. Die Nachteile an der vertikalen Skalierung sind jedoch die sehr limitierten Möglichkeiten und die höheren Kosten im Vergleich zur horizontalen Skalierung \cite{sharding}\cite{rdbmssuck}. 

Die Kosten der IT-Systeme sind allerdings jetzt schon ein Problem für die Banken. Eine exakte Ermittlung der Ausgaben für die IT der Banken stellt sich als schwierig heraus, da die Zahlen in der Regel nicht öffentlich gemacht werden. Die Schätzungen der IT-Kosten beziehen sich daher auf die in der Gewinn- und Verlustrechnung öffentlich gemachten Aufwendungen für den Verwaltungsaufwand der Banken. Experten schätzen, dass 15 - 20 \% davon für die IT anfallen. Demnach lagen die Kosten für die Anwendungssysteme der Deutschen Bank im Jahre 2004 bei rund 2,6 Milliarden Euro und die der HypoVereinsbank bei mehr als 900 Millionen Euro. Die zehn größten Banken in Deutschland haben 2004 etwa 6,5 Milliarden Euro für die Instandhaltung und Entwicklung ihrer IT ausgegeben. 
Etwa 24 \% davon fließen alleine in die Buchungsdienste. Gerechnet auf die Girokonten ergibt das Jahresausgaben von etwa 14 Euro pro Girokonto \cite[29-39]{ITidF}. Diese Zahl lässt sich auch durch den Umsatz und die betreuten Konten von Kernbanksystem-Anbietern validieren. So betreute die Fiducia IT 2005 insgesamt 52,6 Millionen Kundenkonten und erzielte einen Jahresumsatz von etwa 728,6 Millionen Euro. Das entspricht einem Preis von 13,80 Euro pro Konto. Die Finanz Informatik erzielte im Jahr 2005 bei 58 Millionen Kundenkonten einen Umsatz von 723 Millionen Euro, was einem Kontopreis von etwa 12,50 Euro entspricht. Dabei fließen nur etwa 20 \% in die Entwicklung neuer Dienste. Die restlichen Kosten fallen für Betrieb und Instandhaltung an. Langfristig sind diese Ausgaben für Banken nicht tragbar \cite[75-91]{ITidF}\cite{SuPdIiB}\cite[41-42]{DdF}\cite{bankingsCosts}. 

\cleardoublepage
\chapter{Grundlagen eines DFS}
\label{basicdfs}
Zu den Skalierungsproblemen und dem hohen Kostenfaktor der Anwendungssysteme der Banken tragen auch die ACID-basierten Datenbanksysteme bei. Auch wenn die Funktionalität, die ACID mit sich bringt, nur für einen Bruchteil der Prozesse benötigt wird, wird so das gesamte System ausgebremst und in den Skalierungsmöglichkeiten eingeschränkt \cite{salt}.
Systeme, die hingegen die Skalierbarkeit und Verfügbarkeit gegenüber der Konsistenz priorisieren, funktionieren nach dem BASE-Prinzip (Basically Available, Soft State, Eventual consistency). Häufig weisen diese Systeme niedrigere Betriebskosten im Vergleich zu den ACID-Alternativen auf \cite{clusterBASE}.
In diesem Kapitel soll zunächst die Bedeutung von BASE geklärt werden. Ein besonderer Fokus liegt dabei auf den Unterschieden der Konsistenz zu traditionellen ACID-Systemen. Dann soll es um die Funktionsweise von DFS gehen. Diese funktionieren nach dem BASE-Prinzip und haben Verfügbarkeit, Ausfallsicherheit und Kosteneffizienz zu ihren Königsdisziplinen gemacht. Zum besseren Verständnis werden zwei populäre Vertreter der DFS näher erläutert. Im letzten Teil des Kapitels werden die Vorteile und Grenzen der DFS nochmal genauer betrachtet und die Anforderungen an ein Buchungssystem mit den Möglichkeiten eines DFS abgeglichen.

\section{Konsistenz und Verfügbarkeit in BASE-Systemen}
ACID-Systeme sind weit verbreitet und bekannt. Sie bringen eine sehr starke Semantik, verursachen aber in verteilten Systemen hohe Komplexität und Kosten. Konsistenz ist das oberste Ziel von ACID-basierten Systemen. Die Verfügbarkeit der Systeme wird nicht garantiert. Im Gegenteil, es wird sogar bevorzugt, keine Antwort zu geben, als eine falsche \cite{clusterBASE}. Die Skalierung solcher Systeme ist schwierig. Ist ein Teilsystem nicht erreichbar, leidet die Verfügbarkeit des gesamten Systems darunter \cite{BASE}. In einer idealen Welt wären Systeme gleichermaßen skalierbar, konsistent und verfügbar. Solche Systeme kann es jedoch nach dem CAP-Theorem (Consistency, Availability, Partition tolerance) nicht geben. Demnach können verteilte Systeme nur zwei von den drei Eigenschaften Konsistenz, Verfügbarkeit und Partitionstoleranz erfüllen. Da Partitionstoleranz jedoch für jede Art der Skalierung benötigt wird, kann die Wahl nur zwischen Verfügbarkeit und Konsistenz fallen. Die Vorstellung, Konsistenz gegen Verfügbarkeit zu tauschen, erscheint häufig bedenklich. Es ist aber wichtig zu verstehen, dass eine Entscheidung für zwei Eigenschaften des CAP-Theorem nie einen hundertprozentigen Ausschluss der dritten bewirkt.
Das heißt, hoch konsistente Systeme verzichten nicht komplett auf die Verfügbarkeit und hoch verfügbare Systeme können einen gewissen Grad an Konsistenz bieten \cite{cap}. Die Herausforderung ist, ein für die Anwendung möglichst passendes Verhältnis zwischen Konsistenz und Verfügbarkeit zu finden. So wie die Verfügbarkeit eines Systems in Prozent angegeben werden kann, gibt es auch Freiraum in der Gestaltung der Konsistenz. Die Autoren Paolo Viotti und Marko Vukolic unterscheiden in ihrem Artikel \textit{Consistency in Non-Transactional Distributed Storage Systems} insgesamt 50 Konsistenzarten \cite{consistency}. Demnach war in den 80er-Jahren nur die strong consistency bekannt. Sie fordert, dass Datenoperationen immer direkt zwischen der Anfrage vom und der Antwort zum Client durchgeführt werden müssen. Zusätzlich muss jeder Lesezugriff den tatsächlich letzten geschriebenen Wert zurückliefern. Am anderen Ende des Konsistenzspektrums befinden sich die weak und eventual consistency. Bei weak consistency, müssen Lesezugriffe nicht immer den tatsächlich letzten geschrieben Wert zurückliefern. Auch die Reihenfolge, in der die Schreiboperationen ausgeführt werden, ist nicht vorgegeben. Mehrfache Lesezugriffe nacheinander müssen nicht immer das gleiche Ergebnis liefern. Eventual consistency ist etwas stärker als weak consistency. Auch hier liefert ein Lesezugriff nicht immer die letzten geschrieben Werte zurück. Wird aber für eine Weile kein Schreibzugriff durchgeführt, werden immer die gleichen Werte gelesen. Es tritt also irgendwann ein konsistenter Stand ein. Eventual consistency aus der Sicht einzelner Datenreplikas in einem verteilten System lässt sich mit drei Eigenschaften beschreiben:
\begin{itemize}
  \item \textbf{Eventual delivery:} Wenn eine Schreiboperation eine Datenreplika verändert, wird diese Schreiboperation irgendwann bei allen Datenreplikas durchgeführt.
  \item \textbf{Convergence:} Alle Datenreplikas, welche die Schreiboperationen erhalten haben, werden irgendwann identisch sein.
  \item \textbf{Termination:} Alle Operationen werden ausgeführt und beendet.
\end{itemize}
Systeme, die auf dem BASE-Prinzip aufsetzen, tauschen eine strong consistency gegen die eventual consistency ein und erreichen so eine hohe Verfügbarkeit. Wo ACID pessimistisch mit Schreibzugriffen umgeht und Konsistenz immer am Ende einer Transaktion fordert, ist BASE optimistisch und garantiert nur, dass irgendwann Konsistenz eintreten wird. Dieser Ansatz stützt sich darauf, dass dem Nutzer eine immer verfügbare Anwendung wichtiger ist, als dass sie immer den korrekten Stand anzeigt - vor allem, wenn sich die Dauer des inkonsistenten Zustands auf wenige Sekunden beläuft. Durch diesen kleinen Abstrich bei der Konsistenz schaffen BASE-Systeme aber viel Raum für Skalierung und Performance-Steigerungen. Kommunikation zwischen den Teilsystemen, die in einem ACID-System zwingend nötig ist, kann in einem BASE-System vernachlässigt oder auf einen besser passenden Zeitpunkt verschoben werden \cite{clusterBASE}. Client-Anfragen können so nahezu immer und schnell beantwortet werden. Zusätzlich belastet das Starten mehrerer parallel laufender Systeme das Netzwerk nicht so stark wie bei vergleichbaren ACID-Systemen.

\section{Funktionsweise eines DFS}
DFS basieren auf dem oben erklärten BASE-Prinzip. Sie lockern die Konsistenz auf und ermöglichen dadurch gute Skalierbarkeit, Verfügbarkeit und Kosteneffizienz. Deswegen haben DFS einen besonderen Stellenwert in der Speicherung und Verarbeitung von Big Data und werden von vielen Firmen in diesem Bereich eingesetzt. Beispiele hierfür sind das GFS, welches von Google selbst eingesetzt wird, Facebooks Haystack, das von der Deutsche Telekom, CERN und Cisco verwendete DFS Ceph \cite{ceph} und das von Yahoo entwickelte und weit verbreitete HDFS.

Um sehr große Datenmengen verwalten zu können, bedienen sich die meisten DFS des sogenannten Object Storage \cite{cephPaper}. Traditionelle Filesysteme arbeiten mit dem File Storage, bei dem die Dateien und die dazugehörigen Metadaten getrennt abgespeichert werden. Gerade bei sehr vielen Dateien wird die Verwaltung der Metadaten hier zu einem Flaschenhals \cite{filestorage}. Beim Object Storage hingegen werden die Datei und die dazugehörigen Metadaten gemeinsam abgespeichert. Die Kombination aus Dateiinhalt und Metadaten wird auch Objekt genannt. Anstelle auf der Datei zu arbeiten, werden alle Operationen auf Ebene der Objekte durchgeführt. Lesen, Schreiben und Löschen funktioniert ähnlich wie bei einem traditionellen Filesystem, jedoch kann in einem Object Storage der Nutzer nicht bestimmen, wie und wo das Objekt tatsächlich abgelegt wird. Der Zugriff auf einen Object Storage erfolgt auch meistens über eine abstrakte Schnittstelle wie zum Beispiel HTTP. Die Speicherverwaltung fällt demnach komplett in den Aufgabenbereich der sogenannten Object Storage Devices (OSD). Mehrere OSDs können so einfach nebeneinander gestellt und durch eine HTTP-Schnittstelle wie ein einziges OSD angesprochen werden. Dafür ist lediglich eine zusätzliche Verwaltung der einzelnen OSDs notwendig. Auf diese Weise lässt sich ein Object Storage extrem gut skalieren und liefert eine nahezu endlose Speicherkapazität. Dadurch, dass der Nutzer einen Cluster an OSDs wie ein einziges ansprechen kann, können die einzelnen OSDs sogar über Ländergrenzen hinweg verteilt sein. So kann zusätzlich Ausfallsicherheit gewährleistet werden \cite{osvideo}\cite{objectstorage}\cite{objectBasedStorage}. In der Regel bestehen Object-Storage-Systeme aus den OSDs selbst und einem Server, welcher diese OSDs verwaltet.

Nachdem die Grundlage eines DFS besprochen wurde, soll die konkrete Funktionsweise zweier DFS erklärt werden. Da das später vorgestellte Konzept auf den Prinzipien der beiden DFS aufbaut, werden die Annahmen der Systeme sowie ein Lese- und Schreibzugriff auf das DFS ausführlich beschrieben. Die beiden betrachteten DFS sind das GFS und Haystack.
Das GFS wurde 2003 im Artikel \textit{The Google File System} vorgestellt \cite{GFS}. Demnach wurde es von Google entwickelt, um die Analyse und Verwaltung ihrer extrem schnell wachsenden Datenmenge zu ermöglichen. Wie alle DFS wurde das GFS ganz im Sinne guter Skalierbarkeit, Verfügbarkeit, Verlässlichkeit und Leistung konzipiert. Aber auch andere Annahmen wurden beim Design des GFS berücksichtigt. So sind Ausfälle oder korrupte Daten bei einem System in dieser Größenordnung eher die Regel als die Ausnahme. Dateien sollen, nachdem sie einmal geschrieben wurden, nur noch gelesen werden. Ein Anhängen von Daten an bereits bestehende Dateien soll aber möglich sein. Die zu verwaltenden Dateien sind in der Regel sehr groß, von einigen Megabyte bis zu mehreren Gigabyte. Demnach muss das System mit Lese- und Schreibzugriffen auf große Datenmengen zurecht kommen. Da kleine Dateien eher die Ausnahme sind, wird dem Lesen und Schreiben dieser keine besondere Aufmerksamkeit geschenkt. Das GFS soll auch mit den Anfragen von sehr vielen Clients umgehen können. Daraus folgt, dass der konkurrierende Zugriff auf eine Datei besonders berücksichtigt werden muss.  

Das GFS stellt den Clients ähnliche Operationen wie gewöhnliche Filesysteme zur Ver\-fü\-gung. Dateien können geöffnet, geschlossen, gelesen und geschrieben werden und sind in Pfaden und Ordnern strukturiert. Zusätzlich können Daten an bestehende Dateien angehängt und Snapshots, also Kopien von Dateien, erzeugt werden. Das GFS selbst besteht dabei aus zwei Hauptkomponenten: einem einzigen Master und mehreren Chunkservern. Dateien werden in 64 Megabyte große Chunks mit jeweils 64 Byte Metadaten aufgeteilt und auf den Chunkservern platziert. Der Master verwaltet dabei den Namensraum für die einzelnen Dateien und Chunks. Außerdem kennt er für jede Datei die dazugehörenden Chunks mit ihren Positionen und den Positionen ihrer Replikas. Für den schnellen Zugriff werden diese Informationen immer im Hauptspeicher des Masters vorgehalten. Um beim Starten des Masters oder im Fehlerfall die Zuweisungen von Datei zu Chunk wiederherstellen zu können, werden alle Änderungen an der Struktur und des Namensraums persistent in einem Operation-Log gespeichert. Dieses agiert als Timeline und weist jeder Änderung einen eindeutigen Zeitstempel zu. Ein einzelner Master ermöglicht eine sehr ausgeklügelte Chunk-Platzierung auf den Chunkservern und vereinfacht das gesamte Design des Systems. Auf der anderen Seite, kann er aber auch zum Flaschenhals werden. Deswegen ist es sehr wichtig, die Anfragen der Clients an den Master so gering wie möglich zu halten. Es werden keine Schreib- und Lesezugriffe auf Dateien direkt durch den Master durchgeführt.

\begin{figure}[htb]
  \centering
  \includegraphics[width=15cm]{img/4/ReadGFS.png}
  \caption[Lesezugriff auf einen Chunk in GFS]{Lesezugriff auf einen Chunk in GFS. Nachempfunden nach \cite{GFS}.}
  \label{readgfs}
\end{figure}

Der Ablauf eines Lesezugriffes ist in Abbildung \ref{readgfs} zu sehen. Der Client weiß, welchen Teil einer Datei er lesen möchte. Aus der bekannten Chunkgröße kann so der Index des benötigten Chunks errechnet werden. Auf eine Anfrage an den Master mit dem Dateinamen und dem Chunkindex antwortet dieser mit der Position des Chunks und all seiner Replikas. Der Client kann nun einen Chunk, der ihm am nächsten liegt, auswählen und die Anfrage an den entsprechenden Chunkserver stellen. Dabei kann er auch übertragen, welche Bytes er innerhalb des Chunks erhalten möchte. Der Chunkserver antwortet dem Client daraufhin mit den angefragten Daten. Da sich die Position der einzelnen Chunks in der Regel nicht ändert, können die Clients diese Information cachen und bei weiteren Lesezugriffen darauf zurückgreifen. Auch das entlastet den Master.

\begin{figure}[htb]
  \centering
  \includegraphics[width=15cm]{img/4/writeGFS.png}
  \caption[Schreiben eines Chunks in GFS]{Ablauf eines Schreibvorgangs in GFS. Nachempfunden nach \cite{GFS}.}
  \label{writeGFS}
\end{figure}

Ein Schreibzugriff hingegen ist in Abbildung \ref{writeGFS} zu sehen. Um zu gewährleisten, dass alle Replikas eines Chunks irgendwann identisch sind, ist es wichtig, dass die Schreiboperationen in der gleichen Reihenfolge durchgeführt werden. GFS realisiert dies durch den Einsatz von Leases. Wenn ein Client eine Datei schreiben möchte, fordert er vom Master ein Lease auf einen Chunk an. Ein Lease hat eine Dauer von 60 Sekunden, kann aber bei Bedarf auch verlängert werden. Der Master gewährt dem Client ein Lease und sendet zusätzlich die Positionen des zu beschreibenden Chunks und seiner Replikas. In GFS gibt es von jedem Chunk standardmäßig drei Replikas. Einer dieser Chunks wird für die Dauer des Leases als Primary gekennzeichnet, die anderen werden zu Secondarys. Daraufhin beginnt der Client, die zu speichernden Daten zu den Replikas zu senden. Dabei ist die Reihenfolge, in der dies geschieht, egal. Es können auch erst die Secondarys und dann der Primary angesprochen werden. Sobald eine Replika Daten erhält, kann diese die Daten zur nächsten weitersenden. Die Daten werden zunächst nur im Hauptspeicher gehalten und nicht persistent abgespeichert. Erst, wenn die Daten bei allen Replikas angekommen sind, sendet der Client einen Schreibauftrag an den Primary Chunk, der die vorangegangenen Daten identifiziert. Der Primary Chunk verteilt diesen Schreibauftrag dann an alle Replikas, die daraufhin die Daten persistent ablegen. Jeder Secondary bestätigt dem Primary, dass die Daten erfolgreich geschrieben wurden, oder teilt einen Fehler mit. Der Primary antwortet letztendlich dem Client und zeigt entweder den Erfolg der Operation an oder sendet die aufgetretenen Fehler. Möchte der Client während der Gültigkeit des Leases ein weiteres Mal Daten schreiben, so muss keine Anfrage mehr an den Master gestellt werden und die Daten können direkt geschrieben werden. Dieser Prozess ermöglicht auch einen einfachen Umgang mit einem zweiten Client, der auf den gleichen Chunk schreiben möchte. Wenn der zweite Client nach einem Lease für den Chunk anfragt, erhält er als Antwort das gleiche Lease und demnach den gleichen Primary wie der erste Client. Auch der zweite Client beginnt, die Daten in beliebiger Reihenfolge auf die Replikas zu streamen. Danach teilt er dem Primary den Schreibauftrag für die geschriebenen Daten mit. Da die Daten erst zum Ende des Leases persistent geschrieben werden, kann der Primary durch Kommunikation mit den Replikas sicherstellen, dass die Daten beider Clients in der richtigen Reihenfolge in den Replikas landen. So kann das Problem mehrerer konkurrierender Zugriffe leicht gelöst werden. Jedoch kann es bis zu 60 Sekunden dauern, bis ein Client tatsächlich den als letzten geschriebenen Wert eines Chunks auslesen kann.

Das Anfertigen von Replikas für jeden Chunk hat neben der Ausfallsicherheit auch noch performanztechnische Vorteile. Clients können immer die Replika lesen, die ihnen am nächsten ist. Wollen mehrere Clients die gleichen Daten lesen, können die Lesezugriffe auf alle bestehenden Replikas verteilt werden. Das GFS nutzt ausgeklügelte Mechanismen, um korrupte Replikas zu erkennen und zu ersetzen. Dabei werden Chunks mit besonders viel gelesenen Daten oder vielen korrupten Replikas am höchsten priorisiert. In regelmäßigen Heartbeat-Nachrichten teilen die Chunkserver dem Master mit, in welchem Zustand sich die einzelnen Chunks befinden. Ein weiteres Problem ist der einsame Master. Dieser stellt einen Single Point of Failure da. Um diesem entgegenzuwirken, werden parallel zum Master mehrere Shadow Master betrieben. Diese hinken dem Master immer einige Operationen hinterher, können aber über das Operation Log immer auf den korrekten Stand gelangen. Inkonsistenz herrscht innerhalb des GFS also nur, wenn ein Chunk gelesen wird, auf dem gerade ein Lease angefordert wurde, – und im Falle eines Ausfalls des Masters – bis der Shadow Master die letzten Operationen aus dem Operation Log durchgeführt hat.

Ähnlich wie Google entwickelte auch Facebook ein eigenes DFS. Ihr Ziel war es, die unzähligen Bilder, die Tag für Tag auf ihr soziales Netzwerk geladen werden, zu speichern und zu verwalten. 2010 beschrieben einige Entwickler von Facebook in dem Artikel \textit{Finding a Needle in Haystack: Facebook's Photo Storage} ihr DFS Haystack \cite{haystack}. 2010 verwaltete dieses bereits etwa 20 Petabytes an Daten. Das System ist in der Lage, mehr als eine Million Lesezugriffe in der Sekunde zu bedienen und einer Milliarde hochgeladener Fotos in der Woche Herr zu werden. Außer auf Skalierbarkeit, Ausfallsicherheit, Verfügbarkeit und Performance wurde bei Haystack ein besonderes Augenmerk auf die Reduzierung der Metadaten pro Datei gelegt. Für die Speicherung von vier Bildern benötigt Haystack gerade einmal 40 Bytes an Metadaten. Gewöhnliche Filesysteme würden dafür 536 Byte anlegen \cite{haystack}. Dadurch sollen alle Metadaten immer im Hauptspeicher gehalten werden und einen schnellen Zugriff ermöglichen. Um dies zu erreichen, wurde auf ein POSIX-konformes Filesystem verzichtet. In Haystack gibt es keine Pfade oder Ordner, für die Metadaten gehalten werden müssen. Bilder werden eindeutig über Schlüssel identifiziert. Ansonsten wurde Haystack unter ähnlichen Annahmen wie das GFS entwickelt. Hardware-Ausfälle sind an der Tagesordnung, es müssen sehr viele Daten gespeichert werden und das System wird von sehr vielen Clients genutzt. Die gewöhnliche Dateigröße unterscheidet sich hingegen von den Anforderungen des GFS. Die hochgeladenen Bilder sind meist nur einige Kilobyte groß und damit vergleichsweise klein. In Haystack werden deshalb mehrere Bilder in einer großen Datei zusammengefasst. Diese Datei wird Volume genannt. Zu Beginn werden leere Volumes mit einer festen Größe angelegt und nach und nach mit Bildern aufgefüllt. Zehn Terabyte an Speicher können so in 100 Volumes mit eine Größe von jeweils 100 Gigabyte aufgeteilt werden.

\begin{figure}
  \centering
  \includegraphics[width=5cm]{img/4/volume.png}
  \caption[Aufbau eines Haystack-Volumes]{ Aufbau eines Volumes. Nachempfunden nach \cite{haystack}.}
  \label{volume}
\end{figure}
Haystack besteht aus drei Systemelementen: dem Haystack-Directory, Haystack-Store und dem Haystack-Cache. Im Folgenden wird auf diese ohne das Präfix Haystack Bezug genommen.
\begin{itemize}
  \item \textbf{Directory:} Die Aufgabe des Directorys ist die Verwaltung aller Volumes. Um die Replikation der Daten zu vereinfachen, fasst das Directory mehrere physikalische Volumes in ein logisches Volume zusammen. Beim Schreibvorgang wird nur noch ein logisches Volume referenziert und in alle darin enthaltenen physikalischen Volumes geschrieben. Zusätzlich kann das Directory Volumes auf read-only setzen, wenn diese ihre Kapazität erreicht haben oder zum Teil korrupt sind.
  \item \textbf{Cache:} Um viele Anfragen direkt aus dem Hauptspeicher beantworten zu können, werden alle Bilder, die das System verlassen, im Cache gespeichert. Dadurch werden vor allem kurz nach dem Hochladen eines neuen Fotos die Maschinen entlastet, denn neue Bilder werden häufiger gelesen als ältere.
  \item \textbf{Store:} Der Store speichert die tatsächlichen Bilddaten. Er besteht ähnlich wie die Chunkserver beim GFS aus mehreren Store Machines. Dabei kümmert sich eine Store Machine um mehrere Volumes, welche jeweils mehrere Millionen Bilder enthalten können. Der Aufbau eines Volumes ist in Abbildung \ref{volume} dargestellt. Demnach steht am Anfang immer ein Superblock, der Informationen zur Anzahl der freien Blöcke und der Größe des Dateisystems beinhaltet \cite{wiki:superblock}. Diesem folgen mehrere Needles, die jeweils ein Bild repräsentieren. Der Zugriff auf ein Bild erfolgt über Angabe des Volume-Schlüssels, eines Offsets und der Größe der zu lesenden Daten. All diese Daten werden von der Store Machine im Speicher gehalten. Zusätzlich wird diese Information in einem Volume Index File gesichert. Das verhindert, dass beim Starten einer Store Machine alle Volumes gelesen werden müssen, um die benötigten Informationen in den Speicher zu schieben. 
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{img/4/ReadHaystack.png}
  \caption[Ablauf beim Lesen eines Bildes in Haystack.]{ Ablauf beim Lesen eines Bildes in Haystack. Nachempfunden nach \cite{haystack}.}
  \label{readHaystack}
\end{figure}

In Abbildung \ref{readHaystack} sind die Schritte zum Lesen eines Bildes in Haystack schematisch dargestellt. Zunächst fragt der Client das Directory nach dem gewünschten Bild. Dieses wird durch einen eindeutigen Schlüssel identifiziert, der die Form \textit{[logisches-Volume-ID, Bild-ID]} aufweist. Über das logische Volume kann das Directory sehr leicht ein physikalisches Volume heraussuchen und die Adresse der Store Machine, die dieses beherbergt, an den Client senden. Der Client sendet jetzt erneut den Schlüssel des Bildes, aber diesmal an die Adresse der Store Machine. Diese Anfrage trifft den Cache. Wurde das Bild zuvor bereits einmal ausgeliefert, kann dieser direkt mit den Bilddaten antworten. Befindet sich das Bild nicht im Cache, so wird die Anfrage zur Store Machine weitergeleitet. Diese sucht mithilfe des Schlüssels für das Bild und des logischen Volumes das physikalische Volume, die Größe des Bildes und den benötigten Offset heraus. Damit kann das Bild aus dem physikalischen Volume gelesen und über den Cache zum Client geschickt werden. 

Wie in Haystack ein Bild gespeichert wird, ist in Abbildung \ref{writeHaystack} zu sehen. Im ersten Schritt fragt der Client das Directory nach einem beschreibbaren logischen Volume. Dieses antwortet mit der ID des logischen Volumes und den Adressen der Store Machines, die die im logischen Volume enthaltenen physikalischen Volumes beherbergen. Daraufhin vergibt der Client einen eindeutigen Schlüssel für das hochzuladende Bild. Der Schlüssel entspricht der oben erklärten Form. Gemeinsam mit dem Schlüssel überträgt der Client dann die Bilddaten an alle vom Directory erhaltenen Store Machines. Diese hängen das Bild in den entsprechenden Volumes an und aktualisieren die Volume-Information in ihrem Hauptspeicher und im Volume Index File. Da Bilder in Volumes immer nur an die bestehenden Bilder angehängt werden können, ist eine Modifikation eines bereits gespeicherten Bildes nicht möglich. Um ein solches Bild zu überschreiben, sendet der Client ein neues Bild mit dem gleichen Schlüssel wie das zu überschreibende Bild. Bei der nächsten Anfrage nach diesem Bild wird die Store Machine immer das Bild zurückgeben, das den größten Offset innerhalb des Volumes besitzt, da dieses immer das aktuellste ist. Der Speicher, der für das ungültig gewordene Bild benötigt wird, bleibt weiterhin belegt. Um die Netzwerkbandbreite und die Leistung der Festplatten optimal auszunutzen, bemüht sich Haystack immer, mehrere Bilder auf einmal zu einem Volume hinzuzufügen. Dies wird möglich, da Nutzer von Facebook häufig Bilder in Alben auf die Plattform laden.

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{img/4/WriteHaystack.png}
  \caption[Ablauf beim Schreiben eines Bildes in Haystack.]{ Ablauf beim Schreiben eines Bildes in Haystack. Nachempfunden nach \cite{haystack}.}
  \label{writeHaystack}
\end{figure}

Damit der Speicher, der durch überschriebene oder korrupte Bilder belegt wird, nicht verloren ist, werden Volumes regelmäßig verdichtet. Dazu wird das Volume Bild für Bild kopiert und korrupte oder überschriebene Bilder übersprungen. Daraufhin wird das ursprüngliche Volume durch das neue ersetzt. Um auch bei korrupten Bildern eine gleichmäßige Replikation zu garantieren, kommunizieren Store Machines und Directory gelegentlich miteinander. Dabei teilen die Store Machines den Inhalt und den Zustand ihrer Volumes mit. Sind einige Needles korrumpiert, kann das Directory dem betroffenen logischen Volume ein neues physikalisches Volume zuordnen und die Replikation starten. Inkonsistenzen treten bei Haystack nur auf, wenn ein Bild angefordert wird, während es noch auf die einzelnen Store Machines verteilt wird, oder wenn ein korruptes Bild angefragt wird, bevor dieser Fehler von der zuständigen Store Machine erkannt wurde. Werden Bilder überschrieben, kann es vorkommen, dass Clients noch das ursprüngliche Bild aus dem Cache erhalten, bis dieses ungültig wird.

Haystack und das GFS ähneln sich in den Grundstruktur ihrer Systeme sehr. Die eigentlichen Daten werden auf extra dafür vorgesehene OSDs gespeichert und ein Master kümmert sich um deren Verwaltung. Beide DFS ermöglichen einen effizienten Zugriff auf Daten, der im besten Fall nur eine einzige Leseoperation von der Festplatte benötigt. Durch das Verteilen der Anfragen auf mehrere OSDs sind beide Systeme nahezu beliebig skalierbar und verfügen über eine enorme Ausfallsicherheit. Dennoch gibt es nicht nur bei den gespeicherten Datenarten Unterschiede. Während das GFS sehr große Dateien verwaltet und diese sogar in Chunks aufteilt, bemüht sich Haystack darum, mehrere Bilder in einer einzigen großen Datei, dem Volume zusammenzufassen. Demnach sind nicht nur die Schreib- sondern auch die Lesezugriffe bei Haystack deutlich kleiner als bei dem GFS. Der Größe der Dateien ist geschuldet, dass sich ein Cache im GFS nicht lohnt. Ein weiterer Unterschied ist in der Anordnung der Dateien zu finden. GFS liefert ein fast POSIX-konformes Dateisystem. Haystack hingegen verzichtet auf Pfade und Ordner komplett. Dadurch kann die Menge an Metadaten weiter reduziert werden. Dieser Schritt ist notwendig, da Haystack kleinere und damit mehr eigenständige Dateien verwaltet. 

\section{DFS zur Bewältigung der Anforderungen eines Buchungssystems}
Die Frage ist, ob sich ein DFS eignet, um die Anforderungen und Probleme eines Buchungssystems zu bewältigen.
Im Kapitel \ref{bookingSystem} wurden die aktuellen und kommenden Herausforderungen, denen sich ein Buchungssystem stellen muss, erörtert. Ob ein DFS diesen gewachsen ist, soll sich durch die in diesem Kapitel erarbeiteten Informationen zeigen \cite{largeHadoop}.


\begin{itemize}
  \item \textbf{Wohldefinierte Schnittstelle:} Die Entwicklung einer wohldefinierten Schnittstelle sowohl für bankeigene Prozesse als auch für Drittanbieter ist unabhängig von der genutzten Persistenzschicht. Die Realisierung eines Buchungssystems mit einem DFS als Grundlage sollte demnach für diese Anforderung kein Problem darstellen. Gegebenenfalls könnten einfache Anfragen sogar direkt die Schnittstelle des DFS nutzen und die Buchungssysteme weiter entlasten.

  \item \textbf{Skalierbarkeit:} Die grundlegende Systemarchitektur vieler OSDs und eines Masters, der diese verwaltet, gewährleistet eine einfache horizontale Skalierung. Speicherkapazität kann nahezu nach Belieben durch das Hinzufügen weiterer OSDs erhöht werden. Auch die Verarbeitung von Lese- und Schreibzugriffen profitiert von dieser Art der Skalierung. Dadurch, dass diese Anfragen nie direkt durch den Master gehen, wird die Last im Netzwerk sowie auf die OSDs verteilt.

  \item \textbf{Ausfallsicherheit:} Die in einem DFS abgelegten Daten werden immer auf mehreren OSDs gespeichert. Ausgeklügelte Algorithmen zur Platzierung der Replikas ermöglichen eine sehr hohe Fehlertoleranz und Ausfallsicherheit. Eine geographische Verteilung der Replikas ermöglicht sogar eine Datenwiederherstellung, wenn ganze Rechenzentren versagen \cite{osvideo}.

  \item \textbf{Verfügbarkeit:} Verfügbarkeit ist eine Kernkompetenz eines DFS. Wie alle anderen BASE-basierten Systeme, lockern DFS die Konsistenz, um Anfragen zu beantworten, auch wenn Teile des Systems unerreichbar sind.

  \item \textbf{Kosteneffizienz:} DFS sind entwickelt worden, um auf Standard-Hardware zu laufen. Die einfache horizontale Skalierung hält die Betriebskosten auch für große Systeme niedrig \cite{rdbmssuck}.

  \item \textbf{Leistung:} Viele Lesezugriffe auf ein DFS benötigen nur einen einzigen Zugriff auf die Festplatte. Informationen zu den Dateien und wo sie sich befinden werden in der Regel im Hauptspeicher vorgehalten. Replikas von Dateien und Chunks ermöglichen dem Client den Zugriff auf die ihm am nächsten liegende Datei sowie eine Lastverteilung. 
\end{itemize}

Diese Punkte erfüllen DFS nahezu mühelos. Das Gewährleisten eines konsistenten Zustands bringt ein DFS hingegen ins Schwitzen. Bevor ein Zahlungsverkehr autorisiert wird, sollte sichergestellt sein, dass der Schuldner über die benötigten Zahlungsmittel verfügt. Für den Halter eines Girokontos ist es auch von großer Bedeutung, dass der in einer Webanwendung angezeigte Kontostand dem tatsächlichen entspricht. Auch die geringe Datenmenge, die bei einer einzelnen Buchung gespeichert werden muss, ist nicht optimal. Informationen zu Gläubiger, Schuldner, Betrag, Währung und Beschreibung der Buchung können in wenigen hundert Bytes bis zu einem Kilobyte abgebildet werden. DFS sind in der Regel auf die Speicherung größerer Dateien ausgelegt. Der Artikel \textit{An optimized approach for storing and accessing small files on cloud storage} zeigt an dem HDFS die Probleme der Verwaltung vieler kleiner Dateien auf \cite{hdfsSmallFiles}. Das HDFS wurde stark von dem GFS inspiriert und unterscheidet sich nur geringfügig \cite{hdfsGfs}. Für jede Datei wird ein eigener Eintrag im Master erstellt. Dieser Eintrag soll für den effizienten Zugriff immer im Hauptspeicher liegen. Viele kleine Dateien benötigten demnach sehr viel mehr Hauptspeicher des Masters, als eine sehr große Datei mit der gleichen Menge an Inhalt. Auch die Beantwortung von Lesezugriffen auf viele kleine Dateien kann die Leistung eines DFS in die Knie zwingen. Abbildung \ref{hdfsSmallFiles} zeigt, wie lange das Herunterladen von 320 Megabyte dauert, je nachdem, auf wie viele Dateien die Datenmenge verteilt ist. 5120 Dateien mit jeweils 64 Kilobyte herunterzuladen, dauert vier und ein viertel mal so lange wie der Download von 40 acht Megabyte großen Dateien. Diese Verzögerung kommt vor allem durch die für jede Datei benötigte Anfrage an den Master \cite{hdfsSmallFiles}. Das effiziente Ablegen vieler kleiner Dateien benötigt demnach eine gesonderte Behandlung.



\begin{filecontents}{date5.dat}
date  value
1   15
2   9
3   6
4   5
5   4
6   3.8
7   3.7
8   3.6
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
xtick=data,
xmin=0, xmax=9,
ymin=0, ymax=16,
xticklabels={64KB,128KB,256KB,512KB,1MB,2MB,4MB,8MB},
xticklabel style = {font=\tiny},
xlabel={Dateigröße},
ylabel={Zeit zum Herunterladen in Sekunden},
]
\addplot table[x=date,y=value] {date5.dat};
\end{axis}
\end{tikzpicture}
\caption[Zeit zum Herunterladen unterschiedlicher Dateigrößen in HDFS]{Zeit zum Herunterladen unterschiedlicher Dateigrößen in HDFS bei gleichem Datendurchsatz (320 MB). Nachempfunden nach \cite{hdfsSmallFiles}.}
\label{hdfsSmallFiles}
\end{center}
\end{figure}

\cleardoublepage
\chapter{Konzept}
\label{concept}
Der Erhalt eines konsistenten Zustandes, sowie die geringe Datenmenge einer einzelnen Buchung scheinen nicht mit den Möglichkeiten eines DFS vereinbar. Wenn der Kontostand eines Kontos nicht sicher bestimmt werden kann, kann die Durchführung einer Buchung den Kontostand unter null bringen. Das darf aber zum Beispiel bei einem Girokonto nicht passieren. Aber gelten diese Bedingungen denn für alle Kontoarten? In diesem Kapitel soll ein Konzept entwickelt werden, das die Stärken eines DFS für ein Buchungssystem zugänglich macht. Dabei wird die Relevanz der strong consistency in Frage gestellt und eine Lösung für das Speichern der kleinen Datenmenge einer Buchung erarbeitet. Zuletzt soll ein Designvorschlag für das Buchungssystem gemacht werden, der eine extreme Skalierbarkeit sowie ausreichende Konsistenz liefert.

\section{Konsistenz und Kontoarten}
\label{konsistenzKonten}
Strong consistency in einem Buchungssystem scheint unabdingbar. Wenn man sich aber auf die Bedeutung der eventual consistency besinnt, wird klar, dass ein DFS auch irgendwann konsistent ist – immer wenn für eine bestimmte Zeit keine Schreibzugriffe getätigt wurden. 
Inkonsistenzen treten also nur im Rahmen weniger Sekunden bis zu einer Minute nach der Durchführung einer Buchung auf. Gibt es demnach keine Konten, für die diese Konsistenz ausreichen würde? Problematisch sind alle Konten, bei denen regelmäßig Geld ein- und ausgeht und ein vorgegebener Saldo nicht unterschritten werden darf. Bei einem Girokonto zum Beispiel darf der Kontostand nicht unter null fallen. Durch einen Dispositionskredit kann der Kontostand zwar negativ werden, aber auch nur bis zu einem bestimmten Betrag. Mit einer eventual consistency kann dieses Verhalten aber nicht garantiert werden. Wenn zum Zeitpunkt einer Buchung kein konsistenter Zustand vorliegt, kann nicht festgestellt werden, ob das Konto noch über genügend Zahlungsmittel verfügt. Würden Buchungen immer nur zu bekannten Zeitpunkten eingehen, könnte man sicher gehen, dass zuvor ein konsistenter Zustand hergestellt wird. Auch Konten, die beliebig überzogen werden können, könnten abgebildet werden. Hier spielt es keine Rolle, ob ein bestimmter Betrag unterschritten wurde, und Buchungen können ohne Berücksichtigung des Kontostandes ausgeführt werden. Ein Girokonto, das auf diese Art funktioniert, schiebt die Verantwortung ein bestimmtes Saldo des Kontos nicht zu unterschreiten zum Kunden. Für diesen birgt das die Gefahr starker Verschuldung.

Konten, die eine starke Konsistenz also nicht zwingend benötigen, sollten nicht einfach von außerhalb der Bank zugänglich sein. Auch Konten, die lediglich Zahlungen empfangen und selbst keine durchführen, würden mit einer gelockerten Konsistenz zurechtkommen. Wird kein Geld abgehoben, kann der Kontostand auch nicht sinken. Aber auch wenige kontrollierte Abbuchungen sind mit eventual consistency realisierbar. Wenn Abbuchungen nur in sehr großen Abständen oder einmalig erfolgen, kann sichergestellt werden, dass zum Zeitpunkt der Buchung der Zustand konsistent ist. All diese Punkte treffen auf die Konten zu, die komplett in der Verantwortung der Bank liegen. Von den Passivgeschäften lässt sich die Termineinlage sehr gut mit einem DFS abbilden. In der Regel handelt es sich hierbei um eine einmalige Einzahlung des Bankkunden, bei der bekannt ist, wann der Betrag plus Zinsen wieder ausgezahlt wird. Für die Dauer der Termineinlage liegt die Verwaltung der Zahlungsmittel bei der Bank. Diese ist durchaus in der Lage, Buchungen nur zu bestimmten Zeitpunkten durchzuführen und sicherzustellen, dass das Kontosaldo nicht unter null fällt. Besonders gut eignen sich jedoch die Aktivgeschäfte für die Abbildung ohne strong consistency. Bei allen Formen des Ratenkredites, gibt es auf Seiten der Bank ein Konto, auf welches der Kreditnehmer seine Raten einzahlen muss. Dieses Konto erfährt nur einen Geldzufluss und zum Ende des Kredits wird der Gesamtbetrag auf ein weiteres Konto der Bank gebucht. Für jede Termineinlage und für jeden Ratenkredit wird also ein Konto angelegt, welches relativ wenige Kontobewegung erfährt und dessen Verwaltung in den Händen der Bank liegt. Wie Abbildung \ref{ratenkredite} zeigt, ist die Anzahl der Konten, die allein für die Ratenkredite angelegt werden, beachtlich. Auch wenn jedes dieser Konten an sich die starke Konsistenz der relationalen Datenbanken nicht benötigt, liegen sie in den gleichen Systemen wie die Girokonten und beanspruchen Speicher und Leistung der relationalen Datenbanken. Alleine die Speicherung der Konten für Termineinlagen und Ratenkredite in einem DFS könnte den Banken Kosten sparen und Druck von den Buchungssystemen nehmen.

\begin{filecontents}{date6.dat}
date       value
2008-01-01  6909
2009-01-01  7611
2010-01-01  7272
2011-01-01  7183
2012-01-01  7697
2013-01-01  7737
2014-01-01  7434
2015-01-01  7442
\end{filecontents}


\begin{figure}[htb]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahre},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anzahl der Kreditverträge in Tausend},
date ZERO=2008-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date6.dat};
\end{axis}
\end{tikzpicture}
\caption[Anzahl der Ratenkreditverträge in Deutschland]{Anzahl der Ratenkreditverträge in Deutschland. Nachempfunden nach \cite{ratenkredite}.}
\label{ratenkredite}
\end{center}
\end{figure}

\section{Annahmen für den Entwurf eines DFS}
Das hier entwickelte Konzept soll aber von der Skalierbarkeit und Leistung auch in der Lage sein, alle anderen Kontoarten abzubilden. Eine Lösung, um die benötigte Konsistenz zu gewährleisten, soll im weiteren Verlauf des Kapitels erarbeitet werden. Zunächst werden einige Annahmen getroffen, nach denen ein DFS und das darauf aufgesetzte Buchungssystem entworfen wird. Die wichtigste Aufgabe eines DFS in einem Buchungssystem ist das persistente Abspeichern aller Buchungen, die für alle Konten eingehen. Eine Buchung besteht im Rahmen dieser Arbeit aus dem Namen des Empfängers und des Senders, dem IBAN und dem BIC des Empfängers, dem zu übermittelnden Geldbetrag sowie der Währung, einem Ausführungsdatum und einem Freitextfeld für eine Beschreibung der Buchung. Die Datenmenge einer Buchung sollte daher ein Kilobyte nicht überschreiten. Das DFS muss mit sehr vielen von diesen Buchungen umgehen können, auch wenn mehrere für ein Konto nahezu parallel eintreffen – zum Beispiel, wenn eine Überweisung und ein Dauerauftrag auf den gleichen Zeitpunkt fallen. Anfragen werden durch die Schnittstellen zu Online- und Mobile-Banking sowie der PSD2 von einer Vielzahl an Clients gestellt, wobei zu erwarten ist, dass die Mehrheit davon Lesezugriffe sind. Dabei wird selten nur eine konkrete Buchung ausgelesen und häufiger alle Buchungen eines gewissen Zeitraums für ein Konto angefordert, beispielsweise alle Buchungen im letzten Monat oder alle Buchungen im letzten halben Jahr. Damit die Bank ihren Funktionen nachgehen und dem Kunden ein gutes Nutzererlebnis bieten kann, muss eine hohe Verfügbarkeit des Buchungssystems gewährleistet sein. Können keine Buchungen durchgeführt werden, ist damit direkt ein Gewinnverlust und Imageschaden der Bank verknüpft. Auch wenn Teile des Systems nicht erreichbar sind, muss das Buchungssystem weiter funktionieren können. Bei Systemen, die mehrere Milliarden Buchungen im Jahr bearbeiten, sind Hardware-Ausfälle und Fehler an der Tagesordnung. 
GFS eignet sich nicht für das persistente Abspeichern der Buchungen, da es auf größere Dateien ausgelegt ist. Für jede Datei wird ein Chunk angelegt, welcher 64 Byte im Hauptspeicher des Masters belegt. Wenn der Master über 16 Gigabyte Hauptspeicher verfügt und für jede Buchung ein Chunk angelegt werden muss, könnten so gerade einmal 250 Millionen Buchungen abgelegt werden, bevor die Grenzen des Masters erreicht wären. Nach Abbildung \ref{bargeldlos} wäre das nur ein Bruchteil der Buchungen, die im Jahr 2011 durchgeführt wurden. Haystack passt etwas besser auf die erarbeiteten Annahmen für das DFS. Die in Haystack verwalteten Bilder sind häufig auch nur wenige Kilobyte groß. Jedoch bietet Haystack keine Möglichkeit, mehrere Bilder auf einmal auszulesen. Jeder Zugriff erfordert den exakten Schlüssel des Bildes. Ein Auslesen aller Bilder, die in einem gewissen Zeitraum hochgeladen wurden, ist nicht möglich. Auch der Cache ist für ein Buchungssystem nicht relevant. Buchungen werden immer nur vom Besitzer des entsprechenden Kontos angefragt. Dass eine Buchung mehrmals nacheinander benötigt wird, ist deshalb unwahrscheinlich.

\section{DFS für kleine Daten}
\label{oneOnlyDFS}
Ein DFS, das für ein Buchungssystem geeignet ist, muss in der Lage sein, viele kleine Dateien beziehungsweise Buchungen abzulegen und auszulesen, wobei besonders das Auslesen mehrerer Buchungen am Stück relevant ist. Auch der Zeitraum, in dem das System inkonsistent ist, soll möglichst klein ausfallen. Wie bereits beschrieben eignen sich sowohl GFS als auch Haystack nicht optimal für diesen Anwendungszweck. Durch den gezielten Einsatz einiger Techniken der beiden DFS sowie von Ansätzen aus mehreren Artikeln ist die Entwicklung eines DFS, das all diese Anforderungen erfüllt, jedoch durchaus möglich \cite{hdfsSmallFiles}\cite{hadoopSmallFiles2}\cite{smallFilesHDFS3}.

Der generelle Aufbau des DFS besteht wieder aus mehreren OSDs, die die Daten verwalten, und einem Master, der wiederum die OSDs verwaltet. Viele kleine Dateien sollen aufgrund ihres logischen Zusammenhangs gruppiert und zusammengefügt werden. Dateien hängen logisch zusammen, wenn sie sich ein gemeinsames Überthema teilen. In einem traditionellen Filesystem ist das häufig der Fall, wenn Dateien im gleichen Ordner liegen. Beispiele sind alle Bilder, die in einer Stadt aufgenommen wurden, oder alle Musikstücke eines Künstlers oder auch alle Buchungen eines Kontos, die in einem gewissen Zeitraum getätigt wurden. Dateien, die einen solchen logischen Zusammenhang besitzen, werden häufig gemeinsam angefragt. Daher sollen hier mehrere Buchungen in eine gemeinsame Datei eingefügt werden. Diese Datei ist vergleichbar mit einem Volume bei Haystack. Um Verwirrung zu vermeiden, wird sie im Folgenden jedoch Bucket genannt. Ein Bucket hat immer eine feste Größe und wird durch das Einfügen von Daten gefüllt. Da das DFS mit sehr kleinen Daten umgehen soll, wird die Größe eines Buckets auf 16 Megabyte festgelegt. Geht man von etwa einem Kilobyte an Daten pro Buchung aus, so kann ein Bucket 16000 Buchungen halten. Um einen Bucket innerhalb eines Jahres zu füllen, müssten über 43 Buchungen am Tag eingehen. Eine Vielzahl an Konten lässt sich also mit nur einem einzigen Bucket abdecken. Aber auch Konten mit sehr vielen Buchungen, wie zum Beispiel von einem Online-Händler, können auf nur wenige Buckets verteilt werden. Die Verwaltung von mehreren kleinen Einträgen in einer großen Datei ist zwingend erforderlich, um den Hauptspeicher des Masters zu entlasten. Dieser kennt immer nur die Adresse des Buckets und nicht dessen gesamten Inhalt. Ein weiterer Vorteil der Gruppierung von logisch zusammenhängenden Daten in einer großen Datei ist, dass Lesezugriffe nur einen einzigen Zugriff auf die Festplatte benötigen, um mehrere Daten in den Hauptspeicher zu laden und die Anfrage des Clients zu beantworten. Bei einer kleineren Bucket-Größe werden mehr Buckets benötigt, um die gleiche Datenmenge zu verwalten, und es wird mehr Hauptspeicher des Masters belagert. Wählt man eine größere Bucket-Größe, so wird der Master entlastet, aber die OSDs müssen mehr Daten durchsuchen, um einen Eintrag zu finden. Die Bucket-Größe von 16 Megabyte liefert ein sinnvolles Verhältnis von Leseaufwand der OSDs und Verwaltungsaufwand des Masters. Nur wie kann der Master wissen, in welchem Bucket sich welche Daten befinden? Alle abgelegten Daten erhalten einen streng aufsteigenden, acht Byte großen Schlüssel. Dieser Schlüssel gibt den Zeitpunkt der Erstellung bis auf Nanosekunden genau wieder. Die Adresse eines Buckets beinhaltet immer den Schlüssel und somit den Zeitpunkt der ältesten Daten, die innerhalb des Buckets abgelegt wurden. Werden weitere Daten in diesen Bucket eingefügt, so werden diese streng aufsteigend eingegliedert. Bevor ein neuer Bucket angelegt wird, muss immer erst der vorherige komplett aufgefüllt werden. Fordert der Client nun Daten mit einem Schlüssel an, muss nur der Bucket gefunden werden, dessen Adresse entweder gleich dem gegebenen Schlüssel ist oder am nächsten vor dem gegebenen Schlüssel liegt. Wenn bekannt ist, in welchem Bucket sich die benötigten Daten befinden, kann dieser gelesen und der entsprechende Abschnitt zurückgeliefert werden. Damit aber nicht bei jeder Anfrage der komplette Bucket durchlaufen werden muss, wird wie bei Haystack ein Bucket Index File angelegt. Im Gegensatz zu Haystack wird darin aber nicht jeder einzelne Eintrag festgehalten, sondern nur einige Checkpoints abgespeichert. Dies geschieht immer, wenn ein gewisser Füllstand erreicht wurde. Standardmäßig soll ein 16 Megabyte großer Bucket mindestens 16 mal unterteilt werden. Der Aufbau eines Buckets und des dazugehörigen Bucket Index Files ist schematisch in Abbildung \ref{bucket} zu sehen. Der Inhalt des Bucket Index Files wird von den OSDs im Hauptspeicher gehalten. Das Einfügen nur weniger Checkpoints ermöglicht entgegen der Indizierung jedes einzelnen Eintrags, dass sehr viele Buckets auf einem OSD gehalten und effizient verwaltet werden können. Ein Eintrag im Bucket Index File speichert den acht Byte großen Schlüssel, plus drei Byte für den Offset innerhalb des Buckets. Die Größe der Daten muss nicht mit abgelegt werden. Jeder Eintrag im Bucket beginnt mit seinem Schlüssel gefolgt von zwei Byte für die Größe der abgelegten Daten. Beim Suchen bestimmter Daten mittels eines Schlüssels wird immer von dem Checkpoint gestartet, der am nächsten vor dem Schlüssel liegt und von dort aus Eintrag für Eintrag gelesen. Für den Zugriff auf beliebige Einträge müssen bei einer durchschnittlichen Dateigröße von einem Kilobyte also bis zu 1000 Einträge gelesen werden, bis der richtige gefunden ist. Dieser Suchvorgang fällt aber beim Lesen mehrerer Einträge am Stück nicht mehr besonders ins Gewicht.

\begin{figure}
  \centering
  \includegraphics[width=10cm]{img/5/bucket.png}
  \caption[Aufbau eines Buckets und dem Bucket Index Files]{ Aufbau eines Buckets (links) und eines Bucket Index Files (rechts).}
  \label{bucket}
\end{figure}

Um Buckets effizient zu nutzen, muss das DFS aber auch in der Lage sein, logisch zusammenhängende Daten zu erkennen. Die einfachste Möglichkeit wäre die Verwendung von Ordnern und Pfaden. Alle Dateien, die sich innerhalb eines Ordners befinden, werden gemeinsam in einen Bucket abgelegt. Die Verwaltung von Ordnern und Pfaden beansprucht aber Logik und Hauptspeicher des Masters. Das Verschieben von Dateien innerhalb der Ordner, das Umbenennen von Ordnern oder das Anlegen neuer Unterordner ist aber vielleicht gar nicht nötig. Eine Technik, die den logischen Zusammenhang in den Schlüssel von Daten fest kodiert, nennt sich Namespace Flattening \cite{smallfilesObjectStorage}. Im Falle von Buchungen für Konten bietet es sich an, im ersten Teil des Schlüssels BIC und IBAN aneinanderzuhängen, gefolgt von dem zweiten Teil des Schlüssels, der einem acht Byte großen Timestamp entspricht. Alle Daten, die beim Ablegen einen identischen ersten Teil des Schlüssels besitzen, landen im gleichen Bucket. Zusätzlich kann nur nach dem ersten Teil des Schlüssels gefragt werden, um alle Buckets zu erhalten, die auf den Schlüssel bezogen logisch zusammenhängende Daten beinhalten. Durch das Namespace Flattening kann wichtiger Hauptspeicher auf dem Master gespart werden. Dieser benötigt mit diesem Ansatz ingesamt 65 Byte Hauptspeicher für einen Bucket. Für BIC und IBAN 45 Byte, für die Adresse der drei Replikas des Buckets jeweils vier Bytes und für den zweiten Teil des Schlüssels, den Timestamp, acht Bytes \cite{bic}\cite{iban}.

Die oben beschriebenen Ansätze ermöglichen das Abspeichern vieler kleiner Daten und das effiziente Auslesen mehrerer zusammenhängender Daten. Ein weiteres Problem, das sehr viele kleine Dateien mit sich bringen, ist die Last der Anfragen auf den Master. Für jeden Lesezugriff muss der Master nach der Position der Datei gefragt werden. Zum Großteil kann dieses Problem bereits durch die Gruppierung mehrerer Daten in eine große Datei mit streng aufsteigendem Schlüssel behoben werden. Wenn aber Lesezugriffe stattfinden, die über die Grenzen von Buckets hinausgehen, ist der Master aktuell immer ein weiteres mal involviert. Dieser Zugriff auf den Master kann jedoch durch das sogenannte Prefetching verhindert werden \cite{smallfilesObjectStorage} \cite{smallFilePrefetching}. Prefetching bezeichnet gewöhnlich das clientseitige Anfragen und Speichern von Daten, bevor sie eigentlich benötigt werden. Das setzt voraus, dass der Client genau weiß, welche Daten er überhaupt anfragen könnte. In einem Buchungssystem ist das aber nicht der Fall. Der Client ist sich nicht bewusst, welche Buchungen in welchen Buckets liegen. Daher soll das hier entwickelte DFS ein serverseitiges Prefetching implementieren. Bei jeder Anfrage nach einzelnen Buchungen oder einem zeitlich zusammenhängenden Block an Buchungen, soll der Master nicht nur die Adresse des Buckets mit den angefragten Daten zurückliefern, sondern auch die Adresse der nächsten Buckets zur Verfügung stellen. Für weitere Lesezugriffe muss dann nicht mehr der Master gefragt werden. Da BIC und IBAN hart in den Schlüssel für Daten einkodiert werden, ändern sich die Adressen der Buckets nur, wenn diese korrupt werden sollten. Solange das nicht der Fall ist, kann der Client diese Information auch im Cache behalten.

Nachdem die generelle Struktur des DFS erklärt wurde, werden nun die Schritte, die zum Schreiben und Lesen von Daten nötig sind, aufgezeigt.
Das Schreiben von Daten erfolgt nahezu identisch wie bei dem GFS. Der Client fragt mit dem ersten Teil des Schlüssels – also BIC und IBAN – beim Master nach einem beschreibbaren Bucket. Jeder Bucket besitzt standardmäßig drei Replikas. Der Master gewährt einen Lease auf einen Bucket und sendet die Adressen der Replikas zum Client. Eine dieser Replikas wird Primary, die anderen Secondarys. Der Client beginnt, in beliebiger Reihenfolge auf die Replikas zu streamen, und sendet erst danach den Schreibauftrag zum Primary. Es wird immer zunächst der zweite Teil des Schlüssels und die Größe der Datei in die Buckets geschrieben, gefolgt von den eigentlichen Nutzdaten. Beim Erreichen einer gewissen Füllmenge des Buckets, wird der geschriebene Eintrag im Bucket Index File als Checkpoint abgelegt und im Hauptspeicher des zuständigen OSDs hinzugefügt. Da es sich hier um kleine Datenmengen handelt, wird die Dauer eines Leases auf zehn Sekunden festgelegt. Durch diese Art des Schreibvorganges ergeben sich die gleichen Vorteile wie beim GFS. Die Reihenfolge in der Daten geschrieben werden, ist immer in allen Replikas der Buckets die gleiche, und der Zugriff durch einen zweiten Client stellt kein Problem für die Konsistenz dar.

Ein Lesezugriff läuft sehr ähnlich wie bei Haystack ab. Neben dem Lesen eines konkreten Eintrages über einen Schlüssel soll aber auch das Lesen mehrerer Einträge, die nach einem bestimmten Zeitpunkt abgelegt wurden, möglich sein. Beim Lesen eines einzelnen Eintrags über einen Schlüssel sendet der Client den Schlüssel an den Master. Dieser findet über den ersten Teil des Schlüssels heraus, in welchen Buckets sich die Daten potentiell befinden können. Über den zweiten Teil des Schlüssels kann der genaue Bucket identifiziert werden. Der Master antwortet dem Client mit der Adresse des Buckets und seiner Replikas für die angefragten Daten und zusätzlich mit der Adresse der zwei am nächsten anliegenden Buckets. Der Client kann jetzt mit dem Schlüssel bei dem OSD anfragen, der den Bucket beheimatet. Das OSD sucht in seinem Hauptspeicher für den zuständigen Bucket nach dem Checkpoint, der gleich ist oder am nächsten vor dem gegebenen Schlüssel liegt. Von diesem Checkpoint aus beginnt die Suche nach dem Eintrag. Der maximale Suchraum für einen Eintrag liegt immer zwischen zwei Checkpoints. Die gefundenen Daten werden zum Client zurückgesendet.  
Um alle Einträge, die in einem gewissen Zeitraum passiert sind, zu lesen, stellt der Client seine Anfrage an den Master mit einem bestimmten BIC und IBAN und einem Datum von und gegebenenfalls einem Datum bis zu dem alle Einträge zurückgeliefert werden sollen. Der Master sucht auch hier wieder anhand der Schlüssel der Buckets die entsprechenden Adressen heraus und sendet sie gemeinsam mit den nächsten weiteren Buckets an den Client. Dieser fragt wieder mit BIC, IBAN und den Daten das zuständige OSD an. Der Suchvorgang wird in Gang gesetzt, doch statt beim passenden Eintrag zu stoppen, werden alle Einträge, deren Schlüssel nach dem gegeben Datum erstellt wurden, als Block zurückgeliefert. Die Daten werden entweder bis zum Ende des Buckets oder bis zum gegebenen Enddatum zum Client gestreamt.
Hat ein Bucket nicht alle benötigten Daten enthalten, kennt der Client über das Prefetching bereits die anliegenden Buckets und kann weitere Daten anfragen.

Ein DFS nach dem hier beschriebenen Design ermöglicht das Ablegen sehr vieler kleiner Datenschnippsel, sofern sie in irgendeiner Form einen logischen Zusammenhang erfüllen. Außerdem ist das Auslesen der so zusammenhängenden Daten sehr performant und kann häufig mit nur einem Festplattenzugriff realisiert werden. Ein OSD benötigt für einen 16 Megabyte großen Bucket gerade einmal 176 Byte Hauptspeicher. Diese kommen aus den elf Bytes pro Checkpoint und den 16 Einträgen pro Bucket zustande. Wenn die OSDs über 16 Gigabyte Hauptspeicher verfügen, können sie so über 90 Millionen Buckets verwalten, wobei jeder potentiell ein Konto komplett abbildet. Die Adresse der Buckets benötigt auf dem Master sogar nur 65 Byte Hauptspeicher. Wenn auch der Master über 16 Gigabyte Hauptspeicher verfügt, ermöglicht er den schnellen Zugriff auf 246 Millionen Buckets mit bis zu vier Billionen Buchungen. Das entspricht einem Festplattenspeicher von etwa vier Petabyte. Dieses DFS sollte in der Lage sein, alle Buchungen, die bei einer Bank eingehen, effizient und schnell zu speichern und auszuliefern. Inkonsistenzen treten nur während der Dauer eines Leases auf, welches zunächst auf zehn Sekunden limitiert wurde. Ist Konsistenz dringend erforderlich, kann die Ausgabe von Leases gestoppt werden und nach zehn Sekunden tritt ein konsistenter Zustand ein. Generell wäre ein solches DFS für die Abbildung der Kontoarten, die nach Kapitel \ref{konsistenzKonten} eine gelockerte Konsistenz ertragen könnten, bereits völlig ausreichend. Problematisch ist jedoch, dass zum Feststellen des Kontostandes immer alle Buchungen eines Kontos gelesen und ausgewertet werden müssten. Dieses Problem und die noch zu schwache Konsistenz, um alle Kontoarten zu realisieren, lässt sich aber durch ein geschicktes Design der Anwendungsschicht des Buchungssystems lösen.

\section{Design des Buchungssystems}
Die Anwendungsschicht des Buchungssystems stellt eine Schnittstelle für Clients und Anwendungen, die das Buchungssystem nutzen, zur Verfügung. Sie ist die einzige Mög\-lich\-keit, um Daten in das DFS zu schreiben und Daten auszulesen. Daher ist die Anwendungsschicht des Buchungssystems für die Validierung von eingehenden Buchungen sowie für die Bereitstellung von erweiterten Datenmanipulationen zuständig. Für Termineinlagen und Ratenkredite würde es ausreichen, wenn Anfragen an das Buchungssystem direkt vom DFS beantwortet werden. Die nur wenigen Kontobewegungen bei diesen Kontoarten erzeugen eine sehr geringe Menge an Daten und Konsistenz kann bei Auszahlung oder Ende des Kredites leicht sichergestellt werden. Konten, bei denen beliebig viele und zu beliebigen Zeitpunkten Buchungen eingehen, haben aber mit der Inkonsistenz des DFS noch ein Problem. Generell ist es dabei egal, ob zum Zeitpunkt, zu dem eine Buchung eingeht, bereits alle anderen Buchungen korrekt ausgelesen werden können. Es ist nur relevant, dass der Kontostand bereits alle vorangegangenen Buchungen berücksichtigt. Die einzige Information, die für Girokonto und Co. also immer konsistent sein muss, ist der tatsächliche Kontostand. Dieser lässt sich mit einem einzigen Eintrag pro Konto effizient erfassen. Bei jeder Buchung, die dem Konto Zahlungsmittel zu- oder abführt, wird dieser Eintrag entsprechend angepasst. Um die benötigte Konsistenz zu gewährleisten, eignet sich für die Verwaltung der Kontostände eine ACID-basierte relationale Datenbank. Diese Datenbank besitzt nur zwei Tabellen: einmal eine Tabelle, die für jedes Konto einen Eintrag mit dessen Kontostand führt, und eine weitere, die zu nächst alle eingehenden Buchungen sicher verwahrt. Diese Datenmenge ist für ein relationales Datenbanksystem leicht zu bewältigen. Ein Eintrag in der Kontostandstabelle besteht aus BIC, IBAN, dem aktuellen Kontostand und dem Datum der letzten geschriebenen Buchung. Ein Eintrag in der Buchungtabelle enthält alle Attribute einer Buchung so, wie sie in der Beschreibung des DFS erwähnt wurden. Bei jeder neuen Buchung startet nun eine Transaktion, die zunächst den Betrag und das Datum in der Kontostandstabelle anpasst, dann aber die Buchung in der Buchungstabelle ablegt. Schlägt der erste oder der zweite Schritt fehl, stellt das ACID-Prinzip sicher, dass weder der Kontostand angepasst noch die Buchung abgespeichert wurde. In regelmäßigen Abständen werden nun die Einträge der Buchungstabelle ausgelesen und in die dazugehörenden Buckets im DFS verschoben. Wenn ein Lesezugriff auf die Buckets bestätigt, dass alle Buchungen korrekt übernommen wurden, wird die Buchungstabelle geleert. Das Einführen einer Kontostandtabelle ermöglicht es, festzustellen, ob ein Konto über die benötigten Zahlungsmittel für eine Buchung verfügt. Außerdem kann über das Datum ermittelt werden, ob bereits alle Buchungen korrekt geschrieben wurden oder noch Buchungen aus der Buchungstabelle offen stehen. Um den Kontostand auszulesen, müssen auch nicht mehr alle Buchungen verrechnet werden, sondern es genügt ein Zugriff auf die Kontostandstabelle.

Kombiniert man diese Idee eines Buchungssystem mit dem in Kapitel \ref{oneOnlyDFS} entwickelten DFS, so läuft das Schreiben einer Buchung nun wie folgt ab. Die Buchung wird über eine Schnittstelle an das Buchungssystem geliefert. Dieses führt über die Kon\-to\-stands\-ta\-be\-lle einige Validierungen aus. Es wird geprüft, ob das Konto existiert und ob es über die nötigen Zahlungsmittel verfügt, um die Buchung zu erfüllen. Ist dem so, wird in einer Transaktion der Eintrag in der Kontostandstabelle entsprechend angepasst und die Buchung zunächst in die Buchungstabelle eingetragen. Bereits hier erhält der Client die Nachricht, dass die Buchung erfolgreich gespeichert wurde. Das spätere Verschieben der Buchung von der Buchungstabelle in den zugehörigen Bucket im DFS ist für die Client-Anfrage nicht mehr relevant.

Beim Lesen von Buchungen für ein Konto wird zunächst geprüft, ob auch der Kontostand übertragen werden soll. Wenn dem so ist, wird er aus der Kontostandstabelle ausgelesen. Daraufhin wird die Anfrage an das DFS weitergeleitet. Dieses liest die Buchungen aus den entsprechenden Buckets aus und gibt sie an das Buchungssystem zurück. Die Buchungen kombiniert mit dem Kontostand ergeben die Antwort für den Client. Das Datum der letzten durchgeführten Buchung in der Kontostandstabelle, ermöglicht es auch, den Kunden Auskunft zu geben, ob die Antwort vollständig war, oder erst noch Buchungen in das DFS überführt werden müssen.

\section{Skalierbarkeit des entworfenen Systems}
Der Sinn dieses Konzepts für ein DFS und des darauf aufsetzenden Buchungssystem, war die Erfüllung einer besonders einfachen und beliebig großen Skalierbarkeit. Besonders Lesezugriffe sollen immer schnell beantwortet werden können. Da der Fokus vor allem auf diesen Punkten lag, hat das Konzept nicht den Anspruch, in allen Aspekten vollständig zu sein. 

Das hier entworfene Buchungssystem und das dazugehörige DFS lässt sich auf mehrere Arten skalieren. Die Anwendungsschicht ist zunächst einmal komplett unabhängig vom DFS und der relationalen Datenbank. Demnach lassen sich beliebig viele Instanzen davon parallel hochfahren. Je nach Anzahl der Anfragen, kann dies sogar lastbezogen geschehen. Wenn sehr viele Anfragen auf das Buchungssystem einprasseln, lassen sich mehrere Anwendungsschichten hochfahren, wenn die Anzahl der Anfragen wieder sinkt, können die zusätzlichen Systeme wieder entfernt werden. Die Skalierung der relationalen Datenbank kann entweder über Replikation erfolgen, also mehrere gleiche Systeme, oder über eine klare Aufteilung der Zuständigkeit auf mehrere Datenbanksysteme, sodass ein Datenbanksystem immer nur für einen bestimmten BIC-Bereich zuständig ist und nur Anfragen beantwortet, die diesen Bereich betreffen. Das DFS lässt sich, wie bereits beschrieben, einfach durch das Hinzufügen weiterer OSDs oder das Aufstocken des Hauptspeichers des Masters skalieren. Dabei ist zu erwähnen, dass die Skalierung des DFS sowie die Skalierung der Anwendungsschicht der Buchungssysteme komplett unabhängig voneinander stattfinden kann. Die Anwendungsschicht braucht keine Anpassungen vorzunehmen, um die gesteigerte Kapazität des DFS zu nutzen. Demnach wäre es auch denkbar, das DFS auf Systeme von Drittanbietern auszulagern. Sollten die Möglichkeiten der Skalierung des DFS an ihre Grenzen stoßen, kann ähnlich wie bei der relationalen Datenbank die Zuständigkeit für die Buchungen anhand des BICs auf mehrere DFS aufgeteilt werden.

\cleardoublepage
\chapter{Implementierung}
\label{implementation}
Ein DFS, so wie im Kapitel \ref{oneOnlyDFS} beschrieben, existiert aktuell nicht. Da die Entwicklung eines DFS aber äußerst komplex und zeitaufwendig ist, wird für die beispielhafte Implementierung auf ein bereits bestehendes DFS, das am besten auf die erarbeiteten Anforderungen passt, zurückgegriffen. Hauptkriterium ist dabei das Speichern und Lesen vieler kleiner Dateien. Von den beiden betrachteten DFS bietet sich dafür Haystack an. Leider gibt es jedoch von Haystack keine öffentlich zugängliche Version. Das Open-Source-Projekt SeaweedFS wurde aber auf Basis des Haystack-Artikels entwickelt und weist dementsprechend eine große Ähnlichkeit in Architektur und Leistung zu seinem Vorbild auf. Bevor in diesem Kapitel näher auf die Anwendung und Details von SeaweedFS eingegangen wird, soll die Programmiersprache Go kurz angerissen werden. In ihr wurden sowohl SeaweedFS und die extra für diese Arbeit angefertigte Client-Bibliothek für SeaweedFS als auch alle anderen Systemkomponenten implementiert. Auch der Aufbau des entwickelten Systems und der Ablauf beim Lesen und Schreiben einer Buchung wird in diesem Kapitel dargelegt. Bei der hier besprochenen Implementierung handelt es sich mehr um einen Proof of Concept als um eine allumfassende Lösung zur Umstrukturierung von Buchungssystemen. Der Schwerpunkt lag auf Skalierung und Ausfallsicherheit. Der Sicherheit der Daten und der Entwicklung einer Nutzerauthentifizierung wurden im Rahmen dieser Implementierung keine Aufmerksamkeit geschenkt.

\section{Ein Überblick zu Golang}
Go ist eine von Robert Griesemer, Rob Pike und Ken Thompson bei Google entwickelte Programmiersprache. 2009 wurde der Quellcode Open Source. Als Grund, wieso nach der Meinung der Autoren von Go eine neue Sprache gebraucht wurde, geben sie die Frustration über bestehende Programmiersprachen zur Systementwicklung an \cite{gofaq}. Demnach musste sich immer zwischen Leichtigkeit der Programmierung, schneller Kompilierung oder schneller Ausführung entschieden werden. Go soll all diese Kritikpunkte erfüllen und die  leichtgewichtige Programmierung, die von interpretierten dynamisch typisierten Programmiersprachen bekannt ist, mit den Vorteilen einer statischen Typisierung kombinieren. Zusätzlich soll Go in der Lage sein, auch große Programme schnell zu kompilieren und auszuführen. Dazu bringt Go schon von Haus aus viele Werkzeuge mit, die die Entwicklung erleichtern sollen. Neben dem Kompilieren der Programme für unterschiedliche Betriebssysteme können Unit-Tests sowie Benchmark-Tests bereits mit der Standard-Runtime von Go entwickelt werden. Zusätzlich liefert Go eine Art Linter, eine vordefinierte Quellcodeformatierung, sowie eine automatisch generierte Projektdokumentation \cite{gocommands}.

Bei Go handelt es sich um eine prozedurale Programmiersprache, die sich in ihrer Syntax stark an C orientiert. Der Code für \textit{Hello World} in Go ist in Abbildung \ref{lst:helloworld} zu sehen.
\begin{lstlisting}[label=lst:helloworld,
           language=Java,
           firstnumber=1,
           caption=Hello World mit Go.]           
package main

import "fmt"

func main() {
  fmt.Println("Hello World")
}

\end{lstlisting}

Besonders aber eignet sich Go zur Entwicklung von Webservern \cite[3-22]{gowebprogramming}. Der Hauptgrund dafür dürfte die Unterstützung der parallelen Programmierung sein. Parallelität wird in Go durch sogenannte Goroutines bewerkstelligt. Diese werden im Gegensatz zu Threads nicht vom Betriebssystem verwaltet, sondern von der Go-Runtime. Das ermöglicht ein schnelles Erstellen und Zerstören von Goroutines. Außerdem wird für das Anlegen einer Goroutine weniger Hauptspeicher als für einen Thread benötigt \cite{goroutines}. Um eine Goroutine zu starten, muss lediglich das Schlüsselwort \textit{go} vor den Funktionsaufruf geschrieben werden. Der in Go standardmäßig  enthaltene Webserver ist dadurch sehr leichtgewichtig und simpel anzuwenden. Ein Beispiel für einen einfachen Webserver und eine Goroutine ist in Listing \ref{lst:webserver} zu sehen. Nach einer im Jahr 2016 durchgeführten Umfrage nutzen 63 \% der Go-Programmierer die Sprache zur Webentwicklung \cite{gosurvey}.

Für die Implementierung eines Buchungssystems fiel die Wahl auf Go – zum einen, weil das verwendete DFS, SeaweedFS, in Go realisiert wurde, und zum anderen, weil die einfache parallele Programmierung mit bereits geringem Aufwand eine Leistungssteigerung der Systeme ermöglicht.

\begin{minipage}{\linewidth}
\begin{lstlisting}[label=lst:webserver,
           language=Java,
           firstnumber=1,
           caption=Code für einen einfachen Webserver in Go und Beispiel einer Goroutine.]           
package main

import (
  "fmt"
  "net/http"
)

// StartAsGoroutine is a simple example for a function that can be 
// called within a Goroutine
func StartAsGoroutine() {
  fmt.Println("This will be called asynchronous!")
}

func main() {
  http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
    go StartAsGoroutine()
    fmt.Fprintf(w, "Hi there it's me!")
  })

  http.ListenAndServe(":8080", nil)
}
\end{lstlisting}
\end{minipage}

\section{SeaweedFS zur Realisierung des Konzepts}
SeaweedFS ist ein Open-Source-DFS, das unter der Apache-Lizenz veröffentlicht und auf Basis des Haystack-Artikels erstellt wurde \cite{seaweedfsRepo}. Die internen Schritte zum Schreiben und Lesen von Dateien laufen wie beim Vorbild ab. Über eine HTTP-Schnittstelle lassen sich beim Directory Schlüssel und Adresse einer Store Machine zum Speichern von Dateien anfragen und dann direkt auf der Store Machine ablegen. Diese beiden Schritte werden in Listing \ref{lst:seaweedfs} mit Hilfe des Kommandozeilen-Programms \textit{curl} dargestellt. 

Mit den Schnittstellen, die SeaweedFS auf Grundlage von Haystack zur Verfügung stellt, lassen sich aber nicht alle Teile des in Kapitel \ref{concept} vorgestellten Konzepts realisieren. So können beispielsweise keine eigenen Schlüssel in der Form BIC, IBAN gefolgt von einem Zeitstempel erzeugt werden. Außerdem gibt es keine Möglichkeit, über ein Datum nach Dateien zu suchen. Das Lesen einer Datei benötigt immer die Kenntnis von deren Schlüssel. Auch die Gruppierung von logisch zusammenhängenden Daten ist so nicht möglich. Der Client kann nicht bestimmen, in welchem Volume und in welcher Reihenfolge innerhalb des Volumes die Daten landen.

\begin{minipage}{\linewidth}
\begin{lstlisting}[label=lst:seaweedfs,
           language={},
           emphstyle=\color{black},
           extendedchars=true,
           literate={ü}{{\"u}}1,
           keywordstyle=\color{black},
           stringstyle=\color{black},
           firstnumber=1,
           caption=Schritte zum Ablegen einer Datei in SeaweedFS.]           
# Anfragen eines Schlüssels
curl http://localhost:9333/dir/assign
{"count":1,"fid":"3,01637037d6","url":"127.0.0.1:8080",
 "publicUrl":"localhost:8080"}

# Abspeichern der Datei mit dem gegebenen Schlüssel
curl -F file=@testData.txt http://localhost:8080/3,01637037d6
{"size": 43234}
\end{lstlisting}
\end{minipage}

Die ersten beiden Probleme können aber über eine Besonderheit von SeaweedFS gelöst werden. Neben dem Directory und den Store Machines verfügt SeaweedFS auch über einen sogenannten Filer. Dieser kann Dateien in Pfaden und Ordnern ablegen. Da SeaweedFS ganz nach dem Vorbild von Haystack entwickelt wurde, werden diese Pfade und Ordner hocheffizient im Hauptspeicher des Filers gehalten. Kommt eine Anfrage für eine Datei innerhalb eines Ordners beim Filer an, müssen drei Schritte unternommen werden. Zunächst muss die Ordner-ID für den der Datei übergeordneten Ordner gefunden werden. Gemeinsam mit der ermittelten Ordner-ID und dem Namen der Datei kann dann der Dateischlüssel herausgesucht werden. Zuletzt wird über den Dateischlüssel der entsprechende Datenblock aus einem Volume geladen. Der letzte Schritt fällt bereits in den Aufgabenbereich des Directorys und der Store Machines. Der Filer ist demnach nur für die Zuordnung von Ordner zu Ordner-ID und Dateiname zu Dateischlüssel zuständig. Während die Ordner und Pfade im Hauptspeicher des Filers liegen, wird die Zuordnung von übergeordnetem Ordner und Dateiname zu Dateischlüssel in einem Key-Value-Store gespeichert. In der Standardkonfiguration von SeaweedFS ist dieser eine LevelDB. Wenn der Dateischlüssel ermittelt wurde, leitet der Filer die Anfrage an das entsprechende Volume weiter. Dieses antwortet dann dem Client mit den Daten.
Über den Filer kann jetzt ein eigener Schlüssel in Form von Ordner und Dateiname vergeben werden. Der erste Teil des Schlüssels, also BIC und IBAN, wird dabei in den Pfad kodiert, der stets aufsteigende  zweite Teil des Schlüssels, der Zeitstempel, in den Dateinamen. Der Schlüssel für eine Buchung hat demnach die Form \textit{{BIC}/{IBAN}/{Timestamp}}. Über den Filer kann auch nach Dateien innerhalb eines Ordners gesucht werden. Gibt man bei einer Anfrage einen \textit{lastFileName} über einen Query-Parameter mit, werden alle Dateien vom Filer zurückgeliefert, die in alphabetischer Reihenfolge nach dem \textit{lastFileName} liegen. Ein Beispiel zum Anlegen, Auslesen und Filtern von Dateien ist in Listing \ref{lst:filer} zu sehen.

In der Standardkonfiguration stellt der Filer jedoch einen Single Point of Failure dar. Durch den Einsatz von Redis oder Apache Cassandra anstelle der LevelDB als Persistenzschicht für den Filer kann ein verteilter Filer realisiert werden. Alternativ könnten auch mehrere Filer gestartet werden, die jeweils einen anderen Namensraum verwalten. Im Rahmen dieser Arbeit wird jedoch nur mit einem einzigen Filer gearbeitet.

\begin{minipage}{\linewidth}
\begin{lstlisting}[label=lst:filer,
           language={},
           emphstyle=\color{black},
           extendedchars=true,
           literate={ü}{{\"u}}1,
           keywordstyle=\color{black},
           stringstyle=\color{black},
           firstnumber=1,
           caption=Schritte zum Ablegen einer Datei über den Filer in SeaweedFS.]           
# Ablegen einer Datei über den Filer
curl -F "file=@Buchung" "http://localhost:8888/book/BIC/IBAN/Timestamp"

# Anfragen einer Datei über den Filer
curl "http://localhost:8888/book/BIC/IBAN/Timestamp"
{"Daten":1,"MehrDaten":2}

# Filtern aller Dateien in einem Ordner über den Dateinamen
curl "http://localhost:8888/book/BIC/IBAN/?lastFileName=Timestamp"
{
  "Path": "/BIC/IBAN/",
  "Files": [
    {
      "name": "Timestamp1",
      "fid": "6,0625473512"
    },
    {
      "name": "Timestamp2",
      "fid": "6,106f19db84"
    }
  ],
  "Directories": null,
  "Limit": 100,
  "LastFileName": "Timestamp2",
  "ShouldDisplayLoadMore": false
}
\end{lstlisting}
\end{minipage}

Für die Kommunikation mit SeaweedFS wurde extra eine eigene Client-Bibliothek entwickelt. Diese trägt den Namen \textit{Weedharvester} und ist unter der MIT-Lizenz auf GitHub öffentlich zugänglich. Weedharvester abstrahiert den Zugriff auf SeaweedFS und kann sowohl direkt an das Directory sowie über den Filer Anfragen senden. Alle wichtigen Operationen wie das Hochladen, Anfragen, Filtern und Löschen von Dateien werden unterstützt. Mit Weedharvester kann jede beliebige Bytefolge in SeaweedFS abgelegt werden. Ein Beispiel für die Nutzung des Standard-Clients und des Filer-Clients ist in Listing \ref{lst:weedharvester} dargestellt.

\begin{minipage}{\linewidth}
\begin{lstlisting}[label=lst:weedharvester,
           language={},
           emphstyle=\color{black},
           extendedchars=true,
           literate={ü}{{\"u}}1,
           keywordstyle=\color{black},
           stringstyle=\color{black},
           firstnumber=1,
           caption=Beispiele zur Verwendung von Weedharvester.]          
# Client
client := NewClient("http://directoryurl.com")

fid, err := client.Create(reader)
reader := client.Read(fid)

# Filer
filer := NewFiler("http://filerurl.com")

err := filer.Create(reader, filename, path)
reader := filer.Read(filename, path)
\end{lstlisting}
\end{minipage}

Datenreplikation kann bei SeaweedFS entweder beim Anlegen jeder Datei separat mitgegeben oder beim Starten des Directorys allgemeingültig festgelegt werden. Über ein Schema mit drei Ziffern kann bestimmt werden, wie viele Replikas auf unterschiedlichen Datenzentren, unterschiedlichen Racks und gleichen Datenzentren oder dem gleichen Rack und unterschiedlichen Servern angelegt werden sollen. Die Kennung für Datenzentrum und Rack wird den Store Machines beim Start mitgegeben.

Nicht nur die Fähigkeit, relativ kleine Dateien zu verwalten, macht SeaweedFS zu einer guten Wahl für die beispielhafte Implementierung eines Buchungssystems, sondern auch die einfache Nutzung. Das Starten und Konfigurieren ist in wenigen Schritten getan. Außerdem existiert auch ein Docker Image für SeaweedFS, mit dem die Konfiguration und Verwaltung noch einfacher ist. Ein Beispiel für eine docker-compose-Datei, die ein Directory, einen Filer und eine Store Machine startet und entsprechend konfiguriert, ist in Listing \ref{lst:docker} zu sehen. Auf diese Weise wird SeaweedFS auch in der entstandenen Implementierung eingesetzt.

\begin{lstlisting}[label=lst:docker,
           language={},
           emphstyle=\color{black},
           extendedchars=true,
           literate={ü}{{\"u}}1,
           keywordstyle=\color{black},
           stringstyle=\color{black},
           firstnumber=1,
           caption=docker-compose-Datei zum Starten eines Filers{,} eines Directories (master) und einer Store Machine (volume) in SeaweedFS.]          
version: '2.1'
services:
  seaweed:
    container_name: seaweedf
    image: chrislusf/seaweedfs
    ports:
      - 8888:8888
      - 9333:9333
      - 8080:8080
    command: 'server -master.port=9333 -volume.port=8080 -filer=true -volume.publicUrl http://docker:8080'
\end{lstlisting}

\section{Aufbau der Implementierung}
Das komplett implementierte System kann auf GitHub unter dem Namen \textit{seaweed-banking} gefunden werden und steht unter der MIT-Lizenz. Die Grundbausteine sind ein Backend, mehrere Account-Updater und SeaweedFS als das zugrundeliegende DFS. Das Backend stellt dabei die Schnittstelle zum Client und somit die einzige Möglichkeit, um mit SeaweedFS zu kommunizieren, dar. Zu seinen Aufgaben gehört genauso das Anlegen und Auslesen von Konten wie das persistente Speichern einer jeden eingehenden Buchung. Auch das Auslesen von Buchungen fällt in den Aufgabenbereich des Backends. Der Account-Updater hingegen ist für das Zusammenfassen mehrerer Buchungen in eine Datei verantwortlich. Wie bereits erwähnt, ist SeaweedFS alleine nicht in der Lage, logisch zusammenhängende Daten in eine Datei zusammenzuführen. Der Account-Updater informiert sich in regelmäßigen Abständen über neue Buchungen und führt diese zusammen. SeaweedFS speichert für jedes Konto diese Zusammenfassungen von Buchungen im Ordner \url{accounts}. Alle Daten, die das System über das Backend betreten oder verlassen, sind in JSON formatiert. Ein in JSON formatiertes Konto ist in Listing \ref{lst:accountJson} zu sehen. Jedes Konto ist durch \textit{bic} und \textit{iban} eindeutig identifizierbar und kann mit einem beliebigen Kontostand initiiert werden.

\begin{lstlisting}[label=lst:accountJson,
           language={},
           firstnumber=1,
           caption=Kontoobjekt im JSON-Format.]           
{
  "name": "RandomAccount",
  "bic": "BQLM8NINHQ4",
  "iban": "DE627131848139984060",
  "balance": 2481
}
\end{lstlisting}
Die Anzahl der Buchungen für ein Konto sind nicht limitiert. Eine einzelne JSON-formatiertes Buchung wird in Listing \ref{lst:transactionJson} dargestellt. Eine Buchung ist eindeutig über ihr \textit{bookingDate} in Kombination mit \textit{bic} und \textit{iban} des \textit{recipient}s, also des Empfängers der Buchung. Anhand der Daten einer Buchung kann nicht herausgefunden werden, ob sie durch Online-Banking, eine Lastschrift oder eine Überweisung entstanden ist. Lediglich die Beschreibung im Feld \textit{intendedUse} könnte darüber Auskunft geben. Bei jeder Bewegung von bargeldlosen Zahlungsmitteln werden immer zwei solcher Buchungen erstellt. Diese beiden Buchungen unterscheiden sich darin, dass der \textit{recipient} und der \textit{sender} getauscht und der Betrag in \textit{valueInSmallestUnit} das Komplementär zur anderen Buchung ist. Jede dieser beiden Buchungen wird dann für den entsprechenden Empfänger abgelegt. Dieses Vorgehen entspricht der zuvor erklärten doppelten Buchführung. Da die beiden Konten, die an einer Zahlungsmittelbewegung beteiligt sind, nicht immer bei der gleichen Bank liegen, kümmert sich ein sogenannter Zahlungsdienstleister um die Verteilung der Buchungen an die zuständigen Banken \cite{wiki:zahlungsdienstleister}. Das Buchungssystem selbst kann dadurch jede Buchung unabhängig voneinander verarbeiten.

\begin{lstlisting}[label=lst:transactionJson,
           language={},
           firstnumber=1,
           caption=Buchung im JSON-Format.]           
{
  "recipient": {
    "name": "RandomAccount",
    "bic": "BQLM8NINHQ4",
    "iban": "DE627131848139984060"
  },
  "sender": {
    "name": "TestUser",
    "bic": "TESTBIC",
    "iban": "TESTIBAN"
  },
  "bookingDate": "2017-03-20T15:30:45.027280618Z",
  "currency": "EUR",
  "valueInSmallestUnit": 1913,
  "intendedUse": "TestCreateTransaction"
}
\end{lstlisting}

Die grundlegende Architektur der Anwendung ist in Abbildung \ref{architectureImpl} dargestellt. Die PostgreSQL-Datenbank beinhaltet wie im Konzept besprochen jeweils eine Tabelle für alle Kontostände und eine Tabelle für alle kürzlich eingegangenen Buchungen. Das Backend verfügt hierbei über alle Schnittstellen, um Konten oder Buchungen anzulegen und auszulesen.  Das Senden einer POST-Anfrage mit einem JSON-formatierten Konto im Body-Teil an die Adresse \url{/accounts} erzeugt sowohl einen Eintrag in der Kontostandstabelle der PostgreSQL-Datenbank, mit den Attributen \textit{bic}, \textit{iban} und \textit{balance} als auch  eine sogenannte \textit{AccountInfo} im DFS unter dem Verzeichnis \url{accounts/{BIC}/{IBAN}} mit dem Zeitpunkt der Erstellung als Dateinamen. Eine \textit{AccountInfo} ist vergleichbar mit dem im Konzept beschriebenen Bucket, nur dass kein Index File zum schnellen Zugriff innerhalb der \textit{AccountInfo} angelegt werden kann. Der Pfad in Kombination mit dem Dateinamen entspricht hierbei wieder dem im Konzept vorgestellten Schlüssel. Sowohl das Speichern des Eintrages in der Kontodatenbank, als auch das Erstellen der \textit{AccountInfo} im DFS muss gelingen, damit das Anlegen eines Kontos als erfolgreich gilt. Eine \textit{AccountInfo}, wie sie im DFS abgelegt wird, ist in Listing \ref{lst:accountinfoJson} dargestellt. Neben den Informationen für ein Konto, speichert die \textit{AccountInfo} noch einen Satz Buchungen für dieses Konto. Über die Felder \textit{oldestTransaction} und \textit{latestTransaction} ist auch das Datum der ältesten und der neusten Buchung innerhalb der \textit{AccountInfo} einfach zugänglich. Das Feld \textit{predecessor} gibt den Namen der vorangehenden \textit{AccountInfo} an.

\begin{lstlisting}[label=lst:accountinfoJson,
           language={},
           firstnumber=1,
           caption=AccountInfo im JSON-Format.]           
{
  "name": "RandomAccount",
  "bic": "2B9PX8YTFLX",
  "iban": "DE440007387504066832",
  "balance": 8542,
  "predecessor": "2017-03-20T15:30:42.027280124Z",
  "oldestTransaction": "2017-03-20T15:30:45.027280618Z",
  "latestTransaction": "2017-03-20T15:30:45.027280618Z",
  "transactions": [
    {
      "recipient": {
        "name": "RandomAccount8",
        "bic": "2B9PX8YTFLX",
        "iban": "DE440007387504066832"
      },
      "sender": {
        "name": "TestUser",
        "bic": "TESTBIC",
        "iban": "TESTIBAN"
      },
      "bookingDate": "2017-03-20T15:30:45.027280618Z",
      "currency": "EUR",
      "valueInSmallestUnit": 1913,
      "intendedUse": "TestCreateTransaction"
    }
}
\end{lstlisting}

Die \textit{AccountInfo} dient der Bündelung von mehreren Buchungen eines Kontos in einer größeren Datei. Wie viele Buchungen in einer \textit{AccountInfo} abgelegt werden, kann konfiguriert werden. Wurde ein Konto angelegt, kann es direkt über eine GET-Anfrage an die Adresse \url{accounts/{BIC}/{IBAN}} beim Backend ausgelesen werden. Eine GET-Anfrage an die Adresse \url{/accounts} liefert hingegen alle bestehenden Konten zurück. Buchungen können nur für bereits bestehende Konten empfangen werden. Würde der Kontostand des Empfängers durch eine Buchung unter null fallen, wird diese abgelehnt. Zum Speichern einer Buchung für ein Konto muss eine POST-Anfrage mit einer JSON-formatierten Buchung im Body-Teil an die Adresse \url{/accounts/{BIC}/{IBAN}/transactions} gestellt werden. Jede eingehende Buchung wirkt sich zum einen auf den entsprechenden Eintrag des Empfängers in der Kontostandstabelle aus, zum anderen wird sie in der Buchungstabelle abgelegt. Diese beiden Schritte werden als Transaktion ausgeführt. Gelingt diese, wird dem Client das erfolgreiche Speichern der Buchung mitgeteilt.


\begin{figure}
  \centering
  \includegraphics[width=15cm]{img/6/architecture.png}
  \caption[Grundlegende Architektur des implementierten Systems]{ Grundlegende Architektur des implementierten Systems.}
  \label{architectureImpl}
\end{figure}

Die weiteren Schritte zum Verarbeiten der Buchung übernehmen die Account-Updater. Diese sind in einen Master und mehrere Slaves aufgeteilt. Welcher Account-Updater Master und welcher Slave ist, wird beim Start festgelegt. Während der Master keinerlei Kenntnis über mögliche Slaves beim Hochfahren besitzt, muss den Slaves die  Adresse des Master-Account-Updaters mitgeteilt werden. Die Slaves melden sich dann selbstständig beim Master und teilen ihre Adresse mit. Der Master ist als einziger berechtigt, den Slaves den Auftrag zu erteilen, die neu eingegangenen Buchungen zu verarbeiten. So  erhält jeder Slave in regelmäßigen Abständen seinen eigenen Satz an Konten, deren Buchungen noch in eine \textit{AccountInfo} zusammengefasst werden müssen. Die Account-Updater beginnen dann mit den gegebenen BIC und IBAN die ausstehenden Buchungen aus der Buchungstabelle der PostgreSQL-Datenbank auszulesen. Das Zusammenfassen der einzelnen Buchungen in eine \textit{AccountInfo} erfolgt in vier Schritten. Zuerst wird für die gegebenen BIC und IBAN die neueste \textit{AccountInfo} ausgelesen. Diese wird dann mit den Buchungen aus der Buchungstabelle befüllt. Dabei ist zu beachten, dass es eine maximale Anzahl an Buchungen innerhalb einer \textit{AccountInfo} gibt. Bei der Wahl der maximalen Anzahl an Buchungen pro \textit{AccountInfo} sind zwei Faktoren zu berücksichtigen. Zum einen sollte sie nicht zu groß sein, um effizient gelesen werden zu können. Zum anderen führt eine zu niedrige Obergrenze dazu, dass für die Anfrage mehrerer Buchungen viele \textit{AccountInfos} gelesen werden müssen. Beim Hinzufügen einer Buchung zu einer \textit{AccountInfo} wird das Feld \textit{latestTransaction} immer auf die neuste Buchung angepasst. Existieren noch Buchungen über die Obergrenze einer \textit{AccountInfo} hinaus, folgt der dritte Schritt. Die volle \textit{AccountInfo} wird im DFS abgelegt und eine neue erstellt. Diese wird mit den übrigen Buchungen befüllt und unter dem Pfad \url{accounts/{BIC}/{IBAN}/} mit dem \textit{bookingDate} der ältesten Buchung als Dateiname in SeaweedFS abgelegt. Der letzte Schritt dient der Validierung des erfolgreichen Update-Vorgangs. Dazu wird die kürzlich abgespeicherte \textit{AccountInfo} ausgelesen und geprüft, ob die Buchungen der Buchungstabelle auch wirklich hinzugefügt wurden. Erst dann werden die Buchungen aus der Buchungstabelle gelöscht. Bei diesem Block an Aufgaben werden Goroutines explizit eingesetzt. Neben der Verteilung der Update-Aufträge an mehrere Account-Updater starten diese auch noch für jedes Konto eine separate Goroutine zur Verarbeitung der ausstehenden Buchungen. Auf diese Weise können die Buchungen der Buchungstabelle schnell abgearbeitet werden.

Buchungen immer erst nach einiger Zeit nach SeaweedFS zu überführen, ist für das Speichermanagement von SeaweedFS relevant. Da SeaweedFS wie Haystack nicht in der Lage ist, Daten an eine bestehende Datei anzuhängen, wird bei jedem Einfügen einer Buchung in eine bestehende \textit{AccountInfo} eine neue Needle im Volume angelegt und die ursprüngliche als ungültig erklärt. Dieser Speicher ist bis zur nächsten Kompression nicht nutzbar. Außerdem muss für jede eingehende Buchung das Volume Index File sowie die dazugehörige Repräsentation im Hauptspeicher angepasst werden. Fasst man Buchungen aber nur in gewissen Abständen zusammen, können manchmal mehrere Buchungen eines Kontos mit nur einem Update-Vorgang berücksichtigt werden. Außerdem muss so eine \textit{AccountInfo} für das Hinzufügen mehrerer Buchungen nur ein einziges Mal ausgelesen werden.

Kann der Master einen Slave während der Verteilung der Update-Vorgänge nicht erreichen, wird dieser bei der Verteilung der Update-Aufträge nicht mehr berücksichtigt. Durch regelmäßige Heartbeat-Nachrichten wird jedoch sichergestellt, dass der Master immer über eine aktuelle Liste funktionsfähiger Slaves verfügt. 

Durch die Anwendung des Filers von SeaweedFS sowie des Account-Updaters, können die Ideen aus dem Konzept modelliert werden.
Mehrere Buchungen eines Kontos wurden in \textit{AccountInfos} gebündelt, die über BIC und IBAN und einem Zeitstempel erreichbar sind. Auch das Suchen der \textit{AccountInfos}, die nach einem bestimmten Zeitpunkt angelegt wurden, ist über den SeaweedFS Filer und die Angabe eines \textit{lastFileName} möglich. Jedoch gibt es keine Datei, welche den schnellen Zugriff innerhalb einer \textit{AccountInfo} ermöglicht. Wann immer eine oder mehrere Buchungen gelesen werden sollen, muss die ganze \textit{AccountInfo} verarbeitet werden. Das Lesen aller Buchungen, die in einem Zeitraum getätigt wurden, ist über die Schnittstelle \url{account/{BIC}/{IBAN}/transactions?from=2017-02-16_13:05:00} des Backends möglich. Es kann auch der Query-Parameter \textit{to} mitgegeben werden, der das Ende des gesuchten Zeitraums angibt. Mit dem gegebenen Startdatum wird der Filer nach allen \textit{AccountInfos}, die danach erstellt wurden, befragt. Es kann vorkommen, dass der Filer, obwohl Buchungen für den angegebenen Zeitraum vorliegen, keine Daten zurückliefert. Das ist immer dann der Fall, wenn der Dateiname der neuesten \textit{AccountInfo} einen Zeitstempel beinhaltet, der älter als das angefragte Datum ist. In diesem Fall muss explizit nach der letzten \textit{AccountInfo} für das Konto gefragt und nach Buchungen im gewünschten Zeitraum durchsucht werden. Liefert der Filer Daten zu mehreren \textit{AccountInfos}, so werden diese alle nacheinander geladen und zu einer großen \textit{AccountInfo} zusammengeführt. Das geschieht so lange, bis alle \textit{AccountInfos} berücksichtigt wurden, oder das angegebene Enddatum erreicht wurde. Als letzter Schritt wird noch die \textit{AccountInfo} angefordert, die der ältesten berücksichtigten vorausging. Über das Feld \textit{predecessor} ist dies leicht möglich. Auch die passenden Buchungen dieser werden noch mit zur großen \textit{AccountInfo} hinzugefügt. Die so kombinierten Daten, werden dann dem Client ausgeliefert. 
Durch die Verzögerung beim Verarbeiten der ausstehenden Buchungen, kann es vorkommen, dass neue Buchungen bei einer Anfrage der \textit{AccountInfo} noch nicht berücksichtigt werden. Über die Kontostandsdatenbank ist jedoch zu jedem Zeitpunkt das Auslesen des korrekten Kontostands möglich.

Zum Abschluss des Kapitels soll noch ein letztes Mal der Schreib- und Lesevorgang einer oder mehrerer Buchungen zusammengefasst werden. Nachdem die Buchung das Backend erreicht hat, wird der Eintrag in der Kontostandstabelle angepasst und die Buchung in der Buchungstabelle abgelegt. In regelmäßigen Abständen wird vom Master-Account-Updater der Auftrag zur Verarbeitung ausstehender Buchungen gegeben. Dabei werden alle Buchungen eines Kontos in einer oder mehreren \textit{AccountInfos} zusammengefasst.

Möchte der Client nun Buchungen für einen bestimmten Zeitraum lesen, stellt er diese Anfrage an die entsprechende Schnittstelle des Backends. Dieses sammelt über den Filer von SeaweedFS alle \textit{AccountInfos}, die möglicherweise Buchungen für den gewünschten Zeitraum halten, und fasst diese in eine große \textit{AccountInfo} zusammen. Diese wird dem Client als Antwort zurückgeliefert.

\cleardoublepage
\chapter{Evaluation}
\label{evaluation}
Das implementierte System soll in der Evaluation genauer untersucht werden. Die Bereiche, die analysiert werden, sind Interoperabilität, Skalierbarkeit, Performanz, Ausfallsicherheit und Wirtschaftlichkeit. Vergleiche zu aktuellen Buchungssystemen können nur in Bezug auf die Persistenzschicht erfolgen. Informationen zum technischen Aufbau sowie Leistung der Anwendungsschicht eines Buchungssystems sind nicht bekannt. 

\section{Interoperabilität des implementierten Systems}
Wie gut sich das entwickelte System in die bestehende Anwendungslandschaft einer Bank integriert, kann nur vermutet werden. Die Entscheidung, die Anwendung in Go zu programmieren, könnte das Einflechten in ein anderes System jedoch erleichtern. Mit Go ist eine einfache Kompilierung für unterschiedliche Plattformen möglich \cite{goCompile}. Der Compiler muss lediglich über ein Flag entsprechend konfiguriert werden, um Binarys für Windows oder Linux zu erzeugen. Auch das eingesetzte DFS SeaweedFS ist für viele unterschiedliche Plattformen erhältlich. Die Anforderungen an die Schnittstellen eines Buchungssystems sind nicht genau bekannt. Nach einer Anpassung der entwickelten Anwendungsschicht sollte aber eine leichte Integration möglich sein. Das Speichern von Konten und Buchungen in einer relationalen Datenbank ist bei Banken bereits üblich. Anstelle aller Buchungen nur noch die abzulegen, die noch nicht verarbeitet wurden, sollte daher nicht schwer fallen. Die geringe Erfahrung im Umgang mit einem DFS im Bankenwesen stellt hingegen eine größere Hürde dar. Die Technologie ist verglichen mit den relationalen Datenbanken, die in den 1980er Jahren entstanden, sehr jung \cite{wiki:datenbankGeschichte}. Ohne langfristige Leistungserfassungen und ausgeklügelte Sicherheitskonzepte wird der Einsatz eines DFS als Persistenzschicht des Buchungssystem für Banken nicht in Frage kommen.

\section{Skalierbarkeit des implementierten Systems}
Die Skalierbarkeit eines DFS wurde im Rahmen dieser Arbeit ausführlich beschrieben. DFS generell und auch SeaweedFS lassen sich leicht horizontal durch Hinzufügen neuer OSDs skalieren. Eine hohe Menge an OSDs wird aber auch mehr Hauptspeicher des Masters erfordern. Daher muss für den Master eine vertikale Skalierung durchgeführt werden. Eine Store Machine in SeaweedFS benötigt, wie bei Haystack, sehr wenig Hauptspeicher, um eine Datei abzulegen. Gerade einmal 24 Byte werden von jeder neuen Datei beansprucht. Verfügt eine Store Machine über 16 Gigabyte Hauptspeicher, kann sie demnach fast 670 Millionen Dateien ablegen.
Wenn jede dieser Dateien 50 Buchungen bündelt, können so mehr als 33 Milliarden Buchungen auf einer einzigen Store Machine gespeichert und über einen schnellen Zugriff im Hauptspeicher auch wieder ausgelesen werden. Das Directory kennt dabei nur die Adresse der Volumes sowie die Zuordnung von logischen zu physikalischen Volumes. Der dafür benötigte Hauptspeicher dürfte einige Bytes betragen. Demnach ist ein einziges Directory in der Lage mehrere tausend Volumes zu adressieren, wobei jedes Volume mehrere Millionen Buchungen speichern kann.

Auch wenn das Directory sehr gut vertikal skaliert, stellt es einen Single Point of Failure dar. Daher bietet SeaweedFS die Möglichkeit, mehrere Directorys parallel zu starten. Diese ernennen untereinander einen Anführer. Fällt dieser aus, nimmt ein anderes Directory seinen Platz ein. Nur der Filer begrenzt in diesem Szenario die Skalierbarkeit von SeaweedFS. Jede abgelegte Datei wird im Filer vermerkt. Die dazu eingesetzte LevelDB kann aber nicht horizontal skaliert werden. Auch wenn die Leistung im Rahmen der durchgeführten Tests ausreichend war, muss für die Verwaltung mehrerer Petabytes an Daten eine andere Lösung gefunden werden. Ein verteilter Filer mit Redis oder Apache Cassandra zum Speichern der Dateinamen könnte aber Abhilfe schaffen. 

Das entwickelte Backend sowie der Account-Updater sind nahezu beliebig skalierbar. Ohne Probleme können mehrere Backends parallel betrieben werden oder je nach Anzahl der Anfragen gestartet oder heruntergefahren werden. Gleiches gilt für den Account-Updater. Auch von diesem können mehrere Instanzen parallel existieren. Es muss nur sicher gestellt sein, dass ein Account-Updater als Master gekennzeichnet ist. 

Die relationale Datenbank, die für Kontostände und ausstehende Buchungen zuständig ist, lässt sich nur begrenzt und schwer skalieren. Jedoch ist die Datenmenge, die gespeichert werden muss, gering. Für jedes Konto existiert ein einziger Eintrag in der Kontostandstabelle. Noch nicht verarbeitete Buchungen werden in regelmäßigen Abständen in das DFS überführt und der Speicherplatz in der Datenbank wieder frei gemacht. Die Aufgaben, die die relationale Datenbank im implementierten System zu erfüllen hat, erfordern in jedem Falle eine geringere Skalierung als die aktuell in Buchungssystemen eingesetzten Datenbanken. Des Weiteren sind Banken mit dem Umgang und der Skalierung von relationalen Datenbanken vertraut und sollten wenig Probleme mit einem System dieser Größe haben.


\section{Performanz des implementierten Systems}
Für die Messung der Performanz wird die Implementierung auf zwei Mac Minis installiert. Beide verfügen über einen Dual-Core-Prozessor Intel Core i5 mit 2,5 Gigahertz sowie 16 Gigabyte Hauptspeicher.
Der Leistungstest umfasst drei Teile, die jeweils für zehn Sekunden mit zehn parallelen Goroutines ausgeführt werden. Im ersten Teil werden Konten, im zweiten Transaktionen angelegt. Im letzten Teil des Tests, werden für die einzelnen Konten alle zuvor angelegten Buchungen ausgelesen. Um die Ergebnisse mit einem System zu vergleichen, das nur auf einer relationalen Datenbank beruht, wird ein Backend herangezogen, das nur auf einer PostgreSQL-Datenbank aufsetzt. Auch hier werden die gleichen Tests durchgeführt. im folgenden wird immer vom DFS-System gesprochen, wenn von dem System mit SeaweedFS die rede ist und vom RDBMS-System, wenn das System gemeint ist, das nur auf der PostgreSQL-Datenbank aufsetzt.

Bei der Testreihe für das implementierte System ist ein Rechner für Backend, Account-Updater sowie die PostgreSQL-Datenbank zuständig.
Der andere Rechner kümmert sich alleine um die Verwaltung von SeaweedFS. Die abgelegten Daten werden nicht repliziert. Der Account-Updater wurde so konfiguriert, dass alle zehn Sekunden jeweils 50 Buchungen zu einer \textit{AccountInfo} gebündelt werden. Um aufzuzeigen, wie sich die Leistung verhält, wenn eine unterschiedliche Anzahl an \textit{AccountInfos} für eine Anfrage gelesen werden muss, werden zunächst 50 Buchungen und dann in 50er Schritten bis hin zu 200 Buchungen angefragt. Hierfür müssen dann vier \textit{AccountInfos} gelesen werden. 
Zum Testen des Systems, welches komplett auf der PostgreSQL-Datenbank basiert, ist ein Rechner für das Backend und ein Rechner für die Datenbank zuständig.

Die Ergebnisse für das Erstellen eines Kontos beziehungsweise einer Buchung in den unterschiedlichen Systemen sind in Tabelle \ref{tab:create} zu sehen. Demnach benötigt das DFS-System etwas länger, um ein Konto anzulegen. Diese Verzögerung ist der Tatsache geschuldet, dass neben dem Eintrag in der Kontostandstabelle auch noch eine \textit{AccountInfo} in SeaweedFS angelegt werden muss. Bei dem RDBMS-System, muss hingegen lediglich ein Eintrag in der Kontentabelle erstellt werden. Das Speichern einer Buchung beansprucht aber auf beiden Systemen fast die gleiche Zeit. Beide Systeme empfangen die Buchung, passen den Kontostand entsprechend an und legen die Buchung in einer Buchungstabelle ab. Das spätere Verschieben der Buchung nach SeaweedFS spielt für die Zeitmessung hier keine Rolle. Eine Erhöhung der Replikation der Daten in SeaweedFS hat nur eine geringfügige Steigerung der Dauer zum Erstellen eines Kontos zur Folge.

\begin{table}[h]
 %title of the table
\centering % centering table
\begin{tabular}{c rr} 
 %inserting double-line
\multicolumn{3}{c}{RDBMS-System}\\
Test & Arithmetisches Mittel & Standardabweichung  \\ [0.5ex]
\hline % inserts single-line
Erstellen eines Kontos & 5,02 ms& 2,13 ms\\ % Entering row contents
Erstellen einer Buchung & 4,35 ms& 1,53 ms\\
\hline\hline % inserts single-line

\multicolumn{3}{c}{DFS-System}\\
Test & Arithmetisches Mittel & Standardabweichung  \\ [0.5ex]
\hline % inserts single-line
Erstellen eines Kontos & 6,62 ms& 0,48 ms\\ % Entering row contents
Erstellen einer Buchung & 4,6 ms& 1,44 ms\\

\end{tabular}
\label{tab:create}
\caption{Dauer zum Erstellen eines Kontos bzw. einer Buchung im RDBMS-System und dem DFS-System. Gemessen über vier Testläufe.}
\end{table}

Größere Unterschiede zwischen den beiden Systemen, sind jedoch beim Lesen der erstellten Buchungen zu sehen. Abbildung \ref{evaRead} zeigt, wie sich die Zeit zum Lesen unterschiedlich vieler Buchungen in beiden Systemen entwickelt. Für die Messungen wurden zunächst 50 Buchungen auf einmal gelesen. Diese Menge wurde in 50er Schritten bis auf 200 Buchungen gesteigert. Das getestete System speicherte dabei immer mehr als 2600 Buchungen und zwischen 1200 und 2000 Konten. Das Diagramm zeigt zunächst, dass beide Systeme für mehr Buchungen auch mehr Zeit benötigen. Zunächst antwortet das DFS-System etwa doppelt so schnell wie das RDBMS-System. Während viele Buchungen eines Kontos in SeaweedFS in einer \textit{AccountInfo} gebündelt werden und im Speicher nah zusammen liegen, müssen bei der PostgreSQL-Datenbank die Einträge einzeln ausgelesen werden. Der Geschwindigkeitsunterschied wird jedoch mit steigender Buchungszahl immer geringer. Der Grund dafür dürfte die Anzahl der gebündelten Buchungen in einer \textit{AccountInfo} sein. Für die ersten 50 Buchungen muss nur eine einzige \textit{AccountInfo} gelesen werden, mit jedem Testschritt kommt eine weitere dazu. Wo am Anfang also noch eine Anfrage an den Filer und ein Festplattenzugriff genügt, vervierfacht sich die Anzahl dieser Operationen für 200 Buchungen. Langfristig, kann angenommen werden, dass das RDBMS-System wieder schneller wird als das DFS-System. Die Maximalanzahl an Buchungen für eine \textit{AccountInfo} ist für das Auslesen derartig großer Blöcke nicht optimal gewählt. Abbildung \ref{evaRead2} zeigt, dass eine höhere Maximalanzahl an Buchungen für eine \textit{AccountInfo} die Lesezeit für eine höhere Anzahl an Buchungen verringert. 
Die Kehrseite einer größeren \textit{AccountInfo} ist jedoch, dass beim Auslesen einer Teilmenge an Buchungen trotzdem die gesamte \textit{AccountInfo} in den Speicher geladen werden muss. Das Auslesen von 50 oder 200 Buchungen aus einer \textit{AccountInfo}, die mit 200 Buchungen gefüllt ist, dauert also fast genauso lange. Die Wahl einer passenden Maximalanzahl an Buchungen in einer \textit{AccountInfo} hängt also von dem erwarteten Leseverhalten der Clients ab. Wenn Buchungen immer für jeden Monat abgefragt werden, und die Anzahl der Buchungen gering ist, kann die Anzahl der gebündelten Buchungen auch klein ausfallen. Wenn hingegen große Zeitbereiche auf einmal ausgelesen werden, eignet sich eine höhere Anzahl an Buchungen pro \textit{AccountInfo} besser.


\begin{filecontents}{date7.dat}
date  value
50      1.5
100     2.4
150     3.5
200     4.3
\end{filecontents}

\begin{filecontents}{date8.dat}
date  value
50      3.0
100     3.8
150     4.4
200     4.9
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xtick=data,
ymin=0, ymax=6,
legend entries={DFS-System,RDBMS-System},
legend pos={south east},
scaled ticks=false,
xlabel={Anzahl der zu lesenden Buchungen},
ylabel={Dauer des Lesevorgangs in Millisekunden},
]
\addplot table[x=date,y=value] {date7.dat};
\addplot table[x=date,y=value] {date8.dat};
\end{axis}
\end{tikzpicture}
\caption[Lesegeschwindigkeit bei unterschiedlicher Anzahl Buchungen im RDS- bzw. DFS-System]{Lesegeschwindigkeit bei steigender Anzahl an Buchungen im RDS- bzw. DFS-System.}
\label{evaRead}
\end{center}
\end{figure}


\begin{filecontents}{date9.dat}
date  value
50      1.5
100     2.4
150     3.5
200     4.3
\end{filecontents}

\begin{filecontents}{date10.dat}
date  value
50      1.5
100     2.3
150     3.1
200     3.9
\end{filecontents}



\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xtick=data,
ymin=0, ymax=6,
legend entries={max. 50 Buchungen, max. 200 Buchungen },
legend pos={south east},
scaled ticks=false,
xlabel={Anzahl der zu lesenden Buchungen},
ylabel={Dauer des Lesevorgangs in Millisekunden},
]
\addplot table[x=date,y=value] {date9.dat};
\addplot table[x=date,y=value] {date10.dat};
\end{axis}
\end{tikzpicture}
\caption[Lesegeschwindigkeit im DFS-System bei unterschiedlicher Anzahl Buchungen mit größerer \textit{AccountInfo}]{Lesegeschwindigkeit im DFS-System bei steigender Anzahl an Buchungen mit unterschiedlichen \textit{AccountInfos}.}
\label{evaRead2}
\end{center}
\end{figure}

\section{Ausfallsicherheit des implementierten Systems}
Auch Ausfallsicherheit ist eine Kernkompetenz von DFS und somit auch von SeaweedFS. Die Replikation der abgelegten Dateien kann entweder pro Datei oder allgemeingültig beim Starten des Directorys mitgegeben werden. Replikas können in unterschiedlichen Servern, Racks oder sogar Datenzentren abgelegt werden. Auch wenn ganze Rechenzentren versagen, bleiben die Daten so erhalten. Lediglich das Directory sowie der Filer stellen im aktuellen System einen Single Point of Failure dar. Das parallele Betreiben mehrerer Directorys und der Einsatz eines verteilten Filers kann für dieses Problem jedoch Abhilfe schaffen. Auch wenn SeaweedFS zwischenzeitlich ausfällt, kann das System noch weiterarbeiten. Zum Anlegen von Buchungen wird keine Verbindung zu SeaweedFS benötigt. Die Buchungstabelle der PostgreSQL-Datenbank speichert die Buchungen solange zwischen, bis SeaweedFS wieder erreichbar ist und die Buchungen überführt werden können. Während eines solchen Ausfalls könnten Kunden lediglich ihre letzten Buchungen nicht einsehen. Der Kontostand wäre dennoch immer korrekt. Der die Ausfallsicherheit limitierende Faktor ist wie bei traditionellen Buchungssystemen die relationale PostgreSQL-Datenbank zum Speichern von Konten und ausstehenden Buchungen. Die Datenmenge, die in der relationalen Datenbank verwaltet werden muss, ist jedoch klein. Eine Skalierung sollte wenn überhaupt nur in einem geringen Maße nötig sein. Dadurch wird die Verfügbarkeit des Systems nicht durch die Verfügbarkeit anderer Systeme begrenzt. Um eine hohe Ausfallsicherheit des RDBMS zu gewährleisten, können die Banken ihre Erfahrung mit den aktuellen Systemen einsetzen. Die Datenmenge ist dabei jedoch deutlich leichter zu verwalten. 


\section{Wirtschaftlichkeit des implementierten Systems}
Zu guter Letzt soll die Wirtschaftlichkeit des Systems betrachtet werden. Skalierbarkeit, Ausfallsicherheit und Leistung der SeaweedFS-basierten Lösung scheinen die Anforderungen eines Buchungssystem gut zu erfüllen. Wenn jedoch der Preis dafür die Kosten aktueller Systeme übersteigt, ist der Umstieg für Banken weniger attraktiv. Die Wirtschaftlichkeit des hier implementierten sowie aktueller Buchungssysteme kann jedoch nicht genau ermittelt werden. Als Näherungswert, werden die Kosten herangezogen, die benötigt werden, um das System mit den Amazon Web Services zu realisieren. Da die Ausgaben für die Anwendungsebene des Buchungssystems unabhängig von der Persistenzschicht sind, werden hier nur die Kosten für die relationalen Datenbanken beziehungsweise das DFS berücksichtigt. Alle Preise beziehen sich auf Maschinen, die in London stehen sollen. Auch hier ist wieder vom DFS-System die Rede, wenn es um das System mit SeaweedFS geht und vom RDBMS-System, wenn die rein auf einer relationalen Datenbank basierende Lösung gemeint ist.

Für SeaweedFS werden Amazon-Elastic-Compute-Cloud-Instanzen (EC2-Instanzen) genutzt \cite{EC2}. Dabei handelt es sich generell um normale Server, die aber über eine Web-Service-Oberfläche einfach konfiguriert oder skaliert werden können. Eine Store Machine wird mit einer speicheroptimierten d2.2xlarge-EC2-Instanz abgebildet. Acht CPUs sowie 61 Gigabyte Hauptspeicher sollten für die Verwaltung von zwölf Terabyte Festplattenspeicher genügen. Das Directory sowie der Filer werden auf einer m4.4xlarge-EC2-Instanz gehostet. Diese besitzt 16 CPUs und 64 Gigabyte Arbeitsspeicher. Sowohl Directory als auch Filer sind in der Verarbeitung nahezu jeder Anfrage involviert und benötigen daher mehr Leistung als die Store Machines. Für die relationale Datenbank zum Speichern des Kontostandes sowie der ausstehenden Buchungen kommt der Amazon Relational Database Service (RDS) zum Einsatz \cite{RDS}. Hier wird eine db.m4.4xlarge-RDS-Instanz mit PostgreSQL gewählt. Diese verfügt über 16 CPUs sowie 64 Gigabyte Hauptspeicher. Da die relationale Datenbank ein Kernelement zum Erstellen von Konten sowie Buchungen bildet, wird diese Leistung benötigt. Als Speicherkapazität sollte ein Terabyte genügen. Zusätzlich wird die Multi-AZ-Bereitstellungsoption eingeplant. Diese stellt sicher, dass beim Ausfall der Hauptdatenbank eine synchron replizierte sekundäre Datenbank die Anfragen beantwortet. Gerade für Banken ist diese Sicherheit sehr relevant.
Sowohl die RDS- als auch die EC2-Instanzen werden als Reserved Instances mit einer einmaligen Zahlung für drei Jahre im Voraus gebucht. Der Stundenpreis ist so deutlich geringer, als bei einer On-Demand-Buchung. Jedoch müssen die Kosten auch gezahlt werden, wenn die Instanz nicht genutzt wird.

Die Datenbankkonfiguration für das DFS-System kann auch für das rein auf einer relationalen Datenbank basierte System genutzt werden. Da hier jedoch eine höhere Skalierung des RDBMS zu erwarten ist, kann die Leistung einer einzelnen Instanz verringert werden. Deswegen kommen hier db.m4.2xlarge-RDS-Instanzen mit jeweils acht CPUs sowie 32 Gigabyte Arbeitsspeicher zum Einsatz. 


Abbildung \ref{costsAmazon} zeigt, wie sich die jährlichen Kosten der beiden so aufgestellten Systeme bei steigendem nutzbaren Speicher entwickeln. Durch die Replikation der Dateien in SeaweedFS kann nicht die Gesamtkapazität des Systems zum Ablegen neuer Daten genutzt werden. In dem hier angewandten Beispiel wird jede Datei auf drei Store Machines abgebildet. Für sechs Terabyte nutzbarer Speicher braucht man demnach 36 Terabyte an verfügbarem Speicher. Um die Gefahr eines Single Point of Failures durch Directory und Filer zu verringern, wurde hier jeweils eine zweite Instanz mit eingerechnet. In diesem Beispiel verwaltet eine EC2-Instanz jeweils vier Terabyte an nutzbaren Speicher im DFS-System. Im RDS-basierten System ist eine RDS-Instanz für sechs Terabyte nutzbaren Speicher zuständig. Für die 60 Terabyte Speicherkapazität werden also entweder 15 EC2-Instanzen oder zehn RDS-Instanzen benötigt. 

Wie Abbildung \ref{costsAmazon} zeigt, sind die Initialkosten des DFS-Systems minimal höher, als die des RBMS-Systems. Bei einer kleineren Datenmenge als zwölf Terabyte, ist das Betreiben des DFS-Systems teurer. Das kommt dadurch zustande, dass auch für wenige Daten bereits ein komplettes DFS sowie eine PostgreSQL-Datenbank benötigt wird. Da diese aber nur eine geringe Datenmenge verwaltet, muss sie bei der Skalierung des Gesamtsystems nicht mitwachsen. Die Kosten steigen also nur durch das Hinzufügen weiterer Store Machines in Form von EC2-Instanzen. Da diese aber deutlich günstiger als eine RDS-Instanz sind, fällt das DFS-System bei mehr als zwölf Terabyte bereits günstiger aus, als das RDBMS-System. Bei 60 Terabyte beträgt dieser Kostenunterschied bereits über 50 \%. Bezogen auf die Kosten der Banken, die in Kapitel \ref{problems} aufgezeigt wurden, könnte so ein Preis von etwa sechs Euro pro Konto erzielt werden. Die Kosten traditioneller Buchungssysteme liegen bei etwa 13 Euro pro Konto. Je größer die zu speichernde Datenmenge wird, desto größer wird der Preisunterschied zwischen den beiden Systemen.

\begin{filecontents}{date11.dat}
date  value
1     49170.63
2     49170.63
3     63973.53
4     78776.43
5     93556.93
6     108077.91
\end{filecontents}

\begin{filecontents}{date12.dat}
date  value
1     23412.00
2     46725.03
3     93350
4     139880.13
5     185931.73
6     231381.34
\end{filecontents}



\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
xtick=data,
ytick={40000, 80000, 120000, 160000, 200000, 240000},
yticklabels={40000, 80000, 120000, 160000, 200000, 240000},
xticklabels={6TB,12TB,24TB,36TB,48TB,60TB},
legend entries={SeaweedFS mit EC2, PostgreSQL mit RDS },
legend pos={south east},
scaled ticks=false,
xlabel={Nutzbarer Speicher},
ylabel={Jährliche Kosten in Dollar},
]
\addplot table[x=date,y=value] {date11.dat};
\addplot table[x=date,y=value] {date12.dat};
\end{axis}
\end{tikzpicture}
\caption[Jährliche Kosten bei steigender Menge an nutzbaren Speicher]{Jährliche Kosten bei steigender Menge an nutzbaren Speicher.}
\label{costsAmazon}
\end{center}
\end{figure}

\cleardoublepage
\chapter{Ausblick}
Das entwickelte Konzept beschäftigt sich vor allem mit der Skalierbarkeit und Ausfallsicherheit von Buchungssystemen. Doch auch das Thema Sicherheit ist für Banken von großer Bedeutung. Die Verschlüsselung einzelner Buchungen oder eines ganzen Buckets, könnte das System vor unberechtigten Zugriffen schützen. Dazu wird jedoch eine ausgeklügelte Schlüsselverwaltung und eine sichere Verschlüsselung benötigt. Um den Effekt des entworfenen DFS auf ein Buchungssystem messbar zu machen, muss das DFS erst entwickelt werden. Danach lässt sich ermitteln, welche Größen der Buckets und welche Anzahl an Checkpoints in einem Bucket Index File die besten Ergebnisse liefern. Eine Analyse des Nutzerverhaltens könnte hier sinnvoll sein. Auch die Last, die die relationale Datenbank tatsächlich zu stemmen hat, muss erst noch ermittelt werden. 

Denkbar wäre auch, ein DFS als ein Buchungsarchiv zu nutzen. Alle Aufgaben des Buchungssystems, würden weiterhin über die aktuellen Systeme der Banken laufen. Das langfristige Lesen und Verwalten von Buchungen würde jedoch das DFS übernehmen. Banken könnten die Buchungen der Kunden so beliebig lange speichern, ohne dadurch Speicher und Leistung des Buchungssystem zu belagern.

Besonders spannend dürfte jedoch die Frage sein, ob in der Realität ein DFS allein nicht ausreicht, um die Buchungen für einfache Girokonten abzudecken. Nur in seltenen Fällen treten Buchungen so kurzzeitig nacheinander bei Konten normaler Endkunden auf, dass die Konsistenz ein Problem darstellen würde. Wenn der Betrieb eines DFS entsprechend günstig ist, könnten die seltenen Fälle einer ungewollten Überbuchung von der Bank selbst gedeckt werden. Weitere Forschung in diesem Gebiet ist jedoch ohne genaue Informationen zu den Ausgaben für die Buchungssysteme und die Risikobereitschaft der Banken nur schwer möglich.

\afterpage{\blankpage}
\chapter{Zusammenfassung}
Ziel der Arbeit war, herauszufinden, welche Auswirkungen der Einsatz eines DFS als Persistenzschicht auf ein Buchungssystem hat. Dabei lag ein besonderes Augenmerk auf Skalierbarkeit sowie Ausfallsicherheit. Dazu musste zunächst der Aufbau und historische Hintergrund aktueller Buchungssysteme verstanden werden. Von der Eigenentwicklung bis zum Einkauf der Systeme bei Drittanbietern hat das Buchungssystem immer eine tragende Rolle im Gesamtsystem der Banken gespielt. Die wichtigste Aufgabe ist dabei das Speichern der Buchungen. Auch die Verfügbarkeit der Systeme spielt eine große Rolle. Ist das System nicht erreichbar, können auch keine Buchungen durchgeführt werden. Die Probleme der Buchungssysteme mit den aktuellen Anforderungen, rechtfertigen bereits die Überlegung, eine andere Technologie als Persistenzschicht einzusetzen – ganz zu schweigen von der zusätzlichen Belastung durch die Second Payment Service Directive (PSD2). Diese zwingt die Banken Drittanbietern zu ermöglichen, Buchungen auf Wunsch des Kunden auszulesen und durchzuführen. Das dürfte vor allem einen Anstieg der Lesezugriffe zur Folge haben. Hauptproblem ist, dass die aktuellen Systeme nicht beliebig skaliert werden können. Wird es trotzdem versucht, verursacht das hohe Kosten. So fließen etwa 20 \% der Kosten für den Verwaltungsaufwand der Banken pro Jahr in die Banken-IT. 

DFS lassen sich hingegen sehr leicht und günstig skalieren. Um genauer zu verstehen, wie ein DFS funktioniert, wurde zunächst das BASE-Prinzip vor allem mit Bezug auf die Konsistenz durchleuchtet. Demnach muss nicht komplett auf Konsistenz verzichtet werden, um die Systeme verfügbarer zu machen. Daraufhin wurden die beiden DFS GFS und Haystack genauer betrachtet. Diese dienten auch später als Grundlage für das entwickelte Konzept. Die Betrachtung der beiden DFS ermöglichte ein besseres Verständnis dafür, wieso DFS so gut skalierbar sind. Außerdem wurde beleuchtet, wie Inkonsistenzen zustande kommen und wie lange sie anhalten. Mit dem Wissen über die Funktionsweise eines DFS konnte deren Skalierbarkeit, Ausfallsicherheit, Verfügbarkeit, Kosteneffizienz und Leistung mit den Anforderungen an ein Buchungssystem verglichen werden. Für die erforderliche Konsistenz, um immer den korrekten Kontostand anzuzeigen, konnte jedoch noch keine Lösung aufgezeigt werden.

Bei der Erarbeitung des Konzepts wurde zunächst in Frage gestellt, ob überhaupt für alle Kontoarten eine starke Konsistenz benötigt wird. Demnach lassen sich Konten mit sehr wenigen und vorhersehbaren Kontobewegungen auch mit einer eventual consistency abbilden. Besonders gut eignen sich hierfür Termineinlagen sowie die Konten für das Kreditgeschäft. Mit mehr als 7,4 Millionen abgeschlossenen Ratenkrediten im Jahr 2015 in Deutschland, wäre eine Abbildung dieser Kontoarten in einem DFS bereits eine große Entlastung aktueller Buchungssysteme. Das daraufhin entwickelte Konzept für ein Buchungssystem versuchte vor allem, drei Annahmen zu erfüllen: Es müssen sehr viele, sehr kleine Daten geschrieben werden. Die Mehrheit der Anfragen sind Lesezugriffe. Diese Lesezugriffe versuchen meistens, mehrere logisch zusammenhängende Daten auszulesen, zum Beispiel alle Buchungen eines Monats. Durch das Anwenden einiger Techniken des GFS und Haystacks wurde ein DFS entworfen, das diese Annahmen erfüllen kann. Logisch zusammenhängende Daten werden in Buckets gebündelt. Das heißt Buchungen eines Kontos landen in einem Bucket. Um nicht immer den gesamten Bucket auszulesen, um einzelne Buchungen zu finden, ermöglicht ein Bucket Index File einen schnellen Zugriff auf die Daten. Zum Auslesen mehrerer Buchungen kann der Inhalt der Buckets zum Client gestreamt werden. Zusätzlich soll das Namespace Flattening eingesetzt werden, um den Hauptspeicher des Master zu entlasten. Auch das serverseitige Prefetching hält den Arbeitsaufwand des Masters gering. Das vorgestellte DFS wäre alleine schon in der Lage, Termineinlagen sowie die Konten für Ratenkredite abzubilden. Durch das Hinzufügen einer relationalen Datenbank für Kontostand und kürzlich eingegangene Buchungen wurde auch die Speicherung von Girokonten mit einem DFS denkbar. Das entworfene Konzept macht sich die Vorteile der DFS zunutze und kombiniert sie mit den Vorteilen einer relationalen Datenbank. Das Resultat ist ein System, das gut skalierbar ist und die für ein Buchungssystem geforderte Konsistenz liefert.

Das so entworfene System musste noch beispielhaft implementiert werden, um die Leistung messbar zu machen. Die Entwicklung eines DFS war aber im Rahmen dieser Arbeit zu aufwendig, und da es kein DFS gibt, das dem im Konzept beschriebenen entspricht, musste auf das am besten passende ausgewichen werden. Dieses sollte wie Haystack mit vielen kleinen Dateien umgehen können. Da Haystack nicht öffentlich zugänglich ist, wurde SeaweedFS gewählt, welches auf den Prinzipien von Haystack entwickelt wurde. Die drei Punkte aus dem Konzept, die realisiert werden mussten, waren: das Ablegen mehrerer kleiner Dateien in einer großen Datei, das Kodieren des logischen Zusammenhanges der Dateien in einem Schlüssel beziehungsweise Dateipfad und die Suche nach Dateien. Die letzten beiden Punkte konnte SeaweedFS alleine über den Filer lösen. Das Zusammenführen mehrerer Buchungen wurde über den Account-Updater realisiert. Die Buchungen wurden jedoch nicht in Buckets, sondern in \textit{AccountInfos} gefüllt. Das implementierte System hat eine große Ähnlichkeit mit dem entworfenen Konzept. Es gibt aber dennoch einige Unterschiede. So können Daten nicht an bestehende Dateien angehängt werden. Um Daten zu ändern, werden diese immer komplett überschrieben. Des Weiteren muss eine \textit{AccountInfo} immer als Ganzes gelesen werden, um an die darin enthaltenen Buchungen zu gelangen. Aus diesem Grund können in einer \textit{AccountInfo} nicht so viele Buchungen wie in einem Bucket gebündelt werden. 

Das implementierte System wurde dann in den Bereichen Interoperabilität, Skalierbarkeit, Performanz, Ausfallsicherheit und Wirtschaftlichkeit evaluiert. Die einfache Erweiterung der Speicherkapazität durch das Hinzufügen weiterer Store Machines räumt Banken die Möglichkeit ein, Buchungen über einen sehr langen Zeitraum persistent zu speichern. Auch die Leistung des entwickelten Systems scheint ausreichend, um als Grundlage für ein Buchungssystem zu dienen. Dabei ist jedoch zu beachten, dass für die optimale Performanz das Zugriffsverhalten der Kunden bekannt sein muss. Die Wahl der Maximalanzahl der Buchungen, die gebündelt werden, ist kritisch für die Antwortzeit des Systems. Die Ausfallsicherheit dürfte den aktuellen Systemen in nichts nachstehen. Durch Replikation und das Starten neuer Directorys, Filer und Store Machines kann ein Single Point of Failure vermieden und das System ohne Einbußen der Performanz skaliert werden. Der größte Vorteil des entwickelten Systems dürfte jedoch die Kosteneffizienz sein. Die Kostenersparnis von mehr als 50 \% tritt zwar erst bei größeren Datenmengen auf, jedoch sollten Banken bereits über genügend Daten verfügen, um sich diesen Vorteil zu Nutze machen zu können. Die Investition in das System kann auch im Zuge der Vorbereitung auf die PSD2 sinnvoll sein. Spätestens, wenn diese in Kraft tritt, müssen Banken einen Plan haben, um mit der stark wachsenden Datenmenge und den Anfragen vieler Clients umzugehen. Dennoch ist der Einsatz eines DFS als Grundlage eines Buchungssystem mit Risiko verbunden. Banken haben keine Erfahrung mit der Technologie und es ist ungewiss, ob sich die Vorteile eines DFS in der Realität auch entfalten können. Außerdem hat das entwickelte System mehr Systemkomponenten als traditionelle Buchungssysteme. Backend, Account-Updater, Filer, Directorys und Store Machines müssen einzeln verwaltet und gewartet werden. Wenn mehr Elemente in einem Prozess involviert sind, können sich jedoch auch leichter Fehler einschleichen. Bevor ein System, wie das entwickelte, eingesetzt werden kann, müssten lange Testreihen durchgeführt werden. Des Weiteren müssen Banken ihren Kunden ein gewisses Maß an Inkonsistenz bei der Anzeige der Buchungen zumuten. 

Der Schritt zum Einsatz eines DFS als Grundlage eines Buchungssystems ist sicher nicht leicht, jedoch trotzdem vielversprechend. Es ermöglicht die Skalierung von Buchungssystemen auf vielen Gebieten:
zum einen die Skalierung der Speicherkapazität durch das Hinzufügen neuer OSDs, zum anderen die Skalierung der Ausfallsicherheit durch den Einsatz von Replikas und nur geringen Abhängigkeiten der Systemkomponenten untereinander. Selbst wenn OSDs oder ein Master nicht erreichbar sind, können Anfragen durch Shadow Master und andere OSDs beantwortet werden. Zu guter Letzt, überzeugt das DFS durch die Skalierbarkeit auf der Kostenebene. Der Einsatz eines DFS macht die Verwaltung großer Datenmengen bezahlbar.

\afterpage{\blankpage}
\backmatter
%%%%%%%%%%%%%%%%%%%
%% create figure list
%%%%%%%%%%%%%%%%%%%

\listoffigures
\addcontentsline{toc}{chapter}{Verzeichnisse}			

%%%%%%%%%%%%%%%%%%%
%% create tables list
%%%%%%%%%%%%%%%%%%%
%\listoftables

%%%%%%%%%%%%%%%%%%%
%% create listings list
%%%%%%%%%%%%%%%%%%%
\lstlistoflistings
\addcontentsline{toc}{chapter}{Listings}				

\printbibliography
\addcontentsline{toc}{chapter}{Literatur}		

%%%%%%%%%%%%%%%%%%%
%% declaration on oath
%%%%%%%%%%%%%%%%%%%

\afterpage{\blankpage}
\addchap{Eidesstattliche Erklärung}

Hiermit versichere ich, dass ich die vorgelegte Bachelorarbeit selbstständig verfasst und noch nicht anderweitig zu Prüfungszwecken vorgelegt habe. Alle benutzten Quellen und Hilfsmittel sind angegeben, wörtliche und sinngemäße Zitate wurden als solche gekennzeichnet.

\vspace{20pt}
\begin{flushright}
$\overline{~~~~~~~~~~~~~~~~~\mbox{\BaAuthor, am \today}~~~~~~~~~~~~~~~~~}$
\end{flushright}
\end{document}

