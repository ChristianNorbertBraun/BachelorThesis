\documentclass[12pt,oneside,a4paper,parskip]{scrbook}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[ngerman]{babel}
\usepackage{floatflt} 
\usepackage{subfigure}
\usepackage[pdftex]{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{color}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{nicefrac}
\usepackage{pdfpages}
\usepackage{float} 
\usepackage{pdflscape}
\usepackage{subfigure}
\usepackage{pdfpages}  
\usepackage[verbose]{placeins} 
\usepackage[nouppercase,headsepline,plainfootsepline]{scrpage2}
\usepackage{listings}		
\usepackage{xcolor}			
\usepackage{color}			
\usepackage{caption}		
\usepackage{subfigure}			
\usepackage{epstopdf}		
\usepackage{longtable}  
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\usetikzlibrary{pgfplots.dateplot}
\usepackage{pgfplotstable}
\usepackage{filecontents}
\usepackage{setspace}
\usepackage[nolist]{acronym}
\usepackage{booktabs}
\usepackage[style=numeric]{biblatex}
%\bibliography{literatur2}
\addbibresource{literatur.bib}

%%%%%%%%%%%%%%%%%%%
%% definitions
%%%%%%%%%%%%%%%%%%%
\def\BaAuthor{Christian Norbert Braun}
\def\BaTitle{Einsatz eines Distributed File Systems zur Skalierung eines Banking-Buchungssystems}
\def\BaSupervisorOne{Prof.\ Dr.\ Steffen Heinzl}
\def\BaSupervisorTwo{Prof.\ Dr.\ Peter Braun}
\def\BaDeadline{31.03.2017}

\hypersetup{
pdfauthor={\BaAuthor},
pdftitle={\BaTitle},
pdfsubject={Subject},
pdfkeywords={Keywords}
}

%%%%%%%%%%%%%%%%%%%
%% configs to include
%%%%%%%%%%%%%%%%%%%
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\definecolor{pblue}{rgb}{0.13,0.13,1}
\definecolor{pgreen}{rgb}{0,0.5,0}
\definecolor{pred}{rgb}{0.9,0,0}
\definecolor{pgrey}{rgb}{0.46,0.45,0.48}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape
  linewidth=\textwidth
}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\lstset{language=xml,
  morestring=[b]",
  morestring=[s]{>}{<},
  morecomment=[s]{<?}{?>},
  stringstyle=\color{black},
  numbers=left,
  numberstyle=\scriptsize,
  stepnumber=1,
  numbersep=8pt,
  identifierstyle=\color{darkblue},
  keywordstyle=\color{cyan},
  backgroundcolor=\color{background},
  morekeywords={xmlns,version,type}% list your attributes here
}

\lstset{language=Java,
  showspaces=false,
  showtabs=false,
  tabsize=4,
  breaklines=true,
  keepspaces=true,      
  numbers=left,
  numberstyle=\scriptsize,
  stepnumber=1,
  numbersep=8pt,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{pgreen},
  keywordstyle=\color{pblue},
  stringstyle=\color{pred},
  basicstyle=\ttfamily,
  backgroundcolor=\color{background},
%  moredelim=[il][\textcolor{pgrey}]{$$},
%  moredelim=[is][\textcolor{pgrey}]{\%\%}{\%\%}
}




\begin{document}

\acrodefplural{dfs}[DFS]{Distributed File Systems}

\begin{acronym}
  \acro{dfs}[DFS]{Distributed File System}
  \acro{hdfs}[HDFS]{Hadoop Distributed File System}
  \acro{api}[API]{Application Programming Interface}
  \acro{bbs}[Buchungssystem]{Banking Buchungssystem}
\end{acronym}


%%%%%%%%%%%%%%%%%%%
%% Titelseite
%%%%%%%%%%%%%%%%%%%


\frontmatter
\titlehead{%  {\centering Seitenkopf}
  {Hochschule für angewandte Wissenschaften Würzburg-Schweinfurt\\
   Fakultät Informatik und Wirtschaftsinformatik}}
\subject{Bachelorarbeit}
\title{\BaTitle\\[15mm]}
\subtitle{\normalsize{vorgelegt an der Hochschule f\"{u}r angewandte Wissenschaften W\"{u}rzburg-Schweinfurt in der Fakult\"{a}t Informatik und Wirtschaftsinformatik zum Abschluss eines Studiums im Studiengang Informatik}}
\author{\BaAuthor}
\date{\normalsize{Eingereicht am: \BaDeadline}}
\publishers{
  \normalsize{Erstpr\"{u}fer: \BaSupervisorOne}\\
  \normalsize{Zweitpr\"{u}fer: \BaSupervisorTwo}\\
}

%\uppertitleback{ }
%\lowertitleback{ }

\maketitle


%%%%%%%%%%%%%%%%%%%
%% abstract
%%%%%%%%%%%%%%%%%%%

\section*{Zusammenfassung}
\addcontentsline{toc}{chapter}{Zusammenfassung}

TODO

\section*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

TODO

\newpage
\chapter*{Danksagung}
\addcontentsline{toc}{chapter}{Danksagung}
Danke an FH und adorsys. An Francis und an Prof.Heinzl. Eventuell auch an Korrekturleser.

%%%%%%%%%%%%%%%%%%%
%% Inhaltsverzeichnis
%%%%%%%%%%%%%%%%%%%
\tableofcontents										



%%%%%%%%%%%%%%%%%%%
%% Main part of the thesis
%%%%%%%%%%%%%%%%%%%
\mainmatter


\chapter{Einführung}\label{ch:intro}
Banken sind Big Data. Jeden Tag fließen unzählige Zahlungsprozesse und Kundendaten durch die Systeme deutscher und internationaler Banken. Alleine im Jahr 2012 schätzte man die Menge der gespeicherten Daten auf etwa 1,9 Petabyte pro Bank \cite{datanami}. Ein Ende des Datenwachstums ist nicht in Sicht. Durch den Einsatz von Mobile- und Online-Banking steigt nicht nur die Anzahl der Vertriebswege sondern auch der ausgeführten Transaktionen \cite{DBBigData}. Kunden rufen mobil ihren Kontostand ab, überweisen online ihre Rechnungen und bezahlen den Einkauf im Shoppingcenter mit der Kreditkarte. Dieses vertriebswegübergreifende Nutzen der Bankprodukte hat nicht nur einen Anstieg der Lese- und Schreibzugriffe auf die Systeme der Banken zur Folge, sondern fordert auch auf allen Vertriebswegen eine ähnlich gutes Nutzererlebnis \cite{bankwirtschaft}.

\section{Motivation}
Die weitläufig eingesetzten Kernbankensysteme sind veraltet und haben Probleme den Anforderungen und der Unmenge an Daten Herr zu werden. Ein Ausbau wäre möglich, ist jedoch aufgrund der benötigten Hard- und Software teuer oder liefert keine langfristige Skalierbarkeit und Ausfallsicherheit \cite{herzKernbankensystem}. Auf dieses Problem stießen vor den Banken schon Firmen wie Google, Facebook oder Yahoo. Die Lösung war in allen drei Fällen der Einsatz eines \acp{dfs}. So entwickelte Google das Google File System \cite{GFS} zur Skalierung ihrer Websuche, Facebook Haystack \cite{haystack} zum Speichern und Lesen von Bildern und Yahoo das \ac{hdfs} \cite{hdfs}. Diese Systeme laufen auf Standard-Hardware und sind daher einfach und günstig skalierbar. Außerdem überzeugen sie auch durch eine hohe Ausfallsicherheit und Verfügbarkeit. Auch Banken könnten durch die Möglichkeiten eines \acp{dfs} profitieren. Was bei Google und Co. funktioniert, birgt auch für Banken eine Chance langfristig mit den aufstrebenden FinTechs zu konkurrieren und das volle Potential ihrer Daten auszunutzen\cite{wiki:fintech}.

\section{Zielsetzung}

Banken führen Änderungen der IT Struktur in der Regel erst dann durch, wenn die zu übernehmende Technologie lang erprobt und sich als zuverlässig erwiesen hat. Doch der Wandel der Kunden im Umgang mit den Bankprodukten und die wachsende Datenmenge zwingt die Finanz Branche zu einem Umdenken \cite{bigdataBigStorage}. Im Rahmen dieser Arbeit sollen die Auswirkungen erarbeitet werden, die der Einsatz eines \acp{dfs} als Persistenzschicht eines Banking Buchungssystems (Buchungssystem) bewirken kann. Da nicht alle Prozesse, die ein Buchungssystem abbildet, die gleichen Anforderungen haben, gilt es diejenigen herauszufinden, welche durch ein \ac{dfs} realisierbar sind. Insbesondere sollen die Möglichkeiten einer verbesserten Skalierbarkeit und Ausfallsicherheit diskutiert werden. Aktuelle Buchungssysteme sind behebig und der Unterhalt für die Banken teuer \cite{bankingsCosts}. Durch ein auf Standard-Hardware optimiertes und leicht skalierbares System könnten Ressourcen akquiriert werden, wenn sie wirklich gebraucht werden und abgeschaltet werden, wenn sie nicht mehr nötig sind. So sollen die Kosten um ein Buchungssystem zu betreiben verringert werden. Im Idealfall profitieren davon nicht nur die Banken sondern auch deren Kunden. Zusätzlich zu den wirtschaftlichen Verbesserungen sollen auch die Entwicklungen in der Ausfallsicherheit und Verfügbarkeit in dieser Arbeit kenntlich gemacht werden.

\section{Umfeld}
Unterstützend und beratend bei der Entwicklung dieser Arbeit tritt das IT Consulting Unternehmen adorsys GmbH \& Co. KG auf. adorsys mit ihrem Hauptsitz in Nürnberg entwickelt mittlerweile seit mehr als 10 Jahren individuelle Software für Banken und Versicherungen. Zu den Kunden zählen neben der Teambank auch ERGO Direkt und Schwäbisch Hall. Auch die Entwicklung eines Open Source Kernbankensystems durch adorsys war zwischenzeitig geplant. Alles in Allem ist adorsys ein Partner mit Expertise im Finanzsektor und bei der Architektur von komplexen Systemen.

\section{Aufbau der Arbeit}
Im folgenden Kapitel wird zunächst genauer auf die Vorgehensweise der Recherche und Entwicklung der Arbeit eingegangen. So werden benötigte Metriken zum Messen der Performanz, Ausfallsicherheit und Skalierbarkeit erarbeitet. Außerdem wird der Rahmen der beispielhaften Implementierung und Bewertung der Lösung weiter abgesteckt.

Im Kapitel \nameref{buchungssystem} werden die grundlegenden Bestandteile und Aufgaben eines Buchungssystems erläutert. Besonders die Probleme und Entwicklungschancen sollen analysiert und ausgearbeitet werden.

Im darauf folgenden Kapitel soll auf Basis der vorher erarbeiteten Probleme die Funktionsweise eines \acp{dfs} näher erläutert und die Vor- und Nachteile analysiert werden. Auch auf typische Anwendungsgebiete und Grenzen wird näher eingegangen.

Nachdem nun die Begriffe des Buchungssystems und \acp{dfs} grob abgesteckt sind, können diese im Konzept-Kapitel verheiratet werden. Hier werden die Vorgänge zum Lesen und Schreiben von Buchungen sowie die Bedeutung von Transaktionen in einem Buchunugssystems betrachtet. Außerdem werden zusätzliche Maßnahmen zur Skalierung und Ausfallsicherheit besprochen.

Das Kapitel zur Implementierung beschreibt die schrittweise Umsetzung des Konzepts und die dafür benötigten Technologien. Auch die konkrete Auswahl der Programmiersprache und des \acp{dfs} wird hier getätigt.

Im Kapitel zur Evaluierung wird die Zielsetzung der Arbeit mit den Funktionen der Testimplementierung verglichen und das Ergebnis bewertet. Das Letzte Kapitel zeigt Möglichkeiten und Erweiterungen der Lösung auf.

\chapter{Vorgehensweise}
Die in der Einführung beschriebene Zielsetzung kann sich leider nur auf sehr wenige wissenschaftliche Quellen stützen. Gerade die benötigte Information zu den Banken ist rar gesät und kann meist nur aus Artikeln von News-Seiten oder Blogs entnommen werden. Angaben zur Datenmenge von Banken oder den Betriebskosten eines Kernbankensystems können daher nur geschätzt werden. Die Entwicklung des Konzepts und die darauf folgende Bewertung begründet sich also mehr auf Analysen und Annahmen als auf empirisch bewiesene Tatsachen. Alle technologiebezogenen Annahmen werden jedoch durch wissenschaftliche Arbeiten, welche sich mit den Grenzen und Möglichkeiten eben dieser Technologie beschäftigen, untermauert.

\section{Analyse der Ist-Situation}
Bevor eine Bewertung der Ist-Situation in irgendeiner Form durchgeführt werden kann, muss zunächst die Bedeutung eines Buchungssystems innerhalb des Kernbankensystems und der Bank verstanden werden. Dabei gilt es, nicht nur die technische Architektur herauszuarbeiten, sondern auch die abzubildenden Prozesse zu erfassen. Dies soll vor allem im Hinblick auf die Anforderungen geschehen, welche sich durch die neuen Vertriebswege wie Online- und Mobile-Banking ergeben haben. Wie sieht der Ablauf zum Erstellen einer Buchung aus? Was mussten Buchungssysteme damals und was müssen sie heute leisten? Entstehen überhaupt Probleme durch die zunehmende Anzahl an Anfragen an die Systeme der Banken? Welche Aufgaben habe welche Teile des Buchungssystems? Durch Fragen wie diese sollen die minimalen Anforderungen eines heutigen Buchungssystems erarbeitet und priorisiert werden. Außerdem soll erforscht werden zu welchen Bedingungen diese Anforderungen von den aktuellen Systemen abgebildet werden und wo die eingesetzte Persistenzschicht die Möglichkeiten des Systems ausbremst.

\section{Einsatzbereiche eines DFS}
Nachdem die Bedeutung des Buchungssystems bekannt ist, werden die Bestandteile herangezogen, bei denen die aktuell eingesetzte Persistenzschicht am schlechtesten auf die Anforderungen passt. Schlecht bedeutet hierbei, dass entweder zu viel oder zu wenig Funktionalität bereit gestellt wird oder, dass der Einsatz der Technologie massive Nachteile mit sich bringt. Danach soll die Funktionsweise mehrerer \ac{dfs} verstanden und deren Möglichkeiten mit den minimal benötigten Anforderungen abgeglichen werden. So lässt sich herausarbeiten, ob ein \ac{dfs} allein überhaupt zur Realisierung eines Bestandteiles des Buchungssystems geeignet ist. Zusätzlich sollen die Vor- und Nachteile eines \ac{dfs} mit den Vor- und Nachteilen der vorher eingesetzten Persistenzschicht verglichen werden. Je nachdem welche Technologie hier besser abschneidet, lässt sich absehen ob sich eine Investition in ein \ac{dfs} lohnt oder nicht.

\section{Entwicklung des Konzepts}
An dieser Stelle ist bereits klar wie ein Buchungssystem funktioniert und welche Anforderungen es zu erfüllen hat. Außerdem sind die Teile des Systems bekannt, welche Raum für Verbesserungen durch ein \ac{dfs} bieten. Auch die generelle Funktionalität von einem \ac{dfs} wurde erarbeitet und herausgefunden welches der gegenwärtigen \ac{dfs} am besten zur Realisierung einzelner Bestandteile eines Buchungssystems geeignet ist. Dieses Wissen soll als Grundlage dienen, ein Konzept zu entwickeln, das noch detaillierter auf die Bedürfnisse des Buchungssystem eingeht. Dabei dient die Funktionsweise des am besten passenden \acp{dfs} als Richtlinie, welches aber noch speziell für den Anwendungsfall eines Banking Buchungssystems angepasst werden soll. Ob bereits ein \ac{dfs} existiert, dass exakt so funktioniert spielt hier zunächst keine Rolle. Es soll vielmehr ein System entwickelt werden, das neben seinen Aufgaben auch die wirtschaftlichen, leistungsbezogenen und auf die Skalierung bezogenen Anforderungen bestmöglich erfüllt. Auf Basis dieses Systems soll das Erstellen und Lesen einer Buchung detailliert erklärt werden. Dieser Prozess kann dann den entsprechenden Schritten eines konventionellen Buchungssystems gegenüber gestellt werden.

\section{Beispielhafte Implementierung}
Um das Ergebnis des Konzepts hinsichtlich der Performanz und Umsetzbarkeit zu testen, soll es beispielhaft implementiert werden.
Dazu muss zum einen die Programmiersprache für das Backend als auch ein konkretes \ac{dfs} ausgewählt werden. Falls es kein \ac{dfs} geben sollte, welches den aus dem Konzept hevorgehenden Anforderungen gerecht wird, muss auf das am besten passende ausgewichen werden. Die tatsächliche Entwicklung eines \acp{dfs} ist äußerst komplex und sprengt bei weiten den Rahmen dieser Arbeit. Das führt dazu, dass das Konzept gegebenenfalls auf ein bereits existierendes \ac{dfs} angepasst werden muss. Des Weiteren steht die Entwicklung einer Schnittstelle zum \acp{dfs} und des Backends an. Alle Schritte sollen eine ausreichende Testabdeckung vorweisen und entsprechend dokumentiert werden.

\section{Bewertung der Lösung}
Die Bewertung der im Rahmen dieser Arbeit entwickelten Lösung kann nur auf Basis der tatsächlichen Implementierung erfolgen. Die Auswirkung von den Teilen des Konzepts, welche sich technisch nicht umsetzen lassen, können nur erahnt werden und sind deshalb nicht zu berücksichtigen. Zur Bewertung der entwickelten Lösung werden die fünf folgenden Kriterien herangezogen.

\begin{enumerate}
  \item \textbf{Integrierbarkeit:} Beschreibt wie hoch der Aufwand geschätzt wird das System in ein Buchungssystem zu integrieren.
  \item \textbf{Skalierbarkeit:} Wie gut lässt sich das System skalieren? Welche Teile können zum Flaschenhals werden? Zusätzlich zu diesen Fragen soll es auch darum gehen, ob eine lastbezogene Skalierung möglich ist und wenn ja, wie viel Aufwand dazu betrieben werden muss.
  \item \textbf{Performanz:} Die Leistung soll durch die Anzahl der beantworteten Anfragen bei maximaler Auslastung bewerten werden. Zusätzlich muss das System nach dem Performanz-Test einen korrekten Stand beinhalten.
  \item \textbf{Ausfallsicherheit:} Bei der Ausfallsicherheit soll betrachtet werden, wie viele Teile des Systems versagen können, bevor es zu einem inkonsistenten Zustand oder einem Ausfall führt.
  \item \textbf{Wirtschaftlichkeit:} Zu guter Letzt soll geschätzt werden was der Betrieb des Systems kostet. Dazu wird sowohl die erforderliche Hardware und deren Kosten betrachtet, als auch der durchschnittliche Preis um das System bei einem externen Anbieter zu hosten. Auch die Kosten für eine mögliche Skalierung und die Gewährleistung einer annehmbaren Ausfallsicherheit werden berücksichtigt.
\end{enumerate}

Der Einfluss und die Möglichkeiten eines \acp{dfs} auf ein Buchungssystem sollen möglichst klar aus diesen Kriterien hervorgehen.
Daher sollen auch aktuell bei Banken eingesetzte Buchungssysteme so gut wie möglich anhand dieser Kriterien bewertet werden. Im direkten Vergleich zeigt sich am besten was der Einsatz eines \acp{dfs} letztendlich für ein Buchungssystem leistet.

\chapter{Wesen und Probleme eines Buchungssystems}
\label{bookingSystem}
Die Anwendungssysteme der Banken sind alt und über lange Zeit gewachsen. Wo 1970 die ersten spartenbezogenen Programme nur bei Kredit-, Einlagegeschäften und
Wertpapierabwicklung unterstützten, bilden jetzt die Systeme der Banken nahezu alle Geschäftsprozesse ab \cite[16]{ITidF}. Um die Bedeutung des Buchungssystems innerhalb dieser komplexen Anwendungsstruktur zu verstehen, soll im Folgenden zunächst auf die Entstehung und Architektur der Anwendungsysteme von Banken eingegangen werden. Danach werden die Aufgaben und Bestandteile eines Buchungssystems erarbeitet und die daraus entstehenden Anforderungen aufgezeigt. Im letzten Schritt sollen die Probleme der aktuellen Systeme besonders hinsichtlich der neuen Vertriebswege behandelt werden. 
\label{buchungssystem}

\section{IT-Systeme der Banken}
Seit den ersten Anwendungssystemen der Banken stand das Konto und die Kontoführung im Zentrum. Das Hinzufügen neuer Funktionen und Anforderungen erfolgte über das anhängen neuer Module an eben diesen Konto bezogenen Kern. Die so angedockten Abwicklungssysteme waren zum Beispiel für die Abwicklung des Inlands- und Auslandszahlungsverkehr, des Kreditwesens oder der Einlagen verantwortlich. Nachdem die Banken auch Unterstützung bei den Geschäftsprozessen forderten, legte sich um die Abwicklungssysteme ein weiterer Ring. Dieser stellte Dienste für Kundenberater und Sachbearbeiter der Banken zur Verfügung \cite[18-20]{ITidF}\cite{SuPdIiB}. Diese Vorgehensweise führte zu einer Silo- oder auch Spartenarchitektur die in Abbildung \ref{zwiebel} dargestellt wird. Systeme dieser Art wurden ursprünglich von den Banken selbst entwickelt. Die stark heterogenen Teilsysteme und die unentwirrbaren Abhängigkeiten zwischen ihnen stellte sich jedoch als nicht weiter tragbar und wartbar heraus \cite{bankEnzy}\cite{SuPdIiB}\cite[52]{ITidF}. Besserung versprach der Einsatz von hoch standardisierten Teilsystemen oder Gesamtbankenlösungen von Drittanbietern. Die Standardisierung erlaubt über Parameter eine eingeschränkte Anpassung der Systeme an die Bedürfnisse der Banken. So können die benötigten Systeme von unterschiedlichen Anbietern eingekauft und verbunden werden. Im Gegensatz dazu steht die Gesamtbankenlösung, die weiterhin versucht allen Anforderungen und Funktionen einer Bank gerecht zu werden. Diese Art von Systemen ist aber aus ähnlichen Gründen, wie die ursprüngliche Anwendungsstruktur der Banken nicht besonders erfolgreich \cite[S. 56 ff.]{ITidF}. 

\begin{figure}
   \makebox[\textwidth]{\includegraphics[width=\paperwidth]{img/3/zwiebelstrktur.png}}
  \caption[Historische Anwendungsstruktur von Banken]{Historisch gewachsene Anwendungsstruktur von Banken. Entnommen aus \cite{SuPdIiB}}
  \label{zwiebel}
\end{figure}

Heutzutage sind bei Genossenschaftsbanken häufig die Produkte von Fiducia GAD \cite{fiducia}, bei kleineren Privatbanken Systeme von FIS Kordoba \cite{kordoba} oder SAP \cite{SAP} und bei den Sparkassen die Lösung der Finanz Informatik One System Plus (OSPlus) \cite{finanzinformatik} im Einsatz. Die Entwicklung eigener Systeme können sich nur noch wenige große Banken wie die Deutsche Bank erlauben. Bis auf die Lösung der Finanz Informatik OSPlus und FIS Kordoba handelt es sich in Deutschland in der Regel um Systeme die keine Gesamtbankenlösung bieten sondern mehr oder weniger Teile der Geschäfts- und Kontoprozesse der Banken abdecken und mit anderen Systemen kombiniert werden können \cite{einfuehrungKernbanksystem}\cite[56-58]{ITidF}. Auch wenn die aktuellen Systeme deutlich besser standardisiert und dadurch wartbarer als früher sind, ist die Struktur immer noch ähnlich oder baut im Kern sogar noch auf den Ursprüngen der Banken IT auf. Die Architektur lässt sich in vier Schichten aufteilen, die von oben nach unten folgendermaßen beschrieben sind \cite[104]{ITidF}:

\begin{enumerate}
\item \textbf{Visualisierungsschicht:} Die Visualisierungsschicht ist die Schnittstelle zum Benutzer und wird auf dessen Endgerät ausgeführt. Sie bezieht einerseits Daten von der Darstellungsschicht und zeigt diese ansprechend an. Andererseits leitet sie die Eingaben des Nutzers an die Darstellungsschicht weiter.
\item \textbf{Darstellungsschicht:} Diese Schicht ist verantwortlich für eine fehlerfreie Kommunikation zwischen Visualisierungsschicht und Anwendungsschicht. Daten werden von einer Schicht empfangen und in das gewünschte Format der anderen Schicht überführt.
\item \textbf{Anwendungsschicht:} Die Anwendungsschicht kümmert sich um die eigentliche Geschäftslogik für einen bestimmten Teilbereich. Sie stellt Schnittstellen für Dienste wie teilweise auch die neuen Vertriebswege Online- und Mobile-banking zur Verfügung und führt aufwendige Datenmanipulationen auf der Datenschicht durch. Aufgrund der teilweise sehr alten und komplexen Systeme finden sich hier auch noch Bereiche die noch in COBOL oder sogar Assembler geschrieben sind.
\item \textbf{Datenhaltungsschicht:} Die Datenhaltungsschicht ist für die Verwaltung der Stammdaten sowie der von der Anwendungsschicht gesendeten und angefragten Informationen zuständig. Auch Operationen für einfache Manipulationen der Daten werden von ihr bereitgestellt. In der Finanzindustrie gelten relationale Datenbanken als Standard für die Persistenzschicht der Datenhaltungsschicht \cite[105]{ITidF}\cite{MarkstudieKernbankensysteme}
\end{enumerate}

Die Datenhaltungsschicht als Basis der Architektur entspricht je nach Aufteilung und Definition entweder allein oder in Kombination mit einzelnen Anwendungsschichten dem Buchungssystem. Ähnlich wie in Abbildung \ref{zwiebel} bildet es auch heute noch das Zentrum der IT Landschaft von Banken. Als ein solches wird es auch häufig als Kernbankensystem bezeichnet. Diese Bezeichnung verdeutlicht auch, dass die Aufgabe des Buchungssystems die Erfüllung von Kernaufgaben ist und allein nur eine begrenzte Funktionalität bereitstellt. Die Unterstützung weitergehender Bank Prozesse muss durch das Aufsetzen weiterer Anwendungsschichten erfolgen \cite[58]{ITidF}. Da in der Literatur und Wirtschaft der Begriff Kernbankensystem auch häufig als eine Gesamtbankenlösung verstanden wird, wird in dieser Arbeit immer von Buchungssystem gesprochen sofern es um die Kernfunktionalität von Banken geht \cite{vergleichCoreBanking}.  


\section{Das Buchungssystem und seine Aufgaben}
Das Buchungssystem steht demnach im Zentrum jedes kontenbasierten Geschäftsvorfalls der Banken \cite{bankEnzy}. Daher muss es auch alle Kernfunktionen eben dieser erfüllen und unterstützen können. Dazu gehören die Zahlungsverkehrs-, Investitions- und Kreditfunktion aber auch die Verwaltung von Kundenstammdaten sowie die grundlegende Kontoführung. Für Investitions- und Kreditfunktion können auch die Begriffe Passiv- und Aktivgeschäfte genutzt werden \cite[12, 86]{DdF}\cite{einfuehrungKernbanksystem}. Im Folgenden soll der Inhalt der einzelnen Funktionen anhand von \cite[69-88]{DdF} und \cite[91-153]{bankwirtschaft} beschrieben werden.

\begin{itemize}
  \item \textbf{Aktivgeschäfte:} Das Aktivgeschäft erhält seinen Namen, da alle Kreditgeschäfte also Forderungen an Kunden in der Bilanz auf der Aktivseite abgebildet werden. Durch das Kreditgeschäft erzielen Banken einen Großteil ihrer Zinserträge. Jedoch ist damit auch ein hohes Risikopotential verbunden. Als Aktivgeschäft zählen unter Anderem alle klassischen Kreditgeschäfte.
  Zum Beispiel der Kontokorrentkredit oder auch Dispositionskredit, räumt Privatpersonen eine Überziehungsmöglichkeit des Kontos ein. Aber auch längerfristige Kreditgeschäfte wie der Hypothekarkredit, Baukredit oder Investitionskredit gehören zu den Aktivgeschäften. Bei den Kreditgeschäften gilt in der Regel, dass von der Bank zum Kunden ein vertraglich festgelegter Betrag fließt, dessen Rückzahlung zusätzlich Zinsen vom Schuldner auf ein Konto der Bank zu erbringen ist.
  \item \textbf{Passivgeschäfte:} Als Passivgeschäft werden alle Einlagegeschäfte der Banken bezeichnet. Im Gegensatz zu den Aktivgeschäften erscheinen sie in der Bilanz auf der Passiv Seite. Aktiv- und Passivgeschäfte stehen in einer engen Verbindung. Häufig finanzieren Kreditinstitute ihre Aktivgeschäfte durch die Einlagen der Kunden. Zu den Passivgeschäften gehören Sichteinlagen, und Termineinlagen. Sichteinlagen sind täglich fällige Gelder. In der Regel handelt es sich hierbei um die Einzahlungen auf ein Girokonto. Ihr Zweck ist hauptsächlich der bargeldlose Zahlungsverkehr. Termineinlagen hingegen entziehen den Kunden den Zugriff auf die Einlage für einen festgelegten Zeitraum welcher in der Regel auf eine Dauer von mindestens 30 Tage und maximal 5 Jahren angelegt wird. Es handelt sich also um Einlagen, welche der Kunde über einen gewissen Zeitraum nicht benötigt. Dafür werden Termineinlagen höher verzinst als Sichteinlagen. Werden Termineinlagen als Kündigungsgelder vereinbart, so gibt es keine Laufzeitfrist und die Auszahlung erfolgt nach Einreichen einer Kündigung und abgelaufener Kündigungsfrist. Bei Passivgeschäften fließt also ein festgelegter Betrag vom Kunden zur Bank, welche über die Einlage in einem festgelegten Rahmen verfügen darf.
  \item \textbf{Zahlungsverkehr:} Der Zahlungsverkehr im Bankengeschäft beschreibt die bare und unbare Übertragung von Zahlungsmitteln im Inland und Ausland. Zur Übertragung können Überweisungen, Kartenzahlungen oder Lastschriften genutzt werden. Die besondere Bedeutung des Zahlungsverkehrs liegt in der Unumgänglichkeit für die Bankkunden. Auch wenn Kunden keine Kredite nehmen oder keine Einlagen tätigen, so müssen dennoch immer Zahlungen über die Systeme der Banken ausgeführt werden. Gerade die zunehmende Digitalisierung wird einen großen Einfluss auf die Entwicklung des bargeldlosen Zahlungsverkehrs haben. 
\end{itemize}

Das Buchungssystem muss all diese Inhalte führen können und Kontobewegungen nachvollziehbar ablegen. Es dient also neben den Anwendungssystemen auch den gesetzlichen Rechnungsabschlüssen und Bilanzen als Grundlage \cite{bankEnzy}\cite{MarkstudieKernbankensysteme}. In der Regel wird diese Nachvollziehbarkeit durch die Doppelte Buchführung gewährleistet. Das heißt, dass eine Kontobewegung immer in den Konten beider beteiligten Parteien auftaucht. In Summe müssen alle Kredit- und Debitbeträge Null ergeben\cite{accounting}.

\section{Anforderungen}
Um die oben genannten Aufgaben Jetzt und auch noch in Zukunft zuverlässig zu erfüllen, müssen Buchungssysteme gewissen Anforderungen gerecht werden. Diese lassen sich in interne und externe Anforderungen unterteilen. Interne Anforderungen beschreiben die Bedingungen, die eine Bank nachkommen muss um die geschäftsinternen Prozesse reibungslos und wirtschaftlich durchzuführen. Beispiele dafür sind eine wohldefinierte Schnittstelle des Buchungssystems um bei Bedarf neue Geschäftsprozesse anzudocken oder ein detaillierte Analyse des Kundenverhaltens durchführen zu können. Auch die Reduktion der IT Wartungskosten gilt als eine interne Anforderung an Buchungssysteme. Externe Anforderungen beziehen sich auf Bedingungen, die Banken von außen auferlegt werden. Dazu gehören gesetzliche Richtlinien, sowie Veränderungen von Angebot und Nachfrage im Markt \cite{capgemini}. So wird durch den Kunden aber auch dem Gesetzgeber das zuverlässige Ablegen einer jeden Kontobewegung beziehungsweise Buchung gefordert. Das Buchungssystem muss folglich immer erreichbar sein und eine hohe Ausfallsicherheit gewährleisten. Da aber neben der Ausfallsicherheit auch die Geschwindigkeit der Abarbeitung bankfachlicher Prozesse relevant ist, spielt die Performanz der Buchungssysteme auch eine tragende Rolle. Banken legen deswegen häufig ihre Buchungssysteme redundant an und verbinden diese über ein sicheres und performantes Datennetz \cite{bankEnzy}\cite[97-99]{ITidF}. Die so nebeneinander gestellten Systeme erlauben auch eine Lastverteilung bei der Bearbeitung mehrerer Anfragen. Gerade in Hinblick auf die neuen Vertriebswege und dem steigenden bargeldlosen Zahlungsverkehr ist ein System, dass sich skalieren lässt unabdingbar \cite{bankEnzy}\cite{capgemini}. Die Abbildungen \ref{online-banking} und \ref{bargeldlos} zeigen dieses Wachstum auf.


\begin{filecontents}{date.dat}
date       value
2006-01-01  34
2007-01-01  34
2008-01-01  36
2009-01-01  37
2010-01-01  35
2011-01-01  44
2012-01-01  44
2013-01-01  45
2014-01-01  54
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahre},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anteil der Nutzer in Prozent},
date ZERO=2005-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date.dat};
\end{axis}
\end{tikzpicture}
\caption[Nutzer von Online-Banking in Deutschland]{Nutzer von Online-Banking in Deutschland. Nachempfunden nach \cite{onlinebanking}.}
\label{online-banking}
\end{center}
\end{figure}


\begin{filecontents}{date2.dat}
date       value
2011-01-01  90.61 
2012-01-01  94.38
2013-01-01  99.52
2014-01-01  103.34
2015-01-01  112.13
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahre},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anzahl der Transaktionen in Mrd.},
date ZERO=20010-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date2.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionen im bargeldlosen Zahlungsverkehr in der EU]{Anzahl der Transaktionen im bargeldlosen Zahlungsverkehr in der EU. Nachempfunden nach \cite{bargeldlos}.}
\label{bargeldlos}
\end{center}
\end{figure}


Die Folge dieser Entwicklung ist nicht nur eine steigende Anzahl an Zugriffen auf die Systeme der Banken, sondern auch eine stetig zunehmende Datenmenge. Nicht umsonst wurde in der Einleitung dieser Arbeit die Verbindung zwischen Big Data und Banken geknüpft. Wo ursprünglich das alleinige Speichern aller Buchungsdaten ausreichend war fordern neue Dienste und Richtlinien nun auch den schnellen und gezielten Zugriff auf Kundendaten, Konten und Buchungen \cite{bigdataBigStorage}. In diesem Zuge ist die Second Payment Service Directive (PSD2) zu nennen. Dabei handelt es sich um eine EU Richtlinie die im Januar 2016 in Kraft getreten ist. Demnach sind Banken verpflichtet auf Wunsch des Kunden alle seine Konten, und durchgeführten Buchungen einem Drittanbieter über eine sichere Verbindung zur Verfügung zu stellen. Auch das Autorisieren von Bezahlungen muss über diese Verbindung möglich sein. Die Europäische Aufsichtsbehörde hat am 23. Februar 2017 die Regulatory Technical Standards (RTS) veröffentlicht, welche festlegen sollen, wie diese Verbindung realisiert werden muss und wie die Schnittstelle für Drittanbieter auszusehen hat \cite{rts}. Jetzt haben die Banken bis zum vierten Quartal 2018 Zeit die Bestimmung in ihre Systeme zu integrieren \cite{eu-psd2}\cite{psd2dk}. Die PSD2 stellt eine enorme Herausforderung für die Buchungssysteme der Banken dar. Wo vorher nur Zugriffe von bankeigenen Systemen möglich waren können jetzt beliebige Drittanbieter Kontodaten und Kontobewegungen abfragen. Als Konsequenz können deutlich steigende Lese- und moderat steigende Schreibzugriffe erwartet werden. Die Vorteile die sich für den Endnutzer ergeben, stellen die Banken hingegen vor ein Problem. Für die Bankkunden sind alle Bankprodukte bankenübergreifend in einer einzigen Anwendung verfügbar. Drittanbieter können Kauf- und Sparverhalten der Kunden analysieren und sinnvolle Hinweise zur Kontoführung geben. Zusätzlich sind Kunden nicht mehr an die eine Schnittstelle ihrer Bank gebunden, um Buchungen durchzuführen, sondern können ihrer Finanzen mit der besten am Markt erhältlichen Anwendung verwalten. Die Banken müssen jedoch zum einen mit der größeren Belastung der Buchungssysteme umgehen und zum Anderen ihre Anwendungen und Infrastruktur umstrukturieren um trotz Konkurrenz die Kunden auf ihrer Plattform halten zu können \cite{psd2vid}.
Die Anforderungen an ein Buchungssystem belaufen sich demnach auf eine wohl definierte Schnittstelle sowohl für bankeigene Prozesse als auch für Drittanbieter, ein gutes Kosten zu Nutzen Verhältnis der Systeme, sowie auf Skalierbarkeit und Ausfallsicherheit welche den Forderungen durch PSD2 und neuen Vertriebswegen wie Mobile- und Online-Banking gerecht wird.

\section{Probleme}
Wie bereits erwähnt sind die IT-Landschaften der Banken und insbesondere die Buchungssysteme alt und über einen langen Zeitraum gewachsen. Neue Funktionalität wurde über das Hinzufügen neuer Schichten realisiert, wobei der Kern gleich blieb. Da aber zur Zeit der Entwicklung der Buchungssysteme die heutigen Anforderungen noch nicht absehbar waren, können diese auch nicht zufriedenstellend erfüllt werden \cite[23-27]{ITidF}\cite{bankEnzy}. Auch die Banken selbst sind sich bewusst, dass etwas getan werden muss, um weiter auf dem Markt relevant zu bleiben \cite{capgemini}.
Besonders die Skalierung der Systeme könnte sich für die Banken als Problem herausstellen. Durch die PSD2 und die neuen Vertriebswege werden eine steigende Datenmenge sowie Lese- und Schreibzugriffe eine hohe Auslastung der Buchungssysteme zur Folge haben \cite{bigdataBigStorage}. Experten gehen davon aus, dass sich die Menge der Daten bis 2020 versiebenfacht \cite{versiebenfacht}. Buchungssysteme können aber nicht beliebig skaliert werden um den Anforderungen Herr zu werden. Die eingesetzten relationalen Datenbanken scheinen für viele Aufgaben essentiell, sind aber durch ihre Architektur und grundlegenden Konzepte nicht für das Speichern und Verwalten beliebig vieler Einträge geeignet \cite{rdbmsBigData}. Die Grundlage für relationale Datenbanken bildet das ACID Prinzip. Atomarität, Konsistenzerhaltung, Isolation und Dauerhaftigkeit beschreiben eine mächtige Möglichkeit Transaktionen innerhalb eines Systems abzubilden. Auch bei nebenläufigen Prozessen kann so immer ein konsistenter Stand gewährleistet und im Fehlerfall wieder hergestellt werden. Diese Möglichkeiten gehen jedoch auf Kosten der Performanz. Um die Einhaltung der ACID Prinzipien zu garantieren, müssen alle an einer Transaktion beteiligten Einträge mit einem exklusiven Lock versehen werden. Bei einer stark verteilten Datenbank erfordert dieser Vorgang ein eigenes verteiltes Commit-Protokoll auch Zwei-Phasen-Commit genannt \cite{dbarchitecture}. Alle an einem Commit beteiligten Datenbanksysteme müssen den Commit bestätigen. Denn entweder wird die Transaktion auf allen Systemen oder auf keinem ausgeführt. Ist ein Datenbanksystem nicht verfügbar, kann die Transaktion nicht durchgeführt werden. Bei zwei Datenbanksystemen mit jeweils 99,9 \% Verfügbarkeit wird diese durch die Abhängigkeit des Zwei-Phasen-Commits auf 99,8 \% reduziert \cite{BASE}. Wie sich der Transaktionsdurchsatz einer relationalen Datenbank bei einem steigenden Anteil an konkurrierenden Anfragen verhält ist in Abbildung \ref{salt} zu erkennen. Hierbei wurden Transaktionen mit jeweils fünf Update-Operationen auf Tabellen mit unterschiedlich vielen Einträgen ausgeführt. Je nach Anzahl $N$ der Reihen in den Tabellen kam es so zu mehr oder weniger konkurrierenden Zugriffen \cite{salt}.
\begin{filecontents}{date3.dat}
date  value
0     70000
1     65000
2     10000
3     2000
4     300
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xtick=data,
ymin=0, ymax=80000,
scaled ticks=false,
xticklabels={0,0{,}0001,0{,}001,0{,}01,0{,}1},
xlabel={Konkurrierender Zugriff (1/\#Reihen)},
ylabel={Transaktionsdurchsatz (Transaktionen/Sekunde)},
]
\addplot table[x=date,y=value] {date3.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionsdurchsatz im Verhältnis zum konkurrierenden Zugriff]{Transaktionsdurchsatz im Verhältnis zum konkurrierenden Zugriff auf Ressourcen in einem ACID basierten System. Nachempfunden nach \cite{salt}.}
\label{salt}
\end{center}
\end{figure}
Der sinkende Transaktionsdurchsatz ist eine Folge der nach ACID-Prinzipien durchgeführten Transaktionen. Hierbei werden Datenbankeinträge erst wieder freigegeben, nachdem die gesamte Transaktion durchgeführt oder abgebrochen wurde. Je nachdem wie das Locking implementiert oder welche Daten angefordert werden, kann eine Transaktion ein Lock für einzelne Tabellenzeilen, die ganze Tabelle, ganze Datenblöcke oder sogar für das ganze Datenbanksystem anfordern. Es ist also durchaus möglich, dass eine Kontotransaktion für einen Kunden auch die Einträge anderer Kunden mit beansprucht \cite{locking}\cite{dbarchitecture}.

Aber nicht nur bei rein schreibenden Transaktionen fällt die Performanz relationaler Datenbanken enorm ab. Die Abbildung \ref{salt2} zeigt den Transaktionsdurchsatz auf eine Tabelle mit 100 Einträgen bei Transaktionen die entweder fünf Einträge verändern oder lesen, während der Anteil der lesenden Transaktionen immer weiter zunimmt. Wie zu sehen ist führt schon ein zehn prozentiger Anteil an Schreibzugriffen zur Halbierung des Transaktionsdurchsatzes.
\begin{filecontents}{date4.dat}
date  value
0     100000
5     90000
10    50000
15    12000
20     8000
50     5000
100    1000
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xmin=0, xmax=100,
ymin=0, ymax=100000,
scaled ticks=false,
xlabel={Anzahl Schreibzugriffe in Prozent},
ylabel={Transaktionsdurchsatz (Transaktionen/Sekunde)},
]
\addplot table[x=date,y=value] {date4.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionsdurchsatz im Verhältnis zum Anteil der Schreibzugriffe]{Transaktionsdurchsatz im Verhältnis zum Anteil der Schreibzugriffe in einem ACID basierten System. Nachempfunden nach \cite{salt}.}
\label{salt2}
\end{center}
\end{figure}

Zusätzlich ist bei einem sehr hohen Durchsatz von Transaktionen nicht zu erwarten, dass zwei Transaktionen mit ähnlichen Daten auch im Speicher immer nah zusammen liegen. Bezogen auf die kontenbezogenen Daten, kann ein hoher Durchsatz von schreibenden Transaktionen zu einer Datenfragmentierung führen. Das bedeutet, dass es unwahrscheinlich ist, dass alle Umsätze eines Kunden innerhalb der Umsatztabelle der Bank auch im Speicher nahe zusammen liegen. Auch das könnte einen Einfluss auf die Lesegeschwindigkeit haben. Relationale Datenbanken werden wenn, dann in der Regel vertikal Skaliert. Horizontale Skalierung ist zwar zum Beispiel über Sharding möglich, ist jedoch meist aufwendig zu implementieren und hat bei verteilten Anfragen negative Auswirkungen auf die Performanz der Datenbanksysteme. Vertikale Skalierung beschreibt die Skalierung von Systemen durch das Aufrüsten von Hardware Komponenten. Also zum Beispiel das Einsetzen eines größeren Hauptspeichers oder eines schnelleren Prozessors. Horizontale Skalierung hingegen wird durch die Verteilung der Anwendung auf mehrere Maschinen erreicht. Die Nachteile an der vertikalen Skalierung sind jedoch die sehr limitierten Möglichkeiten und die höheren Kosten im Vergleich zur horizontalen Skalierung \cite{sharding}\cite{rdbmssuck}. 

Die Kosten der IT-Systeme sind jedoch auch jetzt schon ein Problem für die Banken. Eine exakte Ermittlung der Ausgaben für die IT der Banken stellt sich aber als schwierig heraus, da die Zahlen in der Regel nicht öffentlich gemacht werden. Die Schätzungen der IT Kosten beziehen sich daher auf die in der Gewinn- und Verlustrechnung öffentlich gemachten Aufwendungen für den Verwaltungsaufwand der Banken. Experten schätzen das 15 - 20 \% von diesem für die IT anfallen. Demnach lagen die Kosten für die Anwendungssysteme der Deutschen Bank im Jahre 2004 bei rund 2,6 Mrd. Euro, und die der HypoVereinsbank bei mehr als 900 Mio. Euro. Die zehn größten Banken in Deutschland haben in 2014 etwa 6,5 Mrd. Euro für die Instandhaltung und Entwicklung ihrer IT ausgegeben. 
Etwa 24 \% davon laufen alleine in die Buchungsdienste. Gerechnet auf die Girokonten ergibt das Jahresausgaben von etwa 14 Euro pro Girokonto \cite[29-39]{ITidF}. Diese Zahl lässt sich auch durch den Umsatz und die betreuten Konten von Kernbanksystem Anbietern validieren. So betreute die Fiducia IT 2005 insgesamt 52,6 Mio. Kundenkonten und erzielte einen Jahresumsatz von etwa 728,6 Mio. Euro. Das entspricht einem Preis von 13,80 Euro pro Konto. Die Finanz Informatik im Jahr 2005 erzielte bei 85 Mio. Kundenkonten einen Umsatz von 723 Mio. Euro, was einen Kontopreis von etwa 12,50 Euro entspricht. Dabei fließen nur etwa 20 \% in die Entwicklung neuer Dienste. Die restlichen Kosten fallen für den reinen Betrieb und Instandhaltung an. Langfristig sind diese Ausgaben für Banken nicht tragbar \cite[75-91]{ITidF}\cite{SuPdIiB}\cite[41-42]{DdF}\cite{bankingsCosts}. 


\chapter{Grundlagen eines DFS}
Änderungen an der Skalierung der Buchungssystemen und dem insgesamt hohen Kostenfaktor der Anwendungssysteme von Banken, werden auch durch die ACID basierten Datenbanksysteme erschwert. Auch wenn die Funktionalität die ACID mit sich bringt nur für einen Bruchteil der Prozesse benötigt wird, wird so das gesamte System ausgebremst und in den Skalierungsmöglichkeiten eingeschränkt \cite{salt}.
Systeme die hingegen die Skalierbarkeit und Verfügbarkeit gegenüber der Konsistenz priorisieren funktionieren nach dem BASE (Basically Available, Soft State, Eventual consistency) Prinzip. Häufig weisen diese Systeme niedrigere Betriebskosten im Gegensatz zu den ACID Alternativen auf \cite{clusterBASE}.
In diesem Kapitel soll zunächst die Bedeutung von BASE geklärt werden. Ein besonderer Fokus liegt dabei auf den Unterschieden der Konsistenz zu traditionellen ACID Systemen. Dann soll es um die Funktionsweise von DFS gehen. Diese funktionieren nach dem BASE Prinzip und haben Verfügbarkeit, Ausfallsicherheit und Kosteneffizienz zu ihrer Königsdisziplin gemacht. Zum besseren Verständnis werden zwei populäre Vertreter der DFS näher erläutert. Im letzten Teil des Kapitels werden die Vorteile und Grenzen der DFS nochmal genauer betrachtet und die Anforderungen an ein Buchungssystem mit den Möglichkeiten eines DFS abgeglichen.

\section{BASE}
ACID Systeme sind weit verbreitet und bekannt. Sie bringen eine sehr starke Semantik, verursachen aber in verteilten Systemen hohe Komplexität und Kosten. Konsistenz ist das oberste Ziel von ACID basierten Systemen. Die Verfügbarkeit der Systeme wird nicht garantiert. Im Gegenteil, es wird sogar bevorzugt keine Antwort zu geben, als eine falsche \cite{clusterBASE}. Die Skalierung solcher Systeme ist schwierig. Ist ein Teilsystem nicht erreichbar, leidet die Verfügbarkeit des gesamten Systems darunter \cite{BASE}. In einer idealen Welt wären Systeme gleichermaßen Skalierbar, Konsistent und Verfügbar. Solche Systeme kann es jedoch nach dem CAP Theorem (Consistency, Availability, Partition tolerance) nicht geben. Demnach können verteilte Systeme nur zwei von den drei Eigenschaften, Konsistenz, Verfügbarkeit und Partitionstoleranz erfüllen. Da Partitionstoleranz jedoch für jede Art der Skalierung benötigt wird, kann die Wahl nur zwischen Verfügbarkeit und Konsistenz fallen. Die Vorstellung Konsistenz gegen Verfügbarkeit zu tauschen erscheint häufig bedenklich. Es ist aber wichtig zu verstehen, dass eine Entscheidung für zwei Eigenschaften des CAP Theorem nie einen hundertprozentigen Ausschluss der dritten bewirkt.
Das heißt hoch konsistente Systeme verzichten nicht komplett auf die Verfügbarkeit und hoch verfügbare Systeme können einen gewissen Grad an Konsistenz bieten \cite{cap}. Die Herausforderung ist ein für die Anwendung möglichst passendes Verhältnis zwischen Konsistenz und Verfügbarkeit zu finden. So wie die Verfügbarkeit eines Systems in Prozent angegeben werden kann, gibt es auch Freiraum in der Gestaltung der Konsistenz. Die Autoren Paolo Viotti und Marko Vukolic unterscheiden in ihrem Artikel \textit{Consistency in Non-Transactional Distributed Storage Systems} insgesamt 50 Konsistenzarten \cite{consistency}. Demnach war in den 80iger Jahren nur die strong consistency bekannt. Sie fordert, dass Datenoperationen immer direkt zwischen der Anfrage vom und der Antwort zum Client durchgeführt werden müssen. Zusätzlich muss jeder Lesezugriff den tatsächlich letzten geschriebenen Wert zurückliefern. Am anderen Ende des Konsistenzspektrums befinden sich die weak und eventual consistency. Bei weak consistency, müssen Lesezugriffe nicht immer den tatsächlich letzten geschrieben Wert zurückliefern. Auch die Reihenfolge in der die Schreibeoperationen ausgeführt werden ist nicht vorgegeben. Mehrfache Lesezugriffe nacheinander müssen nicht immer das gleiche Ergebnis liefern. Eventual consistency ist etwas stärker als weak consistency. Auch hier liefert ein Lesezugriff nicht immer die letzten geschrieben Werte zurück. Wird aber für eine Weile kein Schreibzugriff durchgeführt, werden immer die gleichen Werte gelesen. Es tritt also irgendwann ein konsistenter Stand ein. Eventual consistency aus der Sicht einzelner Datenreplikas in einem verteilten System lässt sich mit drei Eigenschaften beschreiben:
\begin{itemize}
  \item \textbf{Eventual delivery:} Wenn ein Datenreplika eine Schreiboperation erreicht, wird diese Schreiboperation irgendwann bei allen Datenreplikas durchgeführt
  \item \textbf{Convergence:} Alle Datenreplikas, welche die Schreiboperationen erhalten haben, werden irgendwann identisch sein
  \item \textbf{Termination:} Alle Operationen werden ausgeführt und beendet.
\end{itemize}
Systeme die auf dem BASE Prinzip aufsetzen tauschen eine strong consistency gegen die eventual consistency ein und erreichen so eine hohe Verfügbarkeit. Wo ACID pessimistisch mit Schreibzugriffen umgeht und Konsistenz immer am Ende einer Transaktion fordert, ist BASE optimistisch und garantiert nur, dass irgendwann Konsistenz eintreten wird. Dieser Ansatz stützt sich darauf, dass dem Nutzer eine immer verfügbare Anwendung wichtiger ist, als das sie immer der korrekte Stand angezeigt. Vor allem wenn sich die Dauer des inkonsistenten Zustands auf wenige Sekunden beläuft. Durch diesen kleinen Abstrich bei der Konsistenz schaffen BASE Systeme aber viel Raum für Skalierung und Performance Steigerungen. Kommunikation zwischen den Teilsystemen die in einem ACID System zwingend nötig ist, kann in einem BASE System vernachlässigt oder auf einen besser passenden Zeitpunkt verschoben werden \cite{clusterBASE}. Client Anfragen können so nahezu immer und schnell beantwortet werden. Zusätzlich belastet das Starten mehrere parallel laufender Systeme das Netzwerk nicht so stark wie bei vergleichbaren ACID Systemen.

\section{Funktionsweise}
DFS basieren auf dem oben erklärten BASE Prinzip. Sie lockern die Konsistenz auf und ermöglichen dadurch gute Skalierbarkeit, Verfügbarkeit und Kosteneffizienz. Deswegen haben DFS einen besonderen Stellenwert in der Speicherung und Verarbeitung von Big Data und werden von vielen Firmen in diesem Bereich eingesetzt. Beispiele hierfür sind das Google File System, welches von Google selbst eingesetzt wird, Facebooks Haystack, das von der deutschen Telekom, CERN und Cisco verwendete DFS Ceph \cite{ceph} und das von Yahoo entwickelte und weit verbreitete Hadoop Distributed File System (HDFS).

Um sehr große Datenmengen verwalten zu können bedienen sich die meisten DFS dem sogenannten Object Storage \cite{cephPaper}. Traditionelle Filesysteme arbeiten mit dem File Storage bei dem die Dateien und die dazugehörigen Metadaten getrennt abgespeichert werden. Gerade bei sehr vielen Dateien wird die Verwaltung der Metadaten hier zu einem Flaschenhals \cite{filestorage}. Beim Object Storage hingegen werden Datei und die dazugehörigen Metadaten gemeinsam abgespeichert. Die Kombination aus Dateiinhalt und den Metadaten wird auch Objekt genannt. Anstelle auf der Datei zu arbeiten werden alle Operationen auf Ebene der Objekte durchgeführt. Lesen, Schreiben und Löschen funktioniert ähnlich wie bei einem traditionellen Filesystem, jedoch kann in einem Object Storage der Nutzer nicht bestimmen wie und wo das Objekt tatsächlich abgelegt wird. Der Zugriff auf einen Object Storage erfolgt auch meistens über eine abstrakte Schnittstelle wie zum Beispiel http. Die Speicherverwaltung fällt demnach komplett in den Aufgabenbereich der sogenannten Object Storage Devices (OSD). Mehrere OSDs können so einfach nebeneinander gestellt werden und durch eine http Schnittstelle wie ein einziges OSD angesprochen werden. Dafür ist lediglich eine zusätzliche Verwaltung der einzelnen OSDs notwendig. Auf diese Weise lässt sich ein Object Storage extrem gut skalieren und liefert eine nahezu endlose Speicherkapazität. Dadurch, dass der Nutzer einen Cluster an OSDs wie einen einzigen ansprechen kann, können die einzelnen OSDs sogar über Ländergrenzen hinweg verteilt sein. So kann zusätzlich Ausfallsicherheit gewährleistet werden \cite{osvideo}\cite{objectstorage}\cite{objectBasedStorage}. In der Regel bestehen Object Storage Systeme aus den OSDs selbst und einem Server, welcher diese OSDs verwaltet.

Nachdem die Grundlage eines DFS besprochen wurde, soll die konkrete Funktionsweise zweier DFS erklärt werden. Da das später entwickelte Konzept auf den Prinzipien der beiden DFS aufbaut, werden die Annahmen der Systeme sowie ein Lese- und Schreibzugriff auf das DFS ausführlich beschrieben. Die beiden betrachteten DFS sind das Google File System (GFS) und Haystack.
Das GFS wurde 2003 im Artikel \textit{The Google File System} vorgestellt \cite{GFS}. Demnach wurde es von Google entwickelt um die Analyse und Verwaltung ihrer extrem schnell wachsenden Datenmenge zu ermöglichen. Wie bei allen DFS wurde das GFS ganz im Sinne guter Skalierbarkeit, Verfügbarkeit, Verlässlichkeit und Leistung konzipiert. Aber auch andere Annahmen wurden bei dem Design des GFS berücksichtigt. So sind Ausfälle oder korrupte Daten bei einem System in dieser Größenordnung eher die Regel als die Ausnahme. Dateien sollen nachdem sie einmal geschrieben wurden nur noch gelesen werden. Ein Anhängen von Daten an bereits bestehenden Dateien soll aber möglich sein. Die zu verwaltenden Dateien sind in der Regel sehr groß, von einigen Megabyte bis zu mehreren Gigabyte. Demnach muss das System mit Lese- und Schreibzugriffen auf große Datenmengen zurecht kommen. Da kleine Dateien eher die Ausnahme sind, wird dem Lesen und Schreiben dieser keine besondere Aufmerksamkeit geschenkt. Das GFS soll auch mit den Anfragen von sehr vielen Clients umgehen können. Daraus folgt, dass der konkurrierende Zugriff auf eine Datei besonders berücksichtigt werden muss.  

Das GFS stellt den Clients ähnliche Operationen wie gewöhnliche File Systeme zur Verfügung. Dateien können geöffnet, geschlossen, gelesen und geschrieben werden und sind in Pfaden und Ordnern strukturiert. Zusätzlich können Daten an bestehende Dateien angehängt und Snapshots also Kopien von Dateien erzeugt werden. Das GFS selbst besteht dabei aus zwei Hauptkomponenten. Einem einzigen Master und mehreren Chunkservern. Dateien werden in 64 Megabyte große Chunks mit jeweils 64 Byte Metadaten aufgeteilt und auf den Chunkservern platziert. Der Master verwaltet dabei den Namensraum für die einzelnen Dateien und Chunks. Außerdem kennt er für jede Datei die dazugehören Chunks mit ihren Positionen und den Positionen ihrer Replikas. Für den schnellen Zugriff werden diese Informationen immer im Hauptspeicher des Masters vorgehalten. Um beim Starten des Masters oder im Fehlerfall die Zuweisungen von Datei zu Chunk wiederherstellen zu können, werden alle Änderungen an der Struktur und des Namensraums persistent in einem Operation Log gespeichert. Dieser agiert als Timeline und weißt jeder Änderung einen eindeutigen Zeitstempel zu. Ein einzelner Master ermöglicht ein sehr ausgeklügelte Chunk Platzierung auf den Chunkservern und vereinfacht das gesamte Design des Systems. Auf der anderen Seite, kann er aber auch zum Flaschenhals werden. Deswegen ist es sehr wichtig die Anfragen der Clients an den Master so gering wie möglich zu halten. Es werden keine Schreib- und Lesezugriffe auf Dateien direkt durch den Master durchgeführt.

\begin{figure}[htb]
  \centering
  \includegraphics[width=15cm]{img/4/ReadGFS.png}
  \caption[Lesezugriff auf einen Chunk in GFS]{Lesezugriff auf einen Chunk in GFS. Nachempfunden nach \cite{GFS}.}
  \label{readgfs}
\end{figure}

Der Ablauf eines Lesezugriffes ist in Abbildung \ref{readgfs} zu sehen. Der Client weiß, welchen Teil einer Datei er lesen möchte. Aus der bekannten Chunkgröße kann so der Index des benötigten Chunks errechnet werden. Auf eine Anfrage an den Master mit dem Dateinamen und dem Chunkindex antwortet dieser mit der Position des Chunks und all seiner Replikas. Der Client kann nun einen Chunk, der ihm am nächsten liegt auswählen und die Anfrage an den entsprechenden Chunkserver stellen. Dabei kann er auch übertragen welche Bytes er innerhalb des Chunks erhalten möchte. Der Chunkserver antwortet dem Client darauf hin mit den angefragten Daten. Da sich die Position der einzelnen Chunks in der Regel nicht ändert, können die Clients diese Information cachen und bei weiteren Lesezugriffen darauf zurückgreifen. Auch das entlastet den Master weiter.

\begin{figure}[htb]
  \centering
  \includegraphics[width=15cm]{img/4/writeGFS.png}
  \caption[Schreiben eines Chunks in GFS]{Ablauf eines Schreibvorgangs in GFS. Nachempfunden nach \cite{GFS}.}
  \label{writeGFS}
\end{figure}

Ein Schreibzugriff hingegen ist in Abbildung \ref{writeGFS} zu sehen. Um zu gewährleisten, dass alle Replikas eines Chunks irgendwann identisch sind, ist es wichtig, dass die Schreiboperationen in der gleichen Reihenfolge durchgeführt werden. GFS realisiert dies durch den Einsatz von Leases. Wenn ein Client eine Datei schreiben möchte, fordert er vom Master ein Lease auf einen Chunk an. Ein Lease hat eine Dauer von 60 Sekunden, kann aber bei Bedarf auch verlängert werden. Der Master gewährt dem Client ein Lease und sendet zusätzlich die Positionen des zu beschreibenden Chunks und seiner Replikas. In GFS gibt es von jedem Chunk standardmäßig drei Replikas. Einer dieser Chunks wird für die Dauer des Leases als Primary gekennzeichnet, die Anderen werden zu Secondaries. Darauf hin beginnt der Client die zu speichernden Daten zu den Replikas zu senden. Dabei ist die Reihenfolge in der dies geschieht egal. Es können auch erst die Secondaries und dann der Primary angesprochen werden. Sobald ein Replika Daten erhält, kann dieses die Daten zum nächsten weitersenden. Die Daten werden zunächst nur im Hauptspeicher gehalten und nicht persistent abgespeichert. Erst wenn die Daten bei allen Replikas angekommen sind, sendet der Client einen Schreibauftrag an den Primary Chunk, der die vorangegangenen Daten identifiziert. Der Primary Chunk verteilt diesen Schreibauftrag dann an alle Replikas welche daraufhin die Daten persistent ablegen. Jeder Secondary bestätigt dem Primary, dass die Daten erfolgreich geschrieben wurden oder teilt einen Fehler mit. Der Primary antwortet letztendlich dem Client und zeigt entweder den Erfolg der Operation an oder sendet die aufgetretenen Fehler. Möchte der Client während der Gültigkeit des Leases ein weiteres mal Daten schreiben, so muss keine Anfrage mehr an den Master getätigt werden und die Daten können direkt geschrieben werden. Dieser Prozess ermöglicht auch einen einfach Umgang mit einem zweiten Client der auf den gleichen Chunk schreiben möchte. Wenn der zweite Client nach einem Lease für den Chunk anfragt, erhält er als Antwort das gleiche Lease und demnach den gleichen Primary wie der erste Client. Auch der zweite Client beginnt die Daten in beliebiger Reihenfolge auf die Replikas zu streamen. Danach teilt er dem Primary den Schreibauftrag für die geschriebenen Daten mit. Da die Daten erst zum Ende des Leases persistent geschrieben werden, kann der Primary durch Kommunikation mit den Replikas sicherstellen, dass die Daten beider Clients in der richtigen Reihenfolge in den Replikas landen. So kann das Problem mehrerer konkurrierender Zugriffe leicht gelöst werden. Jedoch kann es bis zu 60 Sekunden dauern, bis ein Client den tatsächlich als letztes geschriebenen Wert eines Chunks auslesen kann.

Das Anfertigen von Replikas für jeden Chunk hat neben der Ausfallsicherheit auch noch performanztechnische Vorteile. Clients können immer das Replika lesen, das ihnen am nächsten ist. Wollen mehrere Clients die gleichen Daten lesen, können die Lesezugriffe auf alle bestehenden Lesezugriffe verteilt werden. Das GFS nutzt ausgeklügelte Mechanismen, um korrupte Replikas zu erkennen und zu ersetzen. Dabei werden Chunks mit besonders viel gelesenen Daten oder vielen korrupten Replikas am höchsten priorisiert. In regelmäßigen Heartbeat-Nachrichten teilen die Chunkserver dem Master mit, in welchem Zustand sich die einzelnen Chunks befinden. Ein weiteres Problem ist der einsame Master. Dieser stellt einen Single Point of Failure da. Um dies entgegenzuwirken werden parallel zum Master mehrere Shadow Masters betrieben. Diese hinken dem Master immer einige Operationen hinterher, können aber über den Operation Log immer auf den korrekten Stand gelangen. Inkonsistenz herrscht innerhalb des GFS also nur wenn ein Chunk gelesen wird auf dem gerade ein Lease angefordert wurde und bis der Shadow Master, im Falle eines Ausfall des Masters, die letzten Operationen aus dem Operation Log durchgeführt hat.

Ähnlich wie Google entwickelte auch Facebook ein eigenes DFS. Ihr Ziel war es die unzähligen Bilder die Tag für Tag auf ihr soziales Netzwerk geladen werden zu speichern und zu verwalten. 2010 beschrieben einige Entwickler von Facebook in dem Artikel \textit{Finding a Needle in Haystack: Facebook's Photo Storage} ihr DFS Haystack \cite{haystack}. 2010 verwaltete dieses bereits etwa 20 Petabytes an Daten. Das System ist in der Lage mehr als eine Millionen Lesezugriffe in der Sekunde zu bedienen und eine Milliarde hochgeladenen Fotos in der Woche Herr zu werden. Neben Skalierbarkeit, Ausfallsicherheit, Verfügbarkeit und Performance wurde bei Haystack ein besonderes Augenmerk auf die Reduzierung der Metadaten pro Datei gelegt. Für die Speicherung von vier Bildern benötigt Haystack gerade einmal 40 Bytes an Metadaten. Gewöhnliche File Systeme würden dafür 536 Byte anlegen. Dadurch sollen alle Metadaten immer im Hauptspeicher gehalten werden und einen schnellen Zugriff ermöglichen. Um dies zu erreichen, wurde auf ein POSIX konformes File System verzichtet. In Haystack gibt es keine Pfade oder Ordner für die Metadaten gehalten werden müssen. Bilder werden eindeutig über Schlüssel identifiziert. Ansonsten wurde Haystack unter ähnlichen Annahmen wie das GFS entwickelt. Ausfälle sind an der Tagesordnung, es müssen sehr viele Daten gespeichert werden und das System wird von sehr vielen Clients genutzt. Die gewöhnliche Dateigröße unterscheidet sich hingegen von den Anforderungen des GFS. Die hochgeladenen Bilder sind meist nur einige Kilobyte groß und sind damit vergleichsweise klein. In Haystack werden deshalb mehrere Bilder in einer großen Datei zusammengefasst. Diese Datei wird Volume genannt. Zu Beginn werden leere Volumes mit einer festen Größe angelegt und nach und nach mit Bildern aufgefüllt. Zehn Terabyte an Speicher können so in 100 Volumes mit eine Größe von jeweils 100 Gigabyte aufgeteilt werden.

\begin{figure}
  \centering
  \includegraphics[width=5cm]{img/4/volume.png}
  \caption[Aufbau eines Volumes]{ Aufbau eines Volumes. Nachempfunden nach \cite{haystack}.}
  \label{volume}
\end{figure}
Haystack besteht aus drei Systemelementen. Dem Haystack Directory, Haystack Store und dem Haystack Cache. Im Folgenden wird auf diese ohne den Präfix Haystack Bezug genommen.
\begin{itemize}
  \item \textbf{Directory:} Die Aufgabe des Directorys ist die Verwaltung aller Volumes. Um die Replikation der Daten zu vereinfachen, fasst das Directory mehrere physikalische Volumes in ein logisches Volume zusammen. Beim Schreibvorgang wird nur noch ein logisches Volume referenziert und in alle darin enthaltenen physikalischen Volumes geschrieben. Zusätzlich kann das Directory Volumes auf read-only setzen, wenn diese voll oder zum Teil korrupt sind.
  \item \textbf{Cache:} Um viele Anfragen direkt aus dem Hauptspeicher beantworten zu können, werden alle Bilder die das System verlassen im Cache gespeichert. Dadurch werden vor allem kurz nach dem Hochladen eines neuen Fotos die Maschinen entlastet. Denn neue Bilder werden häufiger gelesen als Bilder, die weiter in der Vergangenheit hochgeladen wurden. 
  \item \textbf{Store:} Der Store speichert die tatsächlichen Bilddaten. Er besteht ähnlich wie die Chunkserver beim GFS aus mehreren Store Machines. Dabei kümmert sich eine Store Machine um mehrere Volumes welche jeweils mehrere Millionen Bilder enthalten können. Der Aufbau eines Volumes ist in Abbildung \ref{volume} dargestellt. Demnach steht am Anfang immer ein Superblock, welcher Informationen zur Anzahl der freien Blöcke und der Größe des Dateisystems beinhaltet \cite{wiki:superblock}. Gefolgt wird dieser von mehreren Needles, die jeweils ein Bild repräsentieren. Der Zugriff auf ein Bild erfolgt über Angabe des Volume-Schlüssels, einem Offset und der Größe der zu lesenden Daten. All diese Daten werden von der Store Machine im Speicher gehalten und erlauben so einen schnellen Zugriff. Zusätzlich wird diese Information in einer Volume-Datei gesichert. Das verhindert, dass beim Starten einer Store Machine alle Volumes gelesen werden müssen um die benötigten Informationen in den Speicher zu schieben. 
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{img/4/ReadHaystack.png}
  \caption[Ablauf beim Lesen eines Bildes.]{ Ablauf beim Lesen eines Bildes. Nachempfunden nach \cite{haystack}.}
  \label{readHaystack}
\end{figure}

In Abbildung \ref{readHaystack} sind die Schritte zum Lesen eines Bildes in Haystack schematisch dargestellt. Zunächst fragt der Client das Directory nach dem gewünschten Bild. Dieses wird durch einen eindeutigen Schlüssel identifiziert der die Form \textit{[logisches Volume ID, Bild ID]} aufweist. Über das logische Volume kann das Directory sehr leicht ein physikalisches Volume heraussuchen und die Adresse einer Store Machine, die dieses beherbergt an den Client senden. Der Client sendet jetzt erneut den Schlüssel des Bildes aber diesmal an die Adresse der Store Machine. Diese Anfrage trifft den Cache. Wurde das Bild zuvor bereits einmal ausgeliefert, kann dieser direkt mit den Bilddaten antworten. Befindet sich das Bild nicht im Cache so wird die Anfrage zur Store Machine weitergeleitet. Diese sucht mithilfe des Schlüssels für das Bild und des logischen Volumes das physikalische Volume, die Größe des Bildes und den benötigten Offset heraus. Damit kann das Bild aus dem physikalischen Volume gelesen und über den Cache zum Client geschickt werden. 

Wie in Haystack ein Bild gespeichert wird, ist in Abbildung \ref{writeHaystack} zu sehen. Im ersten Schritt fragt der Client das Directory nach einem beschreibbaren logischen Volume. Dieses antwortet mit der ID des logischen Volumes und den Adressen der Store Machines, die die im logischen Volume enthaltenen physikalischen Volumes beinhalten. Darauf hin vergibt der Client einen eindeutigen Schlüssel für das hochzuladende Bild. Der Schlüssel entspricht der oben erklärten Form. Gemeinsam mit dem Schlüssel übertragt der Client daraufhin die Bilddaten an alle vom Directory erhaltenen Store Machines. Diese hängen das Bild in den entsprechenden Volumes an und aktualisieren die Volume-Information in ihrem Hauptspeicher. Da Bilder in Volumes immer nur an die bestehenden Bilder angehängt werden können, ist eine Modifikation eines bereits gespeicherten Bildes nicht möglich. Um ein solches Bild zu überschreiben, sendet der Client ein neues Bild mit dem gleichen Schlüssel wie das zu überschreibende Bild. Bei der nächsten Anfrage nach diesem Bild wird die Store Machine immer das Bild zurückgeben, das den größten Offset innerhalb des Volumes besitzt, da dieses immer das aktuellste ist. Der Speicher der für das ungültig gewordene Bild benötigt wird, bleibt weiterhin belegt. Um die Netzwerkbandbreite und die Leistung der Festplatten optimal auszunutzen, bemüht sich Haystack immer mehrere Bilder auf einmal zu einem Volume hinzuzufügen. Dies wird ermöglicht, da Nutzer von Facebook in der Regel Bilder in Alben auf die Plattform laden.

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{img/4/WriteHaystack.png}
  \caption[Ablauf beim Schreiben eines Bildes in Haystack.]{ Ablauf beim Schreiben eines Bildes. Nachempfunden nach \cite{haystack}.}
  \label{writeHaystack}
\end{figure}

Damit der Speicher der durch überschriebene oder korrupte Bilder belegt wird nicht verloren ist, werden Volumes regelmäßig verdichtet. Dazu wird das Volume Bild für Bild kopiert und korrupte oder überschriebene Bilder übersprungen. Daraufhin wird das ursprüngliche Volume durch das neue ersetzt. Um auch bei korrupten Bildern eine gleichmäßige Replikation zu garantieren, kommunizieren Store Machines und Directory gelegentlich miteinander. Dabei teilen die Store Machines den Inhalt und den Zustand ihrer Volumes mit. Sind einige Bilddaten korrumpiert, kann das Directory dem betroffenen logischen Volume ein neues physikalisches Volume zuordnen und die Replikation starten. Inkonsistenzen treten bei Haystack nur auf, wenn ein Bild angefordert wird, während es noch auf die einzelnen Store Machines verteilt wird, oder wenn ein korruptes Bild angefragt wird, bevor dieser Fehler von der zuständigen Store Machine erkannt wurde. Werden Bilder überschrieben, kann es auch passieren das Clients noch das ursprüngliche Bild aus dem Cache erhalten bis dieses ungültig wird.

Haystack und das GFS ähneln sich in den Grundstruktur des Systeme sehr. Die eigentlichen Daten werden auf extra dafür vorgesehenen OSDs gespeichert und ein Master kümmert sich um die Verwaltung dieser. Beide DFS ermöglichen einen effizienten Zugriff auf Daten der im besten Fall nur eine einzige Leseoperation von der Festplatte benötigt. Durch das Verteilen der Anfragen auf mehrere OSDs sind beide Systeme nahezu beliebig Skalierbar und liefern eine enorme Ausfallsicherheit. Dennoch gibt es nicht nur bei den gespeicherten Daten Unterschiede. Während das GFS sehr große Dateien verwaltet und diese sogar in Chunks aufteilt, bemüht sich Haystack darum mehrere Bilder in einer einzigen großen Datei, den Volumes zusammenzufassen. Demnach sind nicht nur die Schreibe- sondern auch die Lesezugriffe bei Haystack deutlich kleiner als bei dem GFS. Der Größe der Dateien ist auch die Tatsache geschuldet, dass sich ein Cache im GFS nicht lohnt. Ein weiterer Unterschied ist in der Anordnung der Dateien zu finden. GFS liefert ein fast POSIX konformes Dateisystem. Haystack hingegen verzichtet auf Pfade und Ordner komplett. Dadurch kann die Menge an Metadaten weiter reduziert werden. Dieser Schritt ist notwendig, da Haystack kleinere und damit mehr eigenständige Dateien verwaltet. 

\section{DFS zur Bewältigung der Anforderungen eines Buchungssystems}
Die Frage ist also, ob sich ein DFS dennoch eignet um die Anforderungen und Probleme eines Buchungssystems zu bewältigen.
Im Kapitel \ref{bookingSystem} wurden die aktuellen und kommenden Herausforderungen, denen sich ein Buchungssystem stellen muss erörtert. Ob ein DFS diesen gewachsen ist, soll sich mit dem in diesem Kapitel erarbeiteten Informationen zeigen \cite{largeHadoop}.

\begin{itemize}
  \item \textbf{Skalierbarkeit:} Die grundlegende Systemarchitektur vieler OSDs und eines Masters, der diese verwaltet, gewährleistet eine einfache horizontale Skalierung. Speicherkapazität kann nahezu nach Belieben durch das Hinzufügen weiterer OSDs erhöht werden. Auch die Verarbeitung von Lese- und Schreibzugriffe profitiert von dieser Art der Skalierung. Dadurch, dass diese Anfragen nie direkt durch den Master gehen, wird die Last im Netzwerk sowie auf die OSDs verteilt.

  \item \textbf{Ausfallsicherheit:} Die in einem DFS abgelegten Daten werden immer auf mehreren OSDs abgelegt. Ausgeklügelte Algorithmen zur Platzierung der Replikas ermöglichen eine sehr hohe Fehlertoleranz und Ausfallsicherheit. Die geographische Verteilung der Replikas ermöglicht sogar eine Datenwiederherstellung, wenn ganze Rechenzentren versagen.

  \item \textbf{Verfügbarkeit:} Verfügbarkeit ist eine Kernkompetenz eines DFS. Wie alle anderen BASE basierten Systeme, lockern DFS die Konsistenz, um Anfragen zu beantworten, auch wenn Teile des Systems unerreichbar sind.

  \item \textbf{Kosteneffizienz:} DFS sind entwickelt worden um auf Standard-Hardware zu laufen. Die einfache horizontale Skalierung hält die Betriebskosten niedrig, auch für große Systeme \cite{rdbmssuck}.

  \item \textbf{Leistung:} Viele Lesezugriffe auf ein DFS benötigen nur einen einzigen Zugriff auf die Festplatte. Informationen zu den Dateien und wo sie sich befinden werden in der Regel im Hauptspeicher vorgehalten. Replikas von Dateien und Chunks ermöglichen den Client den Zugriff auf die im am nächsten liegende Datei sowie eine Lastverteilung. 
\end{itemize}

Diese Punkte erfüllen DFS nahezu mühelos. Das Gewährleisten eines konsistenten Zustands bringt ein DFS hingegen ins Schwitzen. Bevor ein Zahlungsverkehr autorisiert wird, sollte sichergestellt sein, dass der Schuldner über die benötigten Zahlungsmittel verfügt. Für den Halter eines Girokontos ist es auch von großer Bedeutung, dass der in einer Webanwendung angezeigte Kontostand dem tatsächlichen entspricht. Auch die geringe Datenmenge, die bei einem einzelnen Zahlungsverkehr gespeichert werden muss ist nicht optimal. Informationen zu Gläubiger, Schuldner, Betrag, Währung und Beschreibung des Zahlungsverkehrs kann in wenigen hundert Bytes abgebildet werden. DFS sind in der Regel auf die Speicherung größerer Dateien ausgelegt. Der Artikel \textit{An optimized approach for storing and accessing small files on cloud storage} zeigt an dem HDFS die Probleme der Verwaltung vieler kleiner Dateien auf \cite{hdfsSmallFiles}. Das HDFS wurde stark von dem GFS inspiriert und unterscheidet sich nur geringfügig \cite{hdfsGfs}. Für jede Datei wird ein eigener Eintrag im Master erstellt. Dieser soll für den effizienten Zugriff immer im Hauptspeicher liegen. Viele kleine Dateien benötigten demnach sehr viel mehr Hauptspeicher des Masters, als eine sehr große Datei mit der gleichen Menge an Inhalt. Auch die Beantwortung von Lesezugriffen auf viele kleine Dateien kann die Leistung eines DFS in die Knie zwingen. Abbildung \ref{hdfsSmallFiles} zeigt, wie lange das Herunterladen von 320 Megabyte dauert, je nachdem auf wie viele Dateien die Datenmenge verteilt ist. 5120 64 Kilobyte große Dateien herunterzuladen dauert vier und ein viertel mal so lange wie der Download von 40 8 Megabyte große Dateien. Diese Verzögerung kommt vor allem durch die für jede Datei benötigte Anfrage an den Master \cite{hdfsSmallFiles}. Das Ablegen vieler kleiner Dateien benötigt demnach eine gesonderte Behandlung.



\begin{filecontents}{date5.dat}
date  value
1   15
2   9
3   6
4   5
5   4
6   3.8
7   3.7
8   3.6
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
xtick=data,
xmin=0, xmax=9,
ymin=0, ymax=16,
xticklabels={64KB,128KB,256KB,512KB,1MB,2MB,4MB,8MB},
xticklabel style = {font=\tiny},
xlabel={Dateigröße},
ylabel={Zeit zum Herunterladen in Sekunden},
]
\addplot table[x=date,y=value] {date5.dat};
\end{axis}
\end{tikzpicture}
\caption[Zeit zum Herunterladen unterschiedlicher Dateigrößen in HDFS]{Zeit zum Herunterladen unterschiedlicher Dateigrößen in HDFS bei gleichem Datendurchsatz (320 MB). Nachempfunden nach \cite{hdfsSmallFiles}.}
\label{hdfsSmallFiles}
\end{center}
\end{figure}

\chapter{Konzept}
\section{Transaktionen aufgeben}
Kann man Transaktionen aufgeben? Wieso lohnt sich das? Was ist mein Ziel damit?
\section{Ausfallsicherheit}
\section{Verwendung des DFS}
Wie wird das DFS eingesetzt? Noch keine Konkrete Implementierung angeben.
\section{Aufbau der Anwendung}
RESTfull Service, Kommunikation mit DFS
\section{Erstellen einer Buchung}
Ablauf
\section{Lesen einer Buchung}
Ablauf
\section{Maßnahmen zur Skalierung}
Wegen DFS leicht skalierbar, aber es muss auch auf Locks beim Schreiben von neuen Dateien geachtet werden.


\chapter{Implementierung}
\section{SeaweedFS}
Einsatz von SeaweedFS und wieso?
Wie wird er hier konkret Konfiguriert
\section{golang}
Wieso habe ich golang verwendet? Bezug zu SeaweedFS 
\section{Schnittstelle zu SeaweedFS}
Wieso wurde eine neue Schnittstelle geschrieben?
Worauf war zu achten? Nutzung des Filers (Distributed Filer)
\section{Bibliothek zur Abbildung von Buchungen}
Einführen der fehlenden Abstraktionsschicht für die spätere Anwendung
\section{RESTful Webservice}
Implementierung einer API zum leichten Anlegen und Lesen von Buchungen. Besonderes Augenmerk auf Modularisierung.

\chapter{Evaluierung}
Mal sehen was hier später steht.

\chapter{Ausblick}
Ich bin sehr gespannt.

\begin{lstlisting}[label=lst:java,
				   language=java,
				   firstnumber=1,
				   caption=Beispiel für einen Quelltext]				   

public void foo() {				   
	// Kommentar
}
\end{lstlisting}

\chapter{Zusammenfassung}


\backmatter
%%%%%%%%%%%%%%%%%%%
%% create figure list
%%%%%%%%%%%%%%%%%%%

\listoffigures
\addcontentsline{toc}{chapter}{Verzeichnisse}			

%%%%%%%%%%%%%%%%%%%
%% create tables list
%%%%%%%%%%%%%%%%%%%
\listoftables

%%%%%%%%%%%%%%%%%%%
%% create listings list
%%%%%%%%%%%%%%%%%%%
%\lstlistoflistings
%\addcontentsline{toc}{chapter}{Listings}				

\printbibliography
\addcontentsline{toc}{chapter}{Literatur}				

%%%%%%%%%%%%%%%%%%%
%% declaration on oath
%%%%%%%%%%%%%%%%%%%

\addchap{Eidesstattliche Erklärung}

Hiermit versichere ich, dass ich die vorgelegte Bachelorarbeit selbstständig verfasst und noch nicht anderweitig zu Prüfungszwecken vorgelegt habe. Alle benutzten Quellen und Hilfsmittel sind angegeben, wörtliche und sinngemäße Zitate wurden als solche gekennzeichnet.

\vspace{20pt}
\begin{flushright}
$\overline{~~~~~~~~~~~~~~~~~\mbox{\BaAuthor, am \today}~~~~~~~~~~~~~~~~~}$
\end{flushright}
\end{document}


