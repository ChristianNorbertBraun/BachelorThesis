\documentclass[12pt,oneside,a4paper,parskip]{scrbook}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[ngerman]{babel}
\usepackage{floatflt} 
\usepackage{subfigure}
\usepackage[pdftex]{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{color}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{nicefrac}
\usepackage{pdfpages}
\usepackage{hyphenat}
\usepackage{float} 
\usepackage{pdflscape}
\usepackage{subfigure}
\usepackage{pdfpages}  
\usepackage[verbose]{placeins} 
\usepackage[nouppercase,headsepline,plainfootsepline]{scrpage2}
\usepackage{listings}	
\usepackage{xcolor}			
\usepackage{color}			
\usepackage{caption}		
\usepackage{subfigure}			
\usepackage{epstopdf}		
\usepackage{longtable}  
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}
\usetikzlibrary{pgfplots.dateplot}
\usepackage{pgfplotstable}
\usepackage{filecontents}
\usepackage{setspace}
\usepackage[nolist]{acronym}
\usepackage{booktabs}
\usepackage[style=numeric]{biblatex}
%\bibliography{literatur2}
\addbibresource{literatur.bib}



\hyphenation{Dritt-an-bie-tern}
\hyphenation{Dritt-an-bie-ter}
\hyphenation{Da-tei-na-men}

%%%%%%%%%%%%%%%%%%%
%% definitions
%%%%%%%%%%%%%%%%%%%
\def\BaAuthor{Christian Norbert Braun}
\def\BaTitle{Einsatz eines Distributed File Systems zur Skalierung eines Banking-Buchungssystems}
\def\BaSupervisorOne{Prof.\ Dr.\ Steffen Heinzl}
\def\BaSupervisorTwo{Prof.\ Dr.\ Peter Braun}
\def\BaDeadline{31.03.2017}

\hypersetup{
pdfauthor={\BaAuthor},
pdftitle={\BaTitle},
pdfsubject={Subject},
pdfkeywords={Keywords}
}

%%%%%%%%%%%%%%%%%%%
%% configs to include
%%%%%%%%%%%%%%%%%%%
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{black}{rgb}{0,0,0}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\definecolor{pblue}{rgb}{0.13,0.13,1}
\definecolor{pgreen}{rgb}{0,0.5,0}
\definecolor{pred}{rgb}{0.9,0,0}
\definecolor{pgrey}{rgb}{0.46,0.45,0.48}

\lstset{
  basicstyle=\ttfamily\color{black},
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape
  linewidth=\textwidth
}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

%\lstset{language=xml,
%  morestring=[b]",
%  morestring=[s]{>}{<},
%  morecomment=[s]{<?}{?>},
%  stringstyle=\color{black},
%  numbers=left,
%  numberstyle=\scriptsize,
%  stepnumber=1,
%  numbersep=8pt,
%  identifierstyle=\color{darkblue},
%  keywordstyle=\color{cyan},
%  backgroundcolor=\color{background},
%  morekeywords={xmlns,version,type}% list your attributes here
%}

\lstset{language=Java,
  showspaces=false,
  showtabs=false,
  tabsize=4,
  breaklines=true,
  keepspaces=true,      
  numbers=left,
  numberstyle=\scriptsize,
  stepnumber=1,
  numbersep=8pt,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{pgreen},
  keywordstyle=\color{pblue},
  stringstyle=\color{pred},
  basicstyle=\ttfamily,
  backgroundcolor=\color{background},
%  moredelim=[il][\textcolor{pgrey}]{$$},
%  moredelim=[is][\textcolor{pgrey}]{\%\%}{\%\%}
}




\begin{document}

\acrodefplural{dfs}[DFS]{Distributed File Systems}

\begin{acronym}
  \acro{dfs}[DFS]{Distributed File System}
  \acro{hdfs}[HDFS]{Hadoop Distributed File System}
  \acro{api}[API]{Application Programming Interface}
  \acro{bbs}[Buchungssystem]{Banking Buchungssystem}
\end{acronym}


%%%%%%%%%%%%%%%%%%%
%% Titelseite
%%%%%%%%%%%%%%%%%%%


\frontmatter
\titlehead{%  {\centering Seitenkopf}
  {Hochschule für angewandte Wissenschaften Würzburg-Schweinfurt\\
   Fakultät Informatik und Wirtschaftsinformatik}}
\subject{Bachelorarbeit}
\title{\BaTitle\\[15mm]}
\subtitle{\normalsize{vorgelegt an der Hochschule f\"{u}r angewandte Wissenschaften W\"{u}rzburg-Schweinfurt in der Fakult\"{a}t Informatik und Wirtschaftsinformatik zum Abschluss eines Studiums im Studiengang Informatik}}
\author{\BaAuthor}
\date{\normalsize{Eingereicht am: \BaDeadline}}
\publishers{
  \normalsize{Erstpr\"{u}fer: \BaSupervisorOne}\\
  \normalsize{Zweitpr\"{u}fer: \BaSupervisorTwo}\\
}

%\uppertitleback{ }
%\lowertitleback{ }

\maketitle


%%%%%%%%%%%%%%%%%%%
%% abstract
%%%%%%%%%%%%%%%%%%%

\section*{Zusammenfassung}
\addcontentsline{toc}{chapter}{Zusammenfassung}

TODO

\section*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

TODO

\newpage
\chapter*{Danksagung}
\addcontentsline{toc}{chapter}{Danksagung}
Danke an FH und adorsys. An Francis und an Prof.Heinzl. Eventuell auch an Korrekturleser.

%%%%%%%%%%%%%%%%%%%
%% Inhaltsverzeichnis
%%%%%%%%%%%%%%%%%%%
\tableofcontents										



%%%%%%%%%%%%%%%%%%%
%% Main part of the thesis
%%%%%%%%%%%%%%%%%%%
\mainmatter


\chapter{Einführung}\label{ch:intro}
Banken sind Big Data. Jeden Tag fließen unzählige Zahlungsprozesse und Kundendaten durch die Systeme deutscher und internationaler Banken. Alleine im Jahr 2012 schätzte man die Menge der gespeicherten Daten auf etwa 1,9 Petabyte pro Bank \cite{datanami}. Ein Ende des Datenwachstums ist nicht in Sicht. Durch den Einsatz von Mobile- und Online-Banking steigt nicht nur die Anzahl der Vertriebswege, sondern auch der ausgeführten Transaktionen \cite{DBBigData}. Kunden rufen mobil ihren Kontostand ab, überweisen online ihre Rechnungen und bezahlen den Einkauf im Shoppingcenter mit der Kreditkarte. Dieses vertriebswegübergreifende Nutzen der Bankprodukte hat nicht nur einen Anstieg der Lese- und Schreibzugriffe auf die Systeme der Banken zur Folge, sondern fordert auch auf allen Vertriebswegen eine ähnlich gutes Nutzererlebnis \cite{bankwirtschaft}.

\section{Motivation}
Die weitläufig eingesetzten Kernbankensysteme sind veraltet und haben Probleme den Anforderungen und der Unmenge an Daten Herr zu werden. Ein Ausbau wäre möglich, ist jedoch aufgrund der benötigten Hard- und Software teuer oder liefert keine langfristige Skalierbarkeit und Ausfallsicherheit \cite{herzKernbankensystem}. Auf dieses Problem stießen vor den Banken schon Firmen wie Google, Facebook oder Yahoo. Die Lösung war in allen drei Fällen der Einsatz eines \acp{dfs}. So entwickelte Google das Google File System \cite{GFS} zur Skalierung ihrer Websuche, Facebook Haystack \cite{haystack} zum Speichern und Lesen von Bildern und Yahoo das \ac{hdfs} \cite{hdfs}. Diese Systeme laufen auf Standard-Hardware und sind daher einfach und günstig skalierbar. Außerdem überzeugen sie auch durch eine hohe Ausfallsicherheit und Verfügbarkeit. Auch Banken könnten durch die Möglichkeiten eines \acp{dfs} profitieren. Was bei Google und Co. funktioniert, birgt auch für Banken eine Chance langfristig mit den aufstrebenden FinTechs zu konkurrieren und das volle Potential ihrer Daten auszunutzen s\cite{wiki:fintech}.

\section{Zielsetzung}

Banken führen Änderungen der IT-Struktur in der Regel erst dann durch, wenn die zu übernehmende Technologie lang erprobt ist und sich als zuverlässig erwiesen hat. Doch der Wandel der Kunden im Umgang mit den Bankprodukten und die wachsende Datenmenge zwingt die Finanzbranche zu einem Umdenken \cite{bigdataBigStorage}. Im Rahmen dieser Arbeit sollen die Auswirkungen erarbeitet werden, die der Einsatz eines \acp{dfs} als Persistenzschicht eines Banking-Buchungssystems (Buchungssystem) bewirken kann. Da nicht alle Prozesse, die ein Buchungssystem abbildet, die gleichen Anforderungen haben, gilt es diejenigen herauszufinden, welche durch ein \ac{dfs} realisierbar sind. Insbesondere sollen die Möglichkeiten einer verbesserten Skalierbarkeit und Ausfallsicherheit diskutiert werden. Aktuelle Buchungssysteme sind behäbig und der Unterhalt für die Banken teuer \cite{bankingsCosts}. Durch ein auf Standard-Hardware optimiertes und leicht skalierbares System könnten Ressourcen akquiriert werden, wenn sie wirklich gebraucht werden, und abgeschaltet werden, wenn sie nicht mehr nötig sind. So sollen die Kosten, ein Buchungssystem zu betreiben, verringert werden. Im Idealfall profitieren davon nicht nur die Banken, sondern auch deren Kunden. Zusätzlich zu den wirtschaftlichen Verbesserungen sollen auch die Entwicklungen in der Ausfallsicherheit und Verfügbarkeit in dieser Arbeit kenntlich gemacht werden.

\section{Umfeld}
Unterstützend und beratend bei der Entwicklung dieser Arbeit tritt das IT-Consulting-Unternehmen adorsys GmbH \& Co. KG auf. adorsys mit ihrem Hauptsitz in Nürnberg entwickelt mittlerweile seit mehr als 10 Jahren individuelle Software für Banken und Versicherungen. Zu den Kunden zählen neben der Teambank auch ERGO Direkt und Schwäbisch Hall. Auch die Entwicklung eines Open-Source-Kernbankensystems durch adorsys war zwischenzeitlich geplant. Alles in allem ist adorsys ein Partner mit Expertise im Finanzsektor und bei der Architektur von komplexen Systemen.

\section{Aufbau der Arbeit}
Im folgenden Kapitel wird zunächst genauer auf die Vorgehensweise der Recherche und Entwicklung der Arbeit eingegangen. So werden benötigte Metriken zum Messen der Performanz, Ausfallsicherheit und Skalierbarkeit erarbeitet. Außerdem wird der Rahmen der beispielhaften Implementierung und Bewertung der Lösung weiter abgesteckt.

Im Kapitel, \nameref{buchungssystem}, werden die grundlegenden Bestandteile und Aufgaben eines Buchungssystems erläutert. Besonders die Probleme und Entwicklungschancen sollen analysiert und ausgearbeitet werden.

Im darauf folgenden Kapitel soll auf Basis der vorher erarbeiteten Probleme die Funktionsweise eines \acp{dfs} näher erläutert und die Vor- und Nachteile analysiert werden. Auch auf typische Anwendungsgebiete und Grenzen wird näher eingegangen.

Nachdem nun die Begriffe des Buchungssystems und \acp{dfs} grob abgesteckt sind, können diese im Konzept-Kapitel verheiratet werden. Hier werden die Vorgänge zum Lesen und Schreiben von Buchungen sowie die Bedeutung von Transaktionen in einem Buchunugssystems betrachtet. Außerdem werden zusätzliche Maßnahmen zur Skalierung und Ausfallsicherheit besprochen.

Das Kapitel zur Implementierung beschreibt die schrittweise Umsetzung des Konzepts und die dafür benötigten Technologien. Auch die konkrete Auswahl der Programmiersprache und des \acp{dfs} wird hier getätigt.

Im Kapitel zur Evaluierung wird die Zielsetzung der Arbeit mit den Funktionen der Testimplementierung verglichen und das Ergebnis bewertet. Das letzte Kapitel zeigt Möglichkeiten und Erweiterungen der Lösung auf.

\chapter{Vorgehensweise}
Die in der Einführung beschriebene Zielsetzung kann sich leider nur auf sehr wenige wissenschaftliche Quellen stützen. Gerade die benötigte Information zu den Banken ist rar gesät und kann meist nur aus Artikeln von News-Seiten oder Blogs entnommen werden. Angaben zur Datenmenge von Banken oder den Betriebskosten eines Kernbankensystems können daher nur geschätzt werden. Die Entwicklung des Konzepts und die darauf folgende Bewertung begründet sich also mehr auf Analysen und Annahmen als auf empirisch bewiesene Tatsachen. Alle technologiebezogenen Annahmen werden jedoch durch wissenschaftliche Arbeiten, welche sich mit den Grenzen und Möglichkeiten eben dieser Technologie beschäftigen, untermauert.

\section{Analyse der Ist-Situation}
Bevor eine Bewertung der Ist-Situation in irgendeiner Form durchgeführt werden kann, muss zunächst die Bedeutung eines Buchungssystems innerhalb des Kernbankensystems und der Bank verstanden werden. Dabei gilt es, nicht nur die technische Architektur herauszuarbeiten, sondern auch die abzubildenden Prozesse zu erfassen. Dies soll vor allem im Hinblick auf die Anforderungen geschehen, welche sich durch die neuen Vertriebswege wie Online- und Mobile-Banking ergeben haben. Wie sieht der Ablauf zum Erstellen einer Buchung aus? Was mussten Buchungssysteme zur Zeit ihrer Entstehung und was müssen sie heute leisten? Entstehen überhaupt Probleme durch die zunehmende Anzahl an Anfragen an die Systeme der Banken? Welche Aufgaben haben, welche Teile des Buchungssystems? Durch Fragen wie diese sollen die minimalen Anforderungen eines heutigen Buchungssystems erarbeitet und priorisiert werden. Außerdem soll erforscht werden, zu welchen Bedingungen diese Anforderungen von den aktuellen Systemen abgebildet werden und wo die eingesetzte Persistenzschicht die Möglichkeiten des Systems ausbremst.

\section{Einsatzbereiche eines DFS}
Nachdem die Bedeutung des Buchungssystems bekannt ist, werden die Bestandteile herangezogen, bei denen die aktuell eingesetzte Persistenzschicht am schlechtesten auf die Anforderungen passt. Schlecht bedeutet hierbei, dass entweder zu viel oder zu wenig Funktionalität bereit gestellt wird oder, dass der Einsatz der Technologie massive Nachteile mit sich bringt. Danach soll die Funktionsweise mehrerer \ac{dfs} verstanden und deren Möglichkeiten mit den minimal benötigten Anforderungen abgeglichen werden. So lässt sich herausarbeiten, ob ein \ac{dfs} allein überhaupt zur Realisierung eines Bestandteiles des Buchungssystems geeignet ist. Zusätzlich sollen die Vor- und Nachteile eines \ac{dfs} mit den Vor- und Nachteilen der vorher eingesetzten Persistenzschicht verglichen werden. Je nachdem, welche Technologie hier besser abschneidet, lässt sich absehen, ob sich eine Investition in ein \ac{dfs} lohnt oder nicht.

\section{Entwicklung des Konzepts}
An dieser Stelle ist bereits klar, wie ein Buchungssystem funktioniert und welche Anforderungen es zu erfüllen hat. Außerdem sind die Teile des Systems bekannt, welche Raum für Verbesserungen durch ein \ac{dfs} bieten. Auch die generelle Funktionalität von einem \ac{dfs} wurde erarbeitet und herausgefunden, welches der gegenwärtigen \ac{dfs} am besten zur Realisierung einzelner Bestandteile eines Buchungssystems geeignet ist. Dieses Wissen soll als Grundlage dienen, ein Konzept zu entwickeln, das noch detaillierter auf die Bedürfnisse des Buchungssystems eingeht. Dabei dient die Funktionsweise des am besten passenden \acp{dfs} als Richtlinie, welches aber noch speziell für den Anwendungsfall eines Banking-Buchungssystems angepasst werden soll. Ob bereits ein \ac{dfs} existiert, dass exakt so funktioniert, spielt hier zunächst keine Rolle. Es soll vielmehr ein System entwickelt werden, das neben seinen Aufgaben auch die wirtschaftlichen, leistungsbezogenen und auf die Skalierung bezogenen Anforderungen bestmöglich erfüllt. Auf Basis dieses Systems soll das Erstellen und Lesen einer Buchung detailliert erklärt werden. Dieser Prozess kann dann den entsprechenden Schritten eines konventionellen Buchungssystems gegenüber gestellt werden.

\section{Beispielhafte Implementierung}
Um das Ergebnis des Konzepts hinsichtlich der Performanz und Umsetzbarkeit zu testen, soll es beispielhaft implementiert werden.
Dazu muss die Programmiersprache für das Backend als auch ein konkretes \ac{dfs} ausgewählt werden. Falls es kein \ac{dfs} geben sollte, welches den aus dem Konzept hevorgehenden Anforderungen gerecht wird, muss auf das am besten passende ausgewichen werden. Die tatsächliche Entwicklung eines \acp{dfs} ist äußerst komplex und sprengt bei weiten den Rahmen dieser Arbeit. Das führt dazu, dass das Konzept gegebenenfalls auf ein bereits existierendes \ac{dfs} angepasst werden muss. Des Weiteren steht die Entwicklung einer Schnittstelle zum \acp{dfs} und des Backends an. Alle Schritte sollen eine ausreichende Testabdeckung vorweisen und entsprechend dokumentiert werden.

\section{Bewertung der Lösung}
Die Bewertung der im Rahmen dieser Arbeit entwickelten Lösung kann nur auf Basis der tatsächlichen Implementierung erfolgen. Die Auswirkung der Teile des Konzepts, welche sich technisch nicht umsetzen lassen, können nur erahnt werden und sind deshalb nicht zu berücksichtigen. Zur Bewertung der entwickelten Lösung werden die fünf folgenden Kriterien herangezogen.

\begin{enumerate}
  \item \textbf{Integrierbarkeit:} Beschreibt, wie hoch der Aufwand geschätzt wird, das System in ein Buchungssystem zu integrieren.
  \item \textbf{Skalierbarkeit:} Wie gut lässt sich das System skalieren? Welche Teile können zum Flaschenhals werden? Zusätzlich zu diesen Fragen soll es auch darum gehen, ob eine lastbezogene Skalierung möglich ist und wenn ja, wie viel Aufwand dazu betrieben werden muss.
  \item \textbf{Performanz:} Die Leistung soll durch die Anzahl der beantworteten Anfragen bei maximaler Auslastung bewertet werden. Zusätzlich muss das System nach dem Performanz-Test einen korrekten Stand aufweisen.
  \item \textbf{Ausfallsicherheit:} Bei der Ausfallsicherheit soll betrachtet werden, wie viele Teile des Systems versagen können, bevor es zu einem inkonsistenten Zustand oder einem Ausfall führt.
  \item \textbf{Wirtschaftlichkeit:} Zu guter Letzt soll geschätzt werden was der Betrieb des Systems kostet. Dazu werden sowohl die erforderliche Hardware und deren Kosten betrachtet, als auch der durchschnittliche Preis, um das System bei einem externen Anbieter zu hosten. Auch die Kosten für eine mögliche Skalierung und die Gewährleistung einer annehmbaren Ausfallsicherheit werden berücksichtigt.
\end{enumerate}

Der Einfluss eines \acp{dfs} auf ein Buchungssystem soll aus diesen Kriterien hervorgehen.
Daher sollen auch aktuell bei Banken eingesetzte Buchungssysteme anhand dieser Kriterien bewertet werden. Im direkten Vergleich zeigt sich am besten, was der Einsatz eines \acp{dfs} für ein Buchungssystem leistet.

\chapter{Wesen und Probleme eines Buchungssystems}
\label{bookingSystem}
Die Anwendungssysteme der Banken sind alt und über lange Zeit gewachsen. Wo 1970 die ersten spartenbezogenen Programme nur bei Kredit-, Einlagegeschäften und
Wertpapierabwicklung unterstützten, bilden jetzt die Systeme der Banken nahezu alle Geschäftsprozesse ab \cite[16]{ITidF}. Um die Bedeutung des Buchungssystems innerhalb dieser komplexen Anwendungsstruktur zu verstehen, soll im Folgenden zunächst auf die Entstehung und Architektur der Anwendungsysteme von Banken eingegangen werden. Danach werden die Aufgaben und Bestandteile eines Buchungssystems erarbeitet und die daraus entstehenden Anforderungen aufgezeigt. Im letzten Schritt sollen die Probleme der aktuellen Systeme besonders hinsichtlich der neuen Vertriebswege behandelt werden. 
\label{buchungssystem}

\section{IT-Systeme der Banken}
Seit den ersten Anwendungssystemen der Banken stand das Konto und die Kontoführung im Zentrum. Das Hinzufügen neuer Funktionen und Anforderungen erfolgte über das anhängen neuer Module an eben diesen Konto bezogenen Kern. Die so angedockten Abwicklungssysteme waren zum Beispiel für die Durchführung des Inlands- und Auslandszahlungsverkehr, des Kreditwesens oder der Einlagen verantwortlich. Nachdem die Banken auch Unterstützung bei den Geschäftsprozessen forderten, legte sich um die Abwicklungssysteme ein weiterer Ring. Dieser stellte Dienste für Kundenberater und Sachbearbeiter der Banken zur Verfügung \cite[18-20]{ITidF}\cite{SuPdIiB}. Diese Vorgehensweise führte zu einer Silo- oder auch Spartenarchitektur, die in Abbildung \ref{zwiebel} dargestellt wird. Systeme dieser Art wurden ursprünglich von den Banken selbst entwickelt. Die stark heterogenen Teilsysteme und die unentwirrbaren Abhängigkeiten zwischen ihnen stellten sich jedoch als nicht weiter tragbar und wartbar heraus \cite{bankEnzy}\cite{SuPdIiB}\cite[52]{ITidF}. Besserung versprach der Einsatz von hoch standardisierten Teilsystemen oder Gesamtbankenlösungen von Drittanbietern. Die Standardisierung erlaubt über Parameter eine eingeschränkte Anpassung der Systeme an die Bedürfnisse der Banken. So können die benötigten Systeme von unterschiedlichen Anbietern eingekauft und verbunden werden. Im Gegensatz dazu steht die Gesamtbankenlösung, die weiterhin versucht, allen Anforderungen und Funktionen einer Bank gerecht zu werden. Diese Art von Systemen ist aber aus ähnlichen Gründen, wie die ursprüngliche Anwendungsstruktur der Banken nicht besonders erfolgreich \cite[S. 56 ff.]{ITidF}. 

\begin{figure}
   \makebox[\textwidth]{\includegraphics[width=\paperwidth]{img/3/zwiebelstrktur.png}}
  \caption[Historische Anwendungsstruktur von Banken]{Historisch gewachsene Anwendungsstruktur von Banken. Entnommen aus \cite{SuPdIiB}}
  \label{zwiebel}
\end{figure}

Heutzutage sind bei Genossenschaftsbanken häufig die Produkte von Fiducia GAD \cite{fiducia}, bei kleineren Privatbanken Systeme von FIS Kordoba \cite{kordoba} oder SAP \cite{SAP} und bei den Sparkassen die Lösung der Finanz Informatik One System Plus (OSPlus) \cite{finanzinformatik} im Einsatz. Die Entwicklung eigener Systeme können sich nur noch wenige große Banken wie die Deutsche Bank erlauben. Bis auf die Lösung der Finanz Informatik OSPlus und FIS Kordoba handelt es sich in Deutschland in der Regel um Systeme, die keine Gesamtbankenlösung bieten, sondern mehr oder weniger Teile der Geschäfts- und Kontoprozesse der Banken abdecken und mit anderen Systemen kombiniert werden können \cite{einfuehrungKernbanksystem}\cite[56-58]{ITidF}. Auch wenn die aktuellen Systeme deutlich besser standardisiert und dadurch wartbarer als früher sind, ist die Struktur immer noch ähnlich oder baut im Kern sogar noch auf den Ursprüngen der Banken-IT auf. Die Architektur lässt sich in vier Schichten aufteilen, die von oben nach unten folgendermaßen beschrieben sind \cite[104]{ITidF}:

\begin{enumerate}
\item \textbf{Visualisierungsschicht:} Die Visualisierungsschicht ist die Schnittstelle zum Benutzer und wird auf dessen Endgerät ausgeführt. Sie bezieht einerseits Daten von der Darstellungsschicht und zeigt diese ansprechend an. Andererseits leitet sie die Eingaben des Nutzers an die Darstellungsschicht weiter.
\item \textbf{Darstellungsschicht:} Diese Schicht ist verantwortlich für eine fehlerfreie Kommunikation zwischen Visualisierungsschicht und Anwendungsschicht. Daten werden von einer Schicht empfangen und in das gewünschte Format der anderen Schicht überführt.
\item \textbf{Anwendungsschicht:} Die Anwendungsschicht kümmert sich um die eigentliche Geschäftslogik für einen bestimmten Teilbereich. Sie stellt Schnittstellen für Dienste wie teilweise auch die neuen Vertriebswege Online- und Mobile-Banking zur Verfügung und führt aufwendige Datenmanipulationen auf der Datenschicht durch. Aufgrund der teilweise sehr alten und komplexen Systeme finden sich hier auch Bereiche, die noch in COBOL oder sogar Assembler geschrieben sind.
\item \textbf{Datenhaltungsschicht:} Die Datenhaltungsschicht ist für die Verwaltung der Stammdaten sowie der von der Anwendungsschicht gesendeten und angefragten Informationen zuständig. Auch Operationen für einfache Manipulationen der Daten werden von ihr bereitgestellt. In der Finanzindustrie gelten relationale Datenbanken als Standard für die Persistenzschicht der Datenhaltungsschicht \cite[105]{ITidF}\cite{MarkstudieKernbankensysteme}
\end{enumerate}

Die Datenhaltungsschicht als Basis der Architektur entspricht je nach Aufteilung und Definition entweder allein oder in Kombination mit einzelnen Anwendungsschichten dem Buchungssystem. Ähnlich wie in Abbildung \ref{zwiebel} bildet es auch heute noch das Zentrum der IT Landschaft von Banken. Als ein solches wird es auch häufig als Kernbankensystem bezeichnet. Diese Bezeichnung verdeutlicht auch, dass die Aufgabe des Buchungssystems die Erfüllung von Kernaufgaben ist und allein nur eine begrenzte Funktionalität bereitstellt. Die Unterstützung weitergehender Bankprozesse muss durch das Aufsetzen weiterer Anwendungsschichten erfolgen \cite[58]{ITidF}. Da in der Literatur und Wirtschaft der Begriff Kernbankensystem auch häufig als eine Gesamtbankenlösung verstanden wird, wird in dieser Arbeit immer von "Buchungssystem" gesprochen, sofern es um die Kernfunktionalität von Banken geht \cite{vergleichCoreBanking}.  


\section{Das Buchungssystem und seine Aufgaben}
Das Buchungssystem steht demnach im Zentrum jedes kontenbasierten Geschäftsvorfalls der Banken \cite{bankEnzy}. Daher muss es auch alle Kernfunktionen eben dieser erfüllen und unterstützen können. Dazu gehören die Zahlungsverkehrs-, Investitions- und Kreditfunktion aber auch die Verwaltung von Kundenstammdaten sowie die grundlegende Kontoführung. Für Investitions- und Kreditfunktion können auch die Begriffe Passiv- und Aktivgeschäfte genutzt werden \cite[12, 86]{DdF}\cite{einfuehrungKernbanksystem}. Im Folgenden soll der Inhalt der einzelnen Funktionen anhand von \cite[69-88]{DdF} und \cite[91-153]{bankwirtschaft} beschrieben werden.

\begin{itemize}
  \item \textbf{Aktivgeschäfte:} Das Aktivgeschäft erhält seinen Namen, da alle Kreditgeschäfte also Forderungen an Kunden in der Bilanz auf der Aktivseite abgebildet werden. Durch das Kreditgeschäft erzielen Banken einen Großteil ihrer Zinserträge. Jedoch ist damit auch ein hohes Risikopotential verbunden. Als Aktivgeschäft zählen unter anderem alle klassischen Kreditgeschäfte.
  Zum Beispiel, der Kontokorrentkredit oder auch Dispositionskredit, der Privatpersonen eine Überziehungsmöglichkeit des Kontos einräumt. Aber auch längerfristige Kreditgeschäfte wie der Hypothekarkredit, Baukredit oder Investitionskredit gehören zu den Aktivgeschäften. Bei den Kreditgeschäften gilt in der Regel, dass von der Bank zum Kunden ein vertraglich festgelegter Betrag fließt, dessen Rückzahlung zusätzlich Zinsen vom Schuldner auf ein Konto der Bank zu erbringen ist.
  \item \textbf{Passivgeschäfte:} Als Passivgeschäft werden alle Einlagegeschäfte der Banken bezeichnet. Im Gegensatz zu den Aktivgeschäften erscheinen sie in der Bilanz auf der Passivseite. Aktiv- und Passivgeschäfte stehen in einer engen Verbindung. Häufig finanzieren Kreditinstitute ihre Aktivgeschäfte durch die Einlagen der Kunden. Zu den Passivgeschäften gehören Sichteinlagen, und Termineinlagen. Sichteinlagen sind täglich fällige Gelder. In der Regel handelt es sich hierbei um die Einzahlungen auf ein Girokonto. Ihr Zweck ist hauptsächlich der bargeldlose Zahlungsverkehr. Termineinlagen hingegen entziehen den Kunden den Zugriff auf die Einlage für einen festgelegten Zeitraum, der in der Regel auf eine Dauer von mindestens 30 Tage und maximal 5 Jahren angelegt wird. Es handelt sich also um Einlagen, welche der Kunde über einen gewissen Zeitraum nicht benötigt. Dafür werden Termineinlagen höher verzinst als Sichteinlagen. Werden Termineinlagen als Kündigungsgelder vereinbart, so gibt es keine Laufzeitfrist und die Auszahlung erfolgt nach Einreichen einer Kündigung und abgelaufener Kündigungsfrist. Bei Passivgeschäften fließt also ein festgelegter Betrag vom Kunden zur Bank, welche über die Einlage in einem festgelegten Rahmen verfügen darf.
  \item \textbf{Zahlungsverkehr:} Der Zahlungsverkehr im Bankengeschäft beschreibt die bare und unbare Übertragung von Zahlungsmitteln im Inland und Ausland. Zur Übertragung können Überweisungen, Kartenzahlungen oder Lastschriften genutzt werden. Die besondere Bedeutung des Zahlungsverkehrs liegt in der Unumgänglichkeit für die Bankkunden. Auch wenn Kunden keine Kredite nehmen oder keine Einlagen tätigen, so müssen dennoch immer Zahlungen über die Systeme der Banken ausgeführt werden. Gerade die zunehmende Digitalisierung wird einen großen Einfluss auf die Entwicklung des bargeldlosen Zahlungsverkehrs haben. 
\end{itemize}

Das Buchungssystem muss all diese Inhalte führen können und Kontobewegungen nachvollziehbar ablegen. Im Folgenden werden alle Kontobewegungen als Buchungen bezeichnet. Es dient also neben den Anwendungssystemen auch den gesetzlichen Rechnungsabschlüssen und Bilanzen als Grundlage \cite{bankEnzy}\cite{MarkstudieKernbankensysteme}. In der Regel wird diese Nachvollziehbarkeit durch die doppelte Buchführung gewährleistet. Das heißt, dass eine Buchung immer in den Konten beider beteiligten Parteien auftaucht. In Summe müssen alle Kredit- und Debitbeträge Null ergeben \cite{accounting}.

\section{Anforderungen}
Um die oben genannten Aufgaben jetzt und auch noch in Zukunft zuverlässig zu erfüllen, müssen Buchungssysteme gewissen Anforderungen gerecht werden. Diese lassen sich in interne und externe Anforderungen unterteilen. Interne Anforderungen beschreiben die Bedingungen, denen eine Bank nachkommen muss, um die geschäftsinternen Prozesse reibungslos und wirtschaftlich durchzuführen. Beispiele dafür sind eine wohldefinierte Schnittstelle des Buchungssystems, an der bei Bedarf neue Geschäftsprozesse angedockt oder eine detaillierte Analyse des Kundenverhaltens durchgeführt werden kann. Auch die Reduktion der IT-Wartungskosten gilt als eine interne Anforderung an Buchungssysteme. Externe Anforderungen beziehen sich auf Bedingungen, die Banken von außen auferlegt werden. Dazu gehören gesetzliche Richtlinien, sowie Veränderungen von Angebot und Nachfrage im Markt \cite{capgemini}. So wird durch den Kunden aber auch den Gesetzgeber das zuverlässige Ablegen einer jeden Kontobewegung beziehungsweise Buchung gefordert. Das Buchungssystem muss folglich immer erreichbar sein und eine hohe Ausfallsicherheit gewährleisten. Da aber neben der Ausfallsicherheit auch die Geschwindigkeit der Abarbeitung bankfachlicher Prozesse relevant ist, spielt die Performanz der Buchungssysteme auch eine tragende Rolle. Banken legen deswegen häufig ihre Buchungssysteme redundant an und verbinden diese über ein sicheres und performantes Datennetz \cite{bankEnzy}\cite[97-99]{ITidF}. Die so nebeneinander gestellten Systeme erlauben auch eine Lastverteilung bei der Bearbeitung mehrerer Anfragen. Gerade in Hinblick auf die neuen Vertriebswege und den steigenden bargeldlosen Zahlungsverkehr ist ein System, das sich skalieren lässt, unabdingbar \cite{bankEnzy}\cite{capgemini}. Die Abbildungen \ref{online-banking} und \ref{bargeldlos} zeigen dieses Wachstum auf.


\begin{filecontents}{date.dat}
date       value
2006-01-01  34
2007-01-01  34
2008-01-01  36
2009-01-01  37
2010-01-01  35
2011-01-01  44
2012-01-01  44
2013-01-01  45
2014-01-01  54
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahr},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anteil der Nutzer in Prozent},
date ZERO=2005-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date.dat};
\end{axis}
\end{tikzpicture}
\caption[Nutzer von Online-Banking in Deutschland]{Nutzer von Online-Banking in Deutschland. Nachempfunden nach \cite{onlinebanking}.}
\label{online-banking}
\end{center}
\end{figure}


\begin{filecontents}{date2.dat}
date       value
2011-01-01  90.61 
2012-01-01  94.38
2013-01-01  99.52
2014-01-01  103.34
2015-01-01  112.13
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahr},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anzahl der Transaktionen in Mrd.},
date ZERO=20010-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date2.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionen im bargeldlosen Zahlungsverkehr in der EU]{Anzahl der Transaktionen im bargeldlosen Zahlungsverkehr in der EU. Nachempfunden nach \cite{bargeldlos}.}
\label{bargeldlos}
\end{center}
\end{figure}


Die Folge dieser Entwicklung ist nicht nur eine steigende Anzahl an Zugriffen auf die Systeme der Banken, sondern auch eine stetig zunehmende Datenmenge. Nicht umsonst wurde in der Einleitung dieser Arbeit die Verbindung zwischen Big Data und Banken geknüpft. Wo ursprünglich das alleinige Speichern aller Buchungsdaten ausreichend war, fordern neue Dienste und Richtlinien nun auch den schnellen und gezielten Zugriff auf Kundendaten, Konten und Buchungen \cite{bigdataBigStorage}. In diesem Zuge ist die Second Payment Service Directive (PSD2) zu nennen. Dabei handelt es sich um eine EU-Richtlinie, die im Januar 2016 in Kraft getreten ist. Demnach sind Banken verpflichtet, auf Wunsch des Kunden alle seine Konten und durchgeführten Buchungen einem Drittanbieter über eine sichere Verbindung zur Verfügung zu stellen. Auch das Autorisieren von Bezahlungen muss über diese Verbindung möglich sein. Die Europäische Aufsichtsbehörde hat am 23. Februar 2017 die Regulatory Technical Standards (RTS) veröffentlicht, welche festlegen sollen, wie diese Verbindung realisiert werden muss und wie die Schnittstelle für Drittanbieter auszusehen hat \cite{rts}. Jetzt haben die Banken bis zum vierten Quartal 2018 Zeit die Bestimmung in ihre Systeme zu integrieren \cite{eu-psd2}\cite{psd2dk}. Die PSD2 stellt eine enorme Herausforderung für die Buchungssysteme der Banken dar. Wo vorher nur Zugriffe von bankeigenen Systemen möglich waren können jetzt beliebige Drittanbieter Kontodaten und Kontobewegungen abfragen. Als Konsequenz können deutlich steigende Lese- und moderat steigende Schreibzugriffe erwartet werden. Die Vorteile, die sich für den Endnutzer ergeben, stellen die Banken hingegen vor ein Problem. Für die Bankkunden sind alle Bankprodukte bankenübergreifend in einer einzigen Anwendung verfügbar. Drittanbieter können Kauf- und Sparverhalten der Kunden analysieren und sinnvolle Hinweise zur Kontoführung geben. Zusätzlich sind Kunden nicht mehr an die eine Schnittstelle ihrer Bank gebunden, um Buchungen durchzuführen, sondern können ihrer Finanzen mit der besten am Markt erhältlichen Anwendung verwalten. Die Banken müssen jedoch zum einen mit der größeren Belastung der Buchungssysteme umgehen und zum anderen ihre Anwendungen und Infrastruktur umstrukturieren, um trotz Konkurrenz die Kunden auf ihrer Plattform halten zu können \cite{psd2vid}.
Die Anforderungen an ein Buchungssystem belaufen sich demnach auf eine wohldefinierte Schnittstelle sowohl für bankeigene Prozesse als auch für Drittanbieter, ein gutes Kosten-Nutzen-Verhältnis der Systeme sowie auf Skalierbarkeit und Ausfallsicherheit, die den Forderungen durch PSD2 und neuen Vertriebswegen wie Mobile- und Online-Banking gerecht wird.

\section{Probleme}
Wie bereits erwähnt, sind die IT-Landschaften der Banken und insbesondere die Buchungssysteme alt und über einen langen Zeitraum gewachsen. Neue Funktionalität wurde über das Hinzufügen neuer Schichten realisiert, wobei der Kern gleich blieb. Da aber zur Zeit der Entwicklung der Buchungssysteme die heutigen Anforderungen noch nicht absehbar waren, können diese auch nicht zufriedenstellend erfüllt werden \cite[23-27]{ITidF}\cite{bankEnzy}. Auch die Banken selbst sind sich bewusst, dass etwas getan werden muss, um weiter auf dem Markt relevant zu bleiben \cite{capgemini}.
Besonders die Skalierung der Systeme könnte sich für die Banken als Problem herausstellen. Durch die PSD2 und die neuen Vertriebswege werden eine steigende Datenmenge sowie Lese- und Schreibzugriffe eine hohe Belastung der Buchungssysteme zur Folge haben \cite{bigdataBigStorage}. Experten gehen davon aus, dass sich die Menge der Daten bis 2020 versiebenfacht \cite{versiebenfacht}. Buchungssysteme können aber nicht beliebig skaliert werden, um die Anforderungen zu meistern. Die eingesetzten relationalen Datenbanken scheinen für viele Aufgaben essentiell, sind aber durch ihre Architektur und grundlegenden Konzepte nicht für das Speichern und Verwalten beliebig vieler Einträge geeignet \cite{rdbmsBigData}. Die Grundlage für relationale Datenbanken bildet das ACID-Prinzip (Atomicity, Consistency, Isolation, Durability) Prinzip. Atomarität, Konsistenzerhaltung, Isolation und Dauerhaftigkeit beschreiben eine mächtige Möglichkeit, Transaktionen innerhalb eines Systems abzubilden. Auch bei nebenläufigen Prozessen kann so immer ein konsistenter Stand gewährleistet und im Fehlerfall wieder hergestellt werden. Diese Möglichkeiten gehen jedoch auf Kosten der Performanz. Um die Einhaltung des ACID-Prinzips zu garantieren, müssen alle an einer Transaktion beteiligten Einträge mit einem exklusiven Lock versehen werden. Bei einer stark verteilten Datenbank erfordert dieser Vorgang ein eigenes verteiltes Commit-Protokoll, auch Zwei-Phasen-Commit genannt \cite{dbarchitecture}. Alle an einem Commit beteiligten Datenbanksysteme müssen den Commit bestätigen. Denn entweder wird die Transaktion auf allen Systemen oder auf keinem ausgeführt. Ist ein Datenbanksystem nicht verfügbar, kann die Transaktion nicht durchgeführt werden. Bei zwei Datenbanksystemen mit jeweils 99,9 \% Verfügbarkeit wird diese durch die Abhängigkeit des Zwei-Phasen-Commits auf 99,8 \% Verfügbarkeit des Gesamtsystems reduziert \cite{BASE}. Wie sich der Transaktionsdurchsatz einer relationalen Datenbank bei einem steigenden Anteil an konkurrierenden Anfragen verhält ist in Abbildung, \ref{salt} zu erkennen. Hierbei wurden Transaktionen mit jeweils fünf Update-Operationen auf Tabellen mit unterschiedlich vielen Einträgen ausgeführt. Je nach Anzahl der Reihen in den Tabellen kam es so zu mehr oder weniger konkurrierenden Zugriffen \cite{salt}.
\begin{filecontents}{date3.dat}
date  value
0     70000
1     65000
2     10000
3     2000
4     300
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xtick=data,
ymin=0, ymax=80000,
scaled ticks=false,
xticklabels={0,0{,}0001,0{,}001,0{,}01,0{,}1},
xlabel={Konkurrierender Zugriff (1/\#Reihen der Tabelle)},
ylabel={Transaktionsdurchsatz (Transaktionen/Sekunde)},
]
\addplot table[x=date,y=value] {date3.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionsdurchsatz im Verhältnis zum konkurrierenden Zugriff]{Transaktionsdurchsatz im Verhältnis zum konkurrierenden Zugriff auf Ressourcen in einem ACID basierten System. Nachempfunden nach \cite{salt}.}
\label{salt}
\end{center}
\end{figure}
Der sinkende Transaktionsdurchsatz ist eine Folge der nach ACID-Prinzipien durchgeführten Transaktionen. Hierbei werden Datenbankeinträge erst wieder freigegeben, nachdem die gesamte Transaktion durchgeführt oder abgebrochen wurde. Je nachdem, wie das Locking implementiert oder welche Daten angefordert werden, kann eine Transaktion ein Lock für einzelne Tabellenzeilen, die ganze Tabelle, ganze Datenblöcke oder sogar für das ganze Datenbanksystem anfordern. Es ist also durchaus möglich, dass eine Kontotransaktion für einen Kunden auch die Einträge anderer Kunden mit beansprucht \cite{locking}\cite{dbarchitecture}.

Aber nicht nur bei rein schreibenden Transaktionen fällt die Performanz relationaler Datenbanken enorm ab. Die Abbildung \ref{salt2} zeigt den Transaktionsdurchsatz auf eine Tabelle mit 100 Einträgen bei Transaktionen, die entweder fünf Einträge verändern oder lesen, während der Anteil der lesenden Transaktionen immer weiter zunimmt. Wie zu sehen ist, führt schon ein zehnprozentiger Anteil an Schreibzugriffen zur Halbierung des Transaktionsdurchsatzes.
\begin{filecontents}{date4.dat}
date  value
0     100000
5     90000
10    50000
15    12000
20     8000
50     5000
100    1000
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
/pgf/number format/.cd,
use comma,
1000 sep = {},
xmin=0, xmax=100,
ymin=0, ymax=100000,
scaled ticks=false,
xlabel={Anzahl Schreibzugriffe in Prozent},
ylabel={Transaktionsdurchsatz (Transaktionen/Sekunde)},
]
\addplot table[x=date,y=value] {date4.dat};
\end{axis}
\end{tikzpicture}
\caption[Transaktionsdurchsatz im Verhältnis zum Anteil der Schreibzugriffe]{Transaktionsdurchsatz im Verhältnis zum Anteil der Schreibzugriffe in einem ACID basierten System. Nachempfunden nach \cite{salt}.}
\label{salt2}
\end{center}
\end{figure}

Zusätzlich ist bei einem sehr hohen Durchsatz von Transaktionen nicht zu erwarten, dass zwei Transaktionen mit ähnlichen Daten auch im Speicher immer nah zusammen liegen. Bezogen auf die kontenbezogenen Daten, kann ein hoher Durchsatz von schreibenden Transaktionen zu einer Datenfragmentierung führen. Das bedeutet, dass es unwahrscheinlich ist, dass alle Umsätze eines Kunden innerhalb der Umsatztabelle der Bank auch im Speicher nahe zusammen liegen. Auch das könnte einen Einfluss auf die Lesegeschwindigkeit haben. Relationale Datenbanken werden in der Regel vertikal skaliert, wenn überhaupt. Horizontale Skalierung ist zwar zum Beispiel über Sharding möglich, ist jedoch meist aufwendig zu implementieren und hat bei verteilten Anfragen negative Auswirkungen auf die Performanz der Datenbanksysteme. Vertikale Skalierung beschreibt die Skalierung von Systemen durch das Aufrüsten von Hardware-Komponenten, also zum Beispiel das Einsetzen eines größeren Hauptspeichers oder eines schnelleren Prozessors. Horizontale Skalierung hingegen wird durch die Verteilung der Anwendung auf mehrere Maschinen erreicht. Die Nachteile an der vertikalen Skalierung sind jedoch die sehr limitierten Möglichkeiten und die höheren Kosten im Vergleich zur horizontalen Skalierung \cite{sharding}\cite{rdbmssuck}. 

Die Kosten der IT-Systeme sind jedoch auch jetzt schon ein Problem für die Banken. Eine exakte Ermittlung der Ausgaben für die IT der Banken stellt sich aber als schwierig heraus, da die Zahlen in der Regel nicht öffentlich gemacht werden. Die Schätzungen der IT-Kosten beziehen sich daher auf die in der Gewinn- und Verlustrechnung öffentlich gemachten Aufwendungen für den Verwaltungsaufwand der Banken. Experten schätzen, dass 15 - 20 \% davon für die IT anfallen. Demnach lagen die Kosten für die Anwendungssysteme der Deutschen Bank im Jahre 2004 bei rund 2,6 Mrd. Euro und die der HypoVereinsbank bei mehr als 900 Mio. Euro. Die zehn größten Banken in Deutschland haben 2004 etwa 6,5 Mrd. Euro für die Instandhaltung und Entwicklung ihrer IT ausgegeben. 
Etwa 24 \% davon fließen alleine in die Buchungsdienste. Gerechnet auf die Girokonten ergibt das Jahresausgaben von etwa 14 Euro pro Girokonto \cite[29-39]{ITidF}. Diese Zahl lässt sich auch durch den Umsatz und die betreuten Konten von Kernbanksystem-Anbietern validieren. So betreute die Fiducia IT 2005 insgesamt 52,6 Mio. Kundenkonten und erzielte einen Jahresumsatz von etwa 728,6 Mio. Euro. Das entspricht einem Preis von 13,80 Euro pro Konto. Die Finanz Informatik erzielte im Jahr 2005 bei 58 Mio. Kundenkonten einen Umsatz von 723 Mio. Euro, was einem Kontopreis von etwa 12,50 Euro entspricht. Dabei fließen nur etwa 20 \% in die Entwicklung neuer Dienste. Die restlichen Kosten fallen für Betrieb und Instandhaltung an. Langfristig sind diese Ausgaben für Banken nicht tragbar \cite[75-91]{ITidF}\cite{SuPdIiB}\cite[41-42]{DdF}\cite{bankingsCosts}. 


\chapter{Grundlagen eines DFS}
Zu den Skalierungsproblemen und den hohen Kostenfaktor der Anwendungssysteme der Banken tragen auch die ACID-basierten Datenbanksysteme bei. Auch wenn die Funktionalität, die ACID mit sich bringt, nur für einen Bruchteil der Prozesse benötigt wird, wird so das gesamte System ausgebremst und in den Skalierungsmöglichkeiten eingeschränkt \cite{salt}.
Systeme, die hingegen die Skalierbarkeit und Verfügbarkeit gegenüber der Konsistenz priorisieren, funktionieren nach dem BASE (Basically Available, Soft State, Eventual consistency) Prinzip. Häufig weisen diese Systeme niedrigere Betriebskosten im Vergleich zu den ACID-Alternativen auf \cite{clusterBASE}.
In diesem Kapitel soll zunächst die Bedeutung von BASE geklärt werden. Ein besonderer Fokus liegt dabei auf den Unterschieden der Konsistenz zu traditionellen ACID-Systemen. Dann soll es um die Funktionsweise von DFS gehen. Diese funktionieren nach dem BASE Prinzip und haben Verfügbarkeit, Ausfallsicherheit und Kosteneffizienz zu ihrer Königsdisziplin gemacht. Zum besseren Verständnis werden zwei populäre Vertreter der DFS näher erläutert. Im letzten Teil des Kapitels werden die Vorteile und Grenzen der DFS nochmal genauer betrachtet und die Anforderungen an ein Buchungssystem mit den Möglichkeiten eines DFS abgeglichen.

\section{BASE}
ACID-Systeme sind weit verbreitet und bekannt. Sie bringen eine sehr starke Semantik, verursachen aber in verteilten Systemen hohe Komplexität und Kosten. Konsistenz ist das oberste Ziel von ACID-basierten Systemen. Die Verfügbarkeit der Systeme wird nicht garantiert. Im Gegenteil, es wird sogar bevorzugt, keine Antwort zu geben, als eine falsche \cite{clusterBASE}. Die Skalierung solcher Systeme ist schwierig. Ist ein Teilsystem nicht erreichbar, leidet die Verfügbarkeit des gesamten Systems darunter \cite{BASE}. In einer idealen Welt wären Systeme gleichermaßen skalierbar, konsistent und verfügbar. Solche Systeme kann es jedoch nach dem CAP Theorem (Consistency, Availability, Partition tolerance) nicht geben. Demnach können verteilte Systeme nur zwei von den drei Eigenschaften Konsistenz, Verfügbarkeit und Partitionstoleranz erfüllen. Da Partitionstoleranz jedoch für jede Art der Skalierung benötigt wird, kann die Wahl nur zwischen Verfügbarkeit und Konsistenz fallen. Die Vorstellung, Konsistenz gegen Verfügbarkeit zu tauschen, erscheint häufig bedenklich. Es ist aber wichtig zu verstehen, dass eine Entscheidung für zwei Eigenschaften des CAP-Theorem nie einen hundertprozentigen Ausschluss der dritten bewirkt.
Das heißt, hoch konsistente Systeme verzichten nicht komplett auf die Verfügbarkeit und hoch verfügbare Systeme können einen gewissen Grad an Konsistenz bieten \cite{cap}. Die Herausforderung ist, ein für die Anwendung möglichst passendes Verhältnis zwischen Konsistenz und Verfügbarkeit zu finden. So wie die Verfügbarkeit eines Systems in Prozent angegeben werden kann, gibt es auch Freiraum in der Gestaltung der Konsistenz. Die Autoren Paolo Viotti und Marko Vukolic unterscheiden in ihrem Artikel \textit{Consistency in Non-Transactional Distributed Storage Systems} insgesamt 50 Konsistenzarten \cite{consistency}. Demnach war in den 80er Jahren nur die strong consistency bekannt. Sie fordert, dass Datenoperationen immer direkt zwischen der Anfrage vom und der Antwort zum Client durchgeführt werden müssen. Zusätzlich muss jeder Lesezugriff den tatsächlich letzten geschriebenen Wert zurückliefern. Am anderen Ende des Konsistenzspektrums befinden sich die weak und eventual consistency. Bei weak consistency, müssen Lesezugriffe nicht immer den tatsächlich letzten geschrieben Wert zurückliefern. Auch die Reihenfolge, in der die Schreibeoperationen ausgeführt werden, ist nicht vorgegeben. Mehrfache Lesezugriffe nacheinander müssen nicht immer das gleiche Ergebnis liefern. Eventual consistency ist etwas stärker als weak consistency. Auch hier liefert ein Lesezugriff nicht immer die letzten geschrieben Werte zurück. Wird aber für eine Weile kein Schreibzugriff durchgeführt, werden immer die gleichen Werte gelesen. Es tritt also irgendwann ein konsistenter Stand ein. Eventual consistency aus der Sicht einzelner Datenreplikas in einem verteilten System lässt sich mit drei Eigenschaften beschreiben:
\begin{itemize}
  \item \textbf{Eventual delivery:} Wenn eine Schreiboperation eine Datenreplika verändert, wird diese Schreiboperation irgendwann bei allen Datenreplikas durchgeführt.
  \item \textbf{Convergence:} Alle Datenreplikas, welche die Schreiboperationen erhalten haben, werden irgendwann identisch sein.
  \item \textbf{Termination:} Alle Operationen werden ausgeführt und beendet.
\end{itemize}
Systeme, die auf dem BASE-Prinzip aufsetzen, tauschen eine strong consistency gegen die eventual consistency ein und erreichen so eine hohe Verfügbarkeit. Wo ACID pessimistisch mit Schreibzugriffen umgeht und Konsistenz immer am Ende einer Transaktion fordert, ist BASE optimistisch und garantiert nur, dass irgendwann Konsistenz eintreten wird. Dieser Ansatz stützt sich darauf, dass dem Nutzer eine immer verfügbare Anwendung wichtiger ist, als dass sie immer den korrekten Stand anzeigt - vor allem, wenn sich die Dauer des inkonsistenten Zustands auf wenige Sekunden beläuft. Durch diesen kleinen Abstrich bei der Konsistenz schaffen BASE Systeme aber viel Raum für Skalierung und Performance-Steigerungen. Kommunikation zwischen den Teilsystemen, die in einem ACID-System zwingend nötig ist, kann in einem BASE-System vernachlässigt oder auf einen besser passenden Zeitpunkt verschoben werden \cite{clusterBASE}. Client-Anfragen können so nahezu immer und schnell beantwortet werden. Zusätzlich belastet das Starten mehrerer parallel laufender Systeme das Netzwerk nicht so stark wie bei vergleichbaren ACID-Systemen.

\section{Funktionsweise}
DFS basieren auf dem oben erklärten BASE Prinzip. Sie lockern die Konsistenz auf und ermöglichen dadurch gute Skalierbarkeit, Verfügbarkeit und Kosteneffizienz. Deswegen haben DFS einen besonderen Stellenwert in der Speicherung und Verarbeitung von Big Data und werden von vielen Firmen in diesem Bereich eingesetzt. Beispiele hierfür sind das Google File System, welches von Google selbst eingesetzt wird, Facebooks Haystack, das von der Deutsche Telekom, CERN und Cisco verwendete DFS Ceph \cite{ceph} und das von Yahoo entwickelte und weit verbreitete Hadoop Distributed File System (HDFS).

Um sehr große Datenmengen verwalten zu können bedienen sich die meisten DFS des sogenannten Object Storage \cite{cephPaper}. Traditionelle Filesysteme arbeiten mit dem File Storage, bei dem die Dateien und die dazugehörigen Metadaten getrennt abgespeichert werden. Gerade bei sehr vielen Dateien wird die Verwaltung der Metadaten hier zu einem Flaschenhals \cite{filestorage}. Beim Object Storage hingegen werden Datei und die dazugehörigen Metadaten gemeinsam abgespeichert. Die Kombination aus Dateiinhalt und den Metadaten wird auch Objekt genannt. Anstelle auf der Datei zu arbeiten, werden alle Operationen auf Ebene der Objekte durchgeführt. Lesen, Schreiben und Löschen funktioniert ähnlich wie bei einem traditionellen Filesystem, jedoch kann in einem Object Storage der Nutzer nicht bestimmen wie und wo das Objekt tatsächlich abgelegt wird. Der Zugriff auf einen Object Storage erfolgt auch meistens über eine abstrakte Schnittstelle wie zum Beispiel HTTP. Die Speicherverwaltung fällt demnach komplett in den Aufgabenbereich der sogenannten Object Storage Devices (OSD). Mehrere OSDs können so einfach nebeneinander gestellt werden und durch eine HTTP-Schnittstelle wie ein einziges OSD angesprochen werden. Dafür ist lediglich eine zusätzliche Verwaltung der einzelnen OSDs notwendig. Auf diese Weise lässt sich ein Object Storage extrem gut skalieren und liefert eine nahezu endlose Speicherkapazität. Dadurch, dass der Nutzer einen Cluster an OSDs wie einen einzigen ansprechen kann, können die einzelnen OSDs sogar über Ländergrenzen hinweg verteilt sein. So kann zusätzlich Ausfallsicherheit gewährleistet werden \cite{osvideo}\cite{objectstorage}\cite{objectBasedStorage}. In der Regel bestehen Object-Storage-Systeme aus den OSDs selbst und einem Server, welcher diese OSDs verwaltet.

Nachdem die Grundlage eines DFS besprochen wurde, soll die konkrete Funktionsweise zweier DFS erklärt werden. Da das später vorgestellte Konzept auf den Prinzipien der beiden DFS aufbaut, werden die Annahmen der Systeme sowie ein Lese- und Schreibzugriff auf das DFS ausführlich beschrieben. Die beiden betrachteten DFS sind das Google File System (GFS) und Haystack.
Das GFS wurde 2003 im Artikel \textit{The Google File System} vorgestellt \cite{GFS}. Demnach wurde es von Google entwickelt, um die Analyse und Verwaltung ihrer extrem schnell wachsenden Datenmenge zu ermöglichen. Wie alle DFS wurde das GFS ganz im Sinne guter Skalierbarkeit, Verfügbarkeit, Verlässlichkeit und Leistung konzipiert. Aber auch andere Annahmen wurden beim Design des GFS berücksichtigt. So sind Ausfälle oder korrupte Daten bei einem System in dieser Größenordnung eher die Regel als die Ausnahme. Dateien sollen, nachdem sie einmal geschrieben wurden, nur noch gelesen werden. Ein Anhängen von Daten an bereits bestehende Dateien soll aber möglich sein. Die zu verwaltenden Dateien sind in der Regel sehr groß, von einigen Megabyte bis zu mehreren Gigabyte. Demnach muss das System mit Lese- und Schreibzugriffen auf große Datenmengen zurecht kommen. Da kleine Dateien eher die Ausnahme sind, wird dem Lesen und Schreiben dieser keine besondere Aufmerksamkeit geschenkt. Das GFS soll auch mit den Anfragen von sehr vielen Clients umgehen können. Daraus folgt, dass der konkurrierende Zugriff auf eine Datei besonders berücksichtigt werden muss.  

Das GFS stellt den Clients ähnliche Operationen wie gewöhnliche Filesysteme zur Verfügung. Dateien können geöffnet, geschlossen, gelesen und geschrieben werden und sind in Pfaden und Ordnern strukturiert. Zusätzlich können Daten an bestehende Dateien angehängt und Snapshots, also Kopien von Dateien, erzeugt werden. Das GFS selbst besteht dabei aus zwei Hauptkomponenten: einem einzigen Master und mehreren Chunkservern. Dateien werden in 64 Megabyte große Chunks mit jeweils 64 Byte Metadaten aufgeteilt und auf den Chunkservern platziert. Der Master verwaltet dabei den Namensraum für die einzelnen Dateien und Chunks. Außerdem kennt er für jede Datei die dazugehörenden Chunks mit ihren Positionen und den Positionen ihrer Replikas. Für den schnellen Zugriff werden diese Informationen immer im Hauptspeicher des Masters vorgehalten. Um beim Starten des Masters oder im Fehlerfall die Zuweisungen von Datei zu Chunk wiederherstellen zu können, werden alle Änderungen an der Struktur und des Namensraums persistent in einem Operation-Log gespeichert. Dieses agiert als Timeline und weist jeder Änderung einen eindeutigen Zeitstempel zu. Ein einzelner Master ermöglicht eine sehr ausgeklügelte Chunk-Platzierung auf den Chunkservern und vereinfacht das gesamte Design des Systems. Auf der anderen Seite, kann er aber auch zum Flaschenhals werden. Deswegen ist es sehr wichtig, die Anfragen der Clients an den Master so gering wie möglich zu halten. Es werden keine Schreib- und Lesezugriffe auf Dateien direkt durch den Master durchgeführt.

\begin{figure}[htb]
  \centering
  \includegraphics[width=15cm]{img/4/ReadGFS.png}
  \caption[Lesezugriff auf einen Chunk in GFS]{Lesezugriff auf einen Chunk in GFS. Nachempfunden nach \cite{GFS}.}
  \label{readgfs}
\end{figure}

Der Ablauf eines Lesezugriffes ist in Abbildung \ref{readgfs} zu sehen. Der Client weiß, welchen Teil einer Datei er lesen möchte. Aus der bekannten Chunkgröße kann so der Index des benötigten Chunks errechnet werden. Auf eine Anfrage an den Master mit dem Dateinamen und dem Chunkindex antwortet dieser mit der Position des Chunks und all seiner Replikas. Der Client kann nun einen Chunk, der ihm am nächsten liegt, auswählen und die Anfrage an den entsprechenden Chunkserver stellen. Dabei kann er auch übertragen, welche Bytes er innerhalb des Chunks erhalten möchte. Der Chunkserver antwortet dem Client daraufhin mit den angefragten Daten. Da sich die Position der einzelnen Chunks in der Regel nicht ändert, können die Clients diese Information cachen und bei weiteren Lesezugriffen darauf zurückgreifen. Auch das entlastet den Master weiter.

\begin{figure}[htb]
  \centering
  \includegraphics[width=15cm]{img/4/writeGFS.png}
  \caption[Schreiben eines Chunks in GFS]{Ablauf eines Schreibvorgangs in GFS. Nachempfunden nach \cite{GFS}.}
  \label{writeGFS}
\end{figure}

Ein Schreibzugriff hingegen ist in Abbildung \ref{writeGFS} zu sehen. Um zu gewährleisten, dass alle Replikas eines Chunks irgendwann identisch sind, ist es wichtig, dass die Schreiboperationen in der gleichen Reihenfolge durchgeführt werden. GFS realisiert dies durch den Einsatz von Leases. Wenn ein Client eine Datei schreiben möchte, fordert er vom Master ein Lease auf einen Chunk an. Ein Lease hat eine Dauer von 60 Sekunden, kann aber bei Bedarf auch verlängert werden. Der Master gewährt dem Client ein Lease und sendet zusätzlich die Positionen des zu beschreibenden Chunks und seiner Replikas. In GFS gibt es von jedem Chunk standardmäßig drei Replikas. Einer dieser Chunks wird für die Dauer des Leases als Primary gekennzeichnet, die anderen werden zu Secondaries. Darauf hin beginnt der Client, die zu speichernden Daten zu den Replikas zu senden. Dabei ist die Reihenfolge, in der dies geschieht, egal. Es können auch erst die Secondaries und dann der Primary angesprochen werden. Sobald eine Replika Daten erhält, kann dieses die Daten zum nächsten weitersenden. Die Daten werden zunächst nur im Hauptspeicher gehalten und nicht persistent abgespeichert. Erst, wenn die Daten bei allen Replikas angekommen sind, sendet der Client einen Schreibauftrag an den Primary Chunk, der die vorangegangenen Daten identifiziert. Der Primary Chunk verteilt diesen Schreibauftrag dann an alle Replikas, die daraufhin die Daten persistent ablegen. Jeder Secondary bestätigt dem Primary, dass die Daten erfolgreich geschrieben wurden oder teilt einen Fehler mit. Der Primary antwortet letztendlich dem Client und zeigt entweder den Erfolg der Operation an oder sendet die aufgetretenen Fehler. Möchte der Client während der Gültigkeit des Leases ein weiteres Mal Daten schreiben, so muss keine Anfrage mehr an den Master getätigt werden und die Daten können direkt geschrieben werden. Dieser Prozess ermöglicht auch einen einfachen Umgang mit einem zweiten Client, der auf den gleichen Chunk schreiben möchte. Wenn der zweite Client nach einem Lease für den Chunk anfragt, erhält er als Antwort das gleiche Lease und demnach den gleichen Primary wie der erste Client. Auch der zweite Client beginnt, die Daten in beliebiger Reihenfolge auf die Replikas zu streamen. Danach teilt er dem Primary den Schreibauftrag für die geschriebenen Daten mit. Da die Daten erst zum Ende des Leases persistent geschrieben werden, kann der Primary durch Kommunikation mit den Replikas sicherstellen, dass die Daten beider Clients in der richtigen Reihenfolge in den Replikas landen. So kann das Problem mehrerer konkurrierender Zugriffe leicht gelöst werden. Jedoch kann es bis zu 60 Sekunden dauern, bis ein Client tatsächlich den als letztes geschriebenen Wert eines Chunks auslesen kann.

Das Anfertigen von Replikas für jeden Chunk hat neben der Ausfallsicherheit auch noch performanztechnische Vorteile. Clients können immer die Replika lesen, die ihnen am nächsten ist. Wollen mehrere Clients die gleichen Daten lesen, können die Lesezugriffe auf alle bestehenden Replikas verteilt werden. Das GFS nutzt ausgeklügelte Mechanismen, um korrupte Replikas zu erkennen und zu ersetzen. Dabei werden Chunks mit besonders viel gelesenen Daten oder vielen korrupten Replikas am höchsten priorisiert. In regelmäßigen Heartbeat-Nachrichten teilen die Chunkserver dem Master mit, in welchem Zustand sich die einzelnen Chunks befinden. Ein weiteres Problem ist der einsame Master. Dieser stellt einen Single Point of Failure da. Um diesem entgegenzuwirken, werden parallel zum Master mehrere Shadow Masters betrieben. Diese hinken dem Master immer einige Operationen hinterher, können aber über das Operation Log immer auf den korrekten Stand gelangen. Inkonsistenz herrscht innerhalb des GFS also nur, wenn ein Chunk gelesen wird, auf dem gerade ein Lease angefordert wurde - und im Falle eines Ausfalls des Masters - bis der Shadow Master die letzten Operationen aus dem Operation Log durchgeführt hat.

Ähnlich wie Google entwickelte auch Facebook ein eigenes DFS. Ihr Ziel war es, die unzähligen Bilder, die Tag für Tag auf ihr soziales Netzwerk geladen werden, zu speichern und zu verwalten. 2010 beschrieben einige Entwickler von Facebook in dem Artikel \textit{Finding a Needle in Haystack: Facebook's Photo Storage} ihr DFS Haystack \cite{haystack}. 2010 verwaltete dieses bereits etwa 20 Petabytes an Daten. Das System ist in der Lage, mehr als eine Million Lesezugriffe in der Sekunde zu bedienen und einer Milliarde hochgeladene Fotos in der Woche Herr zu werden. Außer auf Skalierbarkeit, Ausfallsicherheit, Verfügbarkeit und Performance wurde bei Haystack ein besonderes Augenmerk auf die Reduzierung der Metadaten pro Datei gelegt. Für die Speicherung von vier Bildern benötigt Haystack gerade einmal 40 Bytes an Metadaten. Gewöhnliche Filesysteme würden dafür 536 Byte anlegen \cite{haystack}. Dadurch sollen alle Metadaten immer im Hauptspeicher gehalten werden und einen schnellen Zugriff ermöglichen. Um dies zu erreichen, wurde auf ein POSIX-konformes Filesystem verzichtet. In Haystack gibt es keine Pfade oder Ordner, für die Metadaten gehalten werden müssen. Bilder werden eindeutig über Schlüssel identifiziert. Ansonsten wurde Haystack unter ähnlichen Annahmen wie das GFS entwickelt. Hardware-Ausfälle sind an der Tagesordnung, es müssen sehr viele Daten gespeichert werden und das System wird von sehr vielen Clients genutzt. Die gewöhnliche Dateigröße unterscheidet sich hingegen von den Anforderungen des GFS. Die hochgeladenen Bilder sind meist nur einige Kilobyte groß und damit vergleichsweise klein. In Haystack werden deshalb mehrere Bilder in einer großen Datei zusammengefasst. Diese Datei wird Volume genannt. Zu Beginn werden leere Volumes mit einer festen Größe angelegt und nach und nach mit Bildern aufgefüllt. Zehn Terabyte an Speicher können so in 100 Volumes mit eine Größe von jeweils 100 Gigabyte aufgeteilt werden.

\begin{figure}
  \centering
  \includegraphics[width=5cm]{img/4/volume.png}
  \caption[Aufbau eines Volumes]{ Aufbau eines Volumes. Nachempfunden nach \cite{haystack}.}
  \label{volume}
\end{figure}
Haystack besteht aus drei Systemelementen: dem Haystack-Directory, Haystack-Store und dem Haystack-Cache. Im Folgenden wird auf diese ohne das Präfix Haystack Bezug genommen.
\begin{itemize}
  \item \textbf{Directory:} Die Aufgabe des Directorys ist die Verwaltung aller Volumes. Um die Replikation der Daten zu vereinfachen, fasst das Directory mehrere physikalische Volumes in ein logisches Volume zusammen. Beim Schreibvorgang wird nur noch ein logisches Volume referenziert und in alle darin enthaltenen physikalischen Volumes geschrieben. Zusätzlich kann das Directory Volumes auf read-only setzen, wenn diese ihre Kapazität erreicht haben oder zum Teil korrupt sind.
  \item \textbf{Cache:} Um viele Anfragen direkt aus dem Hauptspeicher beantworten zu können, werden alle Bilder, die das System verlassen im Cache gespeichert. Dadurch werden vor allem kurz nach dem Hochladen eines neuen Fotos die Maschinen entlastet. Denn neue Bilder werden häufiger gelesen als ältere.
  \item \textbf{Store:} Der Store speichert die tatsächlichen Bilddaten. Er besteht ähnlich wie die Chunkserver beim GFS aus mehreren Store Machines. Dabei kümmert sich eine Store Machine um mehrere Volumes, welche jeweils mehrere Millionen Bilder enthalten können. Der Aufbau eines Volumes ist in Abbildung \ref{volume} dargestellt. Demnach steht am Anfang immer ein Superblock, welcher Informationen zur Anzahl der freien Blöcke und der Größe des Dateisystems beinhaltet \cite{wiki:superblock}. Gefolgt wird dieser von mehreren Needles, die jeweils ein Bild repräsentieren. Der Zugriff auf ein Bild erfolgt über Angabe des Volume-Schlüssels, eines Offsets und der Größe der zu lesenden Daten. All diese Daten werden von der Store Machine im Speicher gehalten. Zusätzlich wird diese Information in einem Volume Index File gesichert. Das verhindert, dass beim Starten einer Store Machine alle Volumes gelesen werden müssen, um die benötigten Informationen in den Speicher zu schieben. 
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{img/4/ReadHaystack.png}
  \caption[Ablauf beim Lesen eines Bildes.]{ Ablauf beim Lesen eines Bildes. Nachempfunden nach \cite{haystack}.}
  \label{readHaystack}
\end{figure}

In Abbildung \ref{readHaystack} sind die Schritte zum Lesen eines Bildes in Haystack schematisch dargestellt. Zunächst fragt der Client das Directory nach dem gewünschten Bild. Dieses wird durch einen eindeutigen Schlüssel identifiziert, der die Form \textit{[logisches Volume ID, Bild ID]} aufweist. Über das logische Volume kann das Directory sehr leicht ein physikalisches Volume heraussuchen und die Adresse einer Store Machine, die dieses beherbergt, an den Client senden. Der Client sendet jetzt erneut den Schlüssel des Bildes, aber diesmal an die Adresse der Store Machine. Diese Anfrage trifft den Cache. Wurde das Bild zuvor bereits einmal ausgeliefert, kann dieser direkt mit den Bilddaten antworten. Befindet sich das Bild nicht im Cache, so wird die Anfrage zur Store Machine weitergeleitet. Diese sucht mithilfe des Schlüssels für das Bild und des logischen Volumes das physikalische Volume, die Größe des Bildes und den benötigten Offset heraus. Damit kann das Bild aus dem physikalischen Volume gelesen und über den Cache zum Client geschickt werden. 

Wie in Haystack ein Bild gespeichert wird, ist in Abbildung \ref{writeHaystack} zu sehen. Im ersten Schritt fragt der Client das Directory nach einem beschreibbaren logischen Volume. Dieses antwortet mit der ID des logischen Volumes und den Adressen der Store Machines, die die im logischen Volume enthaltenen physikalischen Volumes beinhalten. Darauf hin vergibt der Client einen eindeutigen Schlüssel für das hochzuladende Bild. Der Schlüssel entspricht der oben erklärten Form. Gemeinsam mit dem Schlüssel übertragt der Client daraufhin die Bilddaten an alle vom Directory erhaltenen Store Machines. Diese hängen das Bild in den entsprechenden Volumes an und aktualisieren die Volume-Information in ihrem Hauptspeicher. Da Bilder in Volumes immer nur an die bestehenden Bilder angehängt werden können, ist eine Modifikation eines bereits gespeicherten Bildes nicht möglich. Um ein solches Bild zu überschreiben, sendet der Client ein neues Bild mit dem gleichen Schlüssel wie das zu überschreibende Bild. Bei der nächsten Anfrage nach diesem Bild wird die Store Machine immer das Bild zurückgeben, das den größten Offset innerhalb des Volumes besitzt, da dieses immer das aktuellste ist. Der Speicher der für das ungültig gewordene Bild benötigt wird, bleibt weiterhin belegt. Um die Netzwerkbandbreite und die Leistung der Festplatten optimal auszunutzen, bemüht sich Haystack immer, mehrere Bilder auf einmal zu einem Volume hinzuzufügen. Dies wird ermöglicht, da Nutzer von Facebook in der Regel Bilder in Alben auf die Plattform laden.

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{img/4/WriteHaystack.png}
  \caption[Ablauf beim Schreiben eines Bildes in Haystack.]{ Ablauf beim Schreiben eines Bildes. Nachempfunden nach \cite{haystack}.}
  \label{writeHaystack}
\end{figure}

Damit der Speicher, der durch überschriebene oder korrupte Bilder belegt wird, nicht verloren ist, werden Volumes regelmäßig verdichtet. Dazu wird das Volume Bild für Bild kopiert und korrupte oder überschriebene Bilder übersprungen. Daraufhin wird das ursprüngliche Volume durch das neue ersetzt. Um auch bei korrupten Bildern eine gleichmäßige Replikation zu garantieren, kommunizieren Store Machines und Directory gelegentlich miteinander. Dabei teilen die Store Machines den Inhalt und den Zustand ihrer Volumes mit. Sind einige Bilddaten korrumpiert, kann das Directory dem betroffenen logischen Volume ein neues physikalisches Volume zuordnen und die Replikation starten. Inkonsistenzen treten bei Haystack nur auf, wenn ein Bild angefordert wird, während es noch auf die einzelnen Store Machines verteilt wird, oder wenn ein korruptes Bild angefragt wird, bevor dieser Fehler von der zuständigen Store Machine erkannt wurde. Werden Bilder überschrieben, kann es auch passieren, dass Clients noch das ursprüngliche Bild aus dem Cache erhalten, bis dieses ungültig wird.

Haystack und das GFS ähneln sich in den Grundstruktur des Systeme sehr. Die eigentlichen Daten werden auf extra dafür vorgesehene OSDs gespeichert und ein Master kümmert sich um deren Verwaltung. Beide DFS ermöglichen einen effizienten Zugriff auf Daten, der im besten Fall nur eine einzige Leseoperation von der Festplatte benötigt. Durch das Verteilen der Anfragen auf mehrere OSDs sind beide Systeme nahezu beliebig skalierbar und verfügen über eine enorme Ausfallsicherheit. Dennoch gibt es nicht nur bei den gespeicherten Daten Unterschiede. Während das GFS sehr große Dateien verwaltet und diese sogar in Chunks aufteilt, bemüht sich Haystack darum mehrere Bilder in einer einzigen großen Datei, dem Volume zusammenzufassen. Demnach sind nicht nur die Schreibe- sondern auch die Lesezugriffe bei Haystack deutlich kleiner als bei dem GFS. Der Größe der Dateien ist auch die Tatsache geschuldet, dass sich ein Cache im GFS nicht lohnt. Ein weiterer Unterschied ist in der Anordnung der Dateien zu finden. GFS liefert ein fast POSIX-konformes Dateisystem. Haystack hingegen verzichtet auf Pfade und Ordner komplett. Dadurch kann die Menge an Metadaten weiter reduziert werden. Dieser Schritt ist notwendig, da Haystack kleinere und damit mehr eigenständige Dateien verwaltet. 

\section{DFS zur Bewältigung der Anforderungen eines Buchungssystems}
Die Frage ist, ob sich ein DFS eignet, um die Anforderungen und Probleme eines Buchungssystems zu bewältigen.
Im Kapitel \ref{bookingSystem} wurden die aktuellen und kommenden Herausforderungen, denen sich ein Buchungssystem stellen muss, erörtert. Ob ein DFS diesen gewachsen ist, soll sich durch die in diesem Kapitel erarbeiteten Informationen zeigen \cite{largeHadoop}.


\begin{itemize}
  \item \textbf{Wohldefinierte Schnittstelle:} Die Entwicklung einer wohldefinierten Schnittstelle sowohl für bankeigene Prozesse als auch für Drittanbieter ist unabhängig von der genutzten Persistenzschicht. Die Realisierung eines Buchungssystems mit einem DFS als Grundlage sollte demnach für diese Anforderung kein Problem darstellen. Gegebenenfalls könnten einfache Anfragen sogar direkt die Schnittstelle des DFS nutzen und die Systeme weiter entlasten.

  \item \textbf{Skalierbarkeit:} Die grundlegende Systemarchitektur vieler OSDs und eines Masters, der diese verwaltet, gewährleistet eine einfache horizontale Skalierung. Speicherkapazität kann nahezu nach Belieben durch das Hinzufügen weiterer OSDs erhöht werden. Auch die Verarbeitung von Lese- und Schreibzugriffen profitiert von dieser Art der Skalierung. Dadurch, dass diese Anfragen nie direkt durch den Master gehen, wird die Last im Netzwerk sowie auf die OSDs verteilt.

  \item \textbf{Ausfallsicherheit:} Die in einem DFS abgelegten Daten werden immer auf mehreren OSDs abgelegt. Ausgeklügelte Algorithmen zur Platzierung der Replikas ermöglichen eine sehr hohe Fehlertoleranz und Ausfallsicherheit. Die geographische Verteilung der Replikas ermöglicht sogar eine Datenwiederherstellung, wenn ganze Rechenzentren versagen \cite{osvideo}.

  \item \textbf{Verfügbarkeit:} Verfügbarkeit ist eine Kernkompetenz eines DFS. Wie alle anderen BASE basierten Systeme, lockern DFS die Konsistenz, um Anfragen zu beantworten, auch wenn Teile des Systems unerreichbar sind.

  \item \textbf{Kosteneffizienz:} DFS sind entwickelt worden, um auf Standard-Hardware zu laufen. Die einfache horizontale Skalierung hält die Betriebskosten auch für große Systeme niedrig \cite{rdbmssuck}.

  \item \textbf{Leistung:} Viele Lesezugriffe auf ein DFS benötigen nur einen einzigen Zugriff auf die Festplatte. Informationen zu den Dateien und wo sie sich befinden werden in der Regel im Hauptspeicher vorgehalten. Replikas von Dateien und Chunks ermöglichen dem Client den Zugriff auf die ihm am nächsten liegende Datei sowie eine Lastverteilung. 
\end{itemize}

Diese Punkte erfüllen DFS nahezu mühelos. Das Gewährleisten eines konsistenten Zustands bringt ein DFS hingegen ins Schwitzen. Bevor ein Zahlungsverkehr autorisiert wird, sollte sichergestellt sein, dass der Schuldner über die benötigten Zahlungsmittel verfügt. Für den Halter eines Girokontos ist es auch von großer Bedeutung, dass der in einer Webanwendung angezeigte Kontostand dem tatsächlichen entspricht. Auch die geringe Datenmenge, die bei einer einzelnen Buchung gespeichert werden muss, ist nicht optimal. Informationen zu Gläubiger, Schuldner, Betrag, Währung und Beschreibung der Buchung kann in wenigen hundert Bytes abgebildet werden. DFS sind in der Regel auf die Speicherung größerer Dateien ausgelegt. Der Artikel \textit{An optimized approach for storing and accessing small files on cloud storage} zeigt an dem HDFS die Probleme der Verwaltung vieler kleiner Dateien auf \cite{hdfsSmallFiles}. Das HDFS wurde stark von dem GFS inspiriert und unterscheidet sich nur geringfügig \cite{hdfsGfs}. Für jede Datei wird ein eigener Eintrag im Master erstellt. Dieser soll für den effizienten Zugriff immer im Hauptspeicher liegen. Viele kleine Dateien benötigten demnach sehr viel mehr Hauptspeicher des Masters, als eine sehr große Datei mit der gleichen Menge an Inhalt. Auch die Beantwortung von Lesezugriffen auf viele kleine Dateien kann die Leistung eines DFS in die Knie zwingen. Abbildung \ref{hdfsSmallFiles} zeigt, wie lange das Herunterladen von 320 Megabyte dauert, je nachdem, auf wie viele Dateien die Datenmenge verteilt ist. 5120 Dateien mit jeweils 64 Kilobyte herunterzuladen, dauert vier und ein viertel mal so lange wie der Download von 40 acht Megabyte großen Dateien. Diese Verzögerung kommt vor allem durch die für jede Datei benötigte Anfrage an den Master \cite{hdfsSmallFiles}. Das Ablegen vieler kleiner Dateien benötigt demnach eine gesonderte Behandlung.



\begin{filecontents}{date5.dat}
date  value
1   15
2   9
3   6
4   5
5   4
6   3.8
7   3.7
8   3.6
\end{filecontents}


\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
xtick=data,
xmin=0, xmax=9,
ymin=0, ymax=16,
xticklabels={64KB,128KB,256KB,512KB,1MB,2MB,4MB,8MB},
xticklabel style = {font=\tiny},
xlabel={Dateigröße},
ylabel={Zeit zum Herunterladen in Sekunden},
]
\addplot table[x=date,y=value] {date5.dat};
\end{axis}
\end{tikzpicture}
\caption[Zeit zum Herunterladen unterschiedlicher Dateigrößen in HDFS]{Zeit zum Herunterladen unterschiedlicher Dateigrößen in HDFS bei gleichem Datendurchsatz (320 MB). Nachempfunden nach \cite{hdfsSmallFiles}.}
\label{hdfsSmallFiles}
\end{center}
\end{figure}

\chapter{Konzept}
\label{concept}
Der Erhalt eines konsistenten Zustandes, sowie die geringe Datenmenge einer einzelnen Buchung scheinen nicht mit den Möglichkeiten eines DFS vereinbar. Wenn der Kontostand eines Kontos nicht sicher bestimmt werden kann, kann die Durchführung einer Buchung den Kontostand unter Null bringen. Das darf aber zum Beispiel bei einem Girokonto nicht passieren. Aber gelten diese Bedingungen denn für alle Kontoarten? In diesem Kapitel soll ein Konzept entwickelt werden, das die Stärken eines DFS für ein Buchungssystem zugänglich macht. Dabei wird die Relevanz der strong consistency in Frage gestellt und eine Lösung für das Speichern der kleinen Datenmenge einer Buchung erarbeitet. Zu Letzt soll ein Designvorschlag für das Buchungssystem gemacht werden, der eine extreme Skalierbarkeit sowie ausreichende Konsistenz liefert.

\section{Konsistenz und Kontoarten}
\label{konsistenzKonten}
Strong consistency in einem Buchungssystem scheint unabdingbar. Wenn man sich aber auf die Bedeutung der eventual consistency besinnt, wird klar, dass ein DFS auch irgendwann konsistent ist. Und zwar immer wenn eine bestimmte Zeit keine Schreibzugriffe getätigt wurden. 
Inkonsistenzen treten also nur im Rahmen weniger Sekunden bis zu einer Minute nach der Durchführung einer Buchung auf. Gibt es demnach keine Konten für die diese Konsistenz ausreichen würde? Problematisch sind alle Konten, bei denen regelmäßig Geld ein- und ausgeht und ein vorgegebener Saldo nicht unterschritten werden darf. Bei einem Girokonto zum Beispiel darf der Kontostand nicht unter Null gehen. Durch einen Dispositionskredit kann der Kontostand zwar negativ werden, aber auch nur einen bestimmten Betrag erreichen. Mit einer eventual consistency kann dieses Verhalten aber nicht garantiert werden. Wenn zum Zeitpunkt einer Buchung kein konsistenter Zustand vorliegt, kann nicht festgestellt werden, ob das Konto noch über genügend Zahlungsmittel verfügt. Würden Buchungen immer nur zu bekannten Zeitpunkten eingehen, könnte man sicher gehen, dass zuvor ein konsistenter Zustand hergestellt wird. Auch Konten, die beliebig überzogen werden können, könnten abgebildet werden. Hier spielt es keine Rolle, ob ein bestimmter Betrag unterschritten wurde und Buchungen können ohne Berücksichtigung des Kontostandes ausgeführt werden. Würde ein Girokonto auf diese Art funktionieren läge die Verantwortung ein bestimmtes Saldo des Kontos nicht zu unterschreiten beim Bankkunden mit dem Risiko einer starken Verschuldung. Ganz zu schweigen von der Gefahr, dass ein unberechtigter Zugriff auf ein solches Konto fatal für den Kunden sowie das Vertrauen in die Bank wäre.

Konten die eine starke Konsistenz also nicht zwingend benötigen, sollten nicht einfach von außerhalb der Bank zugänglich sein. Auch Konten die lediglich Zahlungen empfangen und selbst keine durchführen würden mit einer gelockerten Konsistenz zurecht kommen. Wird kein Geld abgehoben, kann der Kontostand auch nicht sinken. Aber auch wenige kontrollierte Abbuchungen sind mit eventual consistency realisierbar. Wenn Abbuchungen nur in sehr großen Abständen oder einmalig erfolgen, kann sichergestellt werden, dass zum Zeitpunkt der Buchung der Zustand konsistent ist. All diese Punkte treffen auf die Konten zu, die komplett in der Verantwortung der Bank liegen. Von den Passivgeschäften lässt sich die Termineinlage sehr gut mit einem DFS abbilden. In der Regel handelt es sich hierbei um eine einmalige Einzahlung des Bankkunden, bei der bekannt ist, wann der Betrag plus Zinsen wieder ausgezahlt wird. Für die Dauer der Termineinlage liegt die Verwaltung der Zahlungsmittel bei der Bank. Diese ist durchaus in der Lage Buchungen nur zu bestimmten Zeitpunkten durchzuführen und sicherzustellen, dass das Kontosaldo nicht unter Null fällt. Besonders gut eignen sich jedoch die Aktivgeschäfte für die Abbildung ohne strong consistency. Bei allen Formen des Ratenkredites, gibt es auf Seiten der Bank ein Konto, auf welches der Kreditnehmer seine Raten einzahlen muss. Dieses Konto erfährt nur einen Geldzufluss und zum Ende des Kredits wird der Gesamtbetrag auf ein weiteres Konto der Bank gebucht. Für jede Termineinlage und für jeden Ratenkredit, wird also ein Konto angelegt, welches relativ wenige Kontobewegung erfährt und dessen Verwaltung in den Händen der Bank liegt. Wie Abbildung \ref{ratenkredite} zeigt ist die Anzahl der Konten die allein für die Ratenkredite angelegt werden beachtlich. Auch wenn jedes dieser Konten an sich die starke Konsistenz der relationalen Datenbanken nicht benötigt, liegen sie in den gleichen Systemen wie die Girokonten und beanspruchen Speicher und Leistung der relationalen Datenbanken. Alleine die Speicherung der Konten für Termineinlagen und Ratenkredite in einem DFS könnte den Banken Kosten sparen und Druck von den Buchungssystemen nehmen.

\begin{filecontents}{date6.dat}
date       value
2008-01-01  6909
2009-01-01  7611
2010-01-01  7272
2011-01-01  7183
2012-01-01  7697
2013-01-01  7737
2014-01-01  7434
2015-01-01  7442
\end{filecontents}


\begin{figure}[htb]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
date coordinates in=x,
xtick=data,
xticklabel style=
{rotate=90,anchor=near xticklabel},
xticklabel=\year,
xlabel={Jahre},
y tick label style={/pgf/number format/1000 sep=},
extra y tick style={grid=major, tick label style={xshift=-1cm}},
ylabel={Anzahl der Kreditverträge in Tausend},
date ZERO=2008-01-01,% <- improves precision!
]
\addplot table[x=date,y=value] {date6.dat};
\end{axis}
\end{tikzpicture}
\caption[Anzahl der Ratenkreditverträge in Deutschland]{Anzahl der Ratenkreditverträge in Deutschland. Nachempfunden nach \cite{ratenkredite}.}
\label{ratenkredite}
\end{center}
\end{figure}

\section{Annahmen}
Das hier entwickelte Konzept soll aber von der Skalierbarkeit und Leistung auch in der Lage sein alle anderen Kontoarten abzubilden. Eine Lösung um die benötigte Konsistenz zu gewährleisten soll im weiteren Verlauf des Kapitels erarbeitet werden. Zunächst sollen einige Annahmen getroffen werden, nach denen ein DFS und das darauf aufgesetzte Buchungssystem entwickelt wird. Die wichtigste Aufgabe eines DFS in einem Buchungssystem ist das persistente Abspeichern aller Buchungen die für alle Konten eingehen. Eine Buchung besteht im Rahmen dieser Arbeit aus dem Namen des Empfängers und des Senders, dem IBAN und dem BIC des Empfängers, dem zu übermittelnden Geldbetrag sowie der Währung, einem Ausführungsdatum und einem Freitextfeld für eine Beschreibung der Buchung. Die Datenmenge einer Buchung sollte daher ein Kilobyte nicht überschreiten. Das DFS muss mit sehr vielen von diesen Buchungen umgehen können, auch wenn mehrere für ein Konto nahezu parallel eintreffen. Zum Beispiel wenn eine Überweisung und ein Dauerauftrag auf den gleichen Zeitpunkt fallen. Anfragen werden durch die Schnittstellen zu Online- und Mobile-Banking sowie der PSD2 von einer Vielzahl an Clients gestellt, wobei zu erwarten ist, dass die Mehrheit davon Lesezugriffe sind. Dabei wird selten nur eine konkrete Buchung ausgelesen und häufiger alle Buchungen eines gewissen Zeitraums für ein Konto angefordert. Beispielsweise alle Buchungen im letzten Monat oder alle Buchungen im letzten halben Jahr. Damit die Bank ihren Funktionen nachgehen und dem Kunden ein gutes Nutzererlebnis bieten kann, muss eine hohe Verfügbarkeit des Buchungssystems gewährleistet sein. Können keine Buchungen durchgeführt werden ist damit direkt ein Gewinnverlust und Imageschaden der Bank verknüpft. Auch wenn Teile des Systems nicht erreichbar sind, muss das Buchungssystem weiter funktionieren können. Bei Systemen, die mehrere Milliarden Buchungen im Jahr bearbeiten, sind Hardware-Ausfälle und Fehler an der Tagesordnung. 
GFS eignet sich nicht für das persistente Abspeichern der Buchungen, da es auf größere Dateien ausgelegt ist. Für jede Datei wird ein Chunk angelegt, welcher 64 Byte im Hauptspeicher des Masters belegt. Wenn der Master über 16 Gigabyte Hauptspeicher verfügt und für jede Buchung ein Chunk angelegt werden muss, könnten so gerade einmal 250 Millionen Buchungen abgelegt werden bevor die Grenzen des Masters erreicht wären. Nach Abbildung \ref{bargeldlos} wäre das nur ein Bruchteil der Buchungen die im Jahr 2011 durchgeführt wurden. Haystack passt etwas besser auf die Anforderungen eines Buchungssystems an ein DFS. Die in Haystack verwalteten Bilder sind häufig auch nur wenige Kilobyte groß. Jedoch bietet Haystack keine Möglichkeit mehrere Bilder auf einmal auszulesen. Jeder Zugriff erfordert den exakten Schlüssel des Bildes. Ein Auslesen aller Bilder die in einem gewissen Zeitraum hochgeladen wurden ist nicht möglich. Auch der Cache ist für ein Buchungssystem nicht relevant. Buchungen werden immer nur vom Besitzer des entsprechenden Kontos angefragt. Das eine Buchung mehrmals nacheinander benötigt wird, ist deshalb unwahrscheinlich.

\section{DFS für kleine Daten}
\label{oneOnlyDFS}
Ein DFS, das für ein Buchungssystem geeignet ist muss in der Lage sein viele kleine Dateien beziehungsweise Buchungen abzulegen und auszulesen. Wobei besonders das Auslesen mehrerer Buchungen am Stück relevant ist. Auch die Zeit in der das System inkonsistent ist, soll möglichst gering ausfallen. Wie bereits beschrieben eignen sich sowohl GFS als auch Haystack nicht optimal für diesen Anwendungszweck. Durch den gezielten Einsatz einiger Techniken der beiden DFS sowie von Ansätzen aus mehreren Artikeln, ist die Entwicklung eines DFS, das all diese Anforderungen erfüllt jedoch durchaus möglich \cite{hdfsSmallFiles}\cite{hadoopSmallFiles2}\cite{smallFilesHDFS3}.

Der generelle Aufbau des DFS besteht wieder aus mehreren OSDs, die die Daten verwalten und einem Master, der wiederum die OSDs verwaltet. Viele kleine Dateien sollen aufgrund ihres logischen Zusammenhangs gruppiert und zusammengefügt werden. Dateien hängen logisch zusammen, wenn sie sich ein gemeinsames Überthema teilen. In einem traditionellen Filesystem ist das häufig der Fall, wenn Dateien im gleichen Ordner liegen. Beispiele sind alle Bilder die in einer Stadt aufgenommen wurden, oder alle Musikstücke eines Künstlers oder auch alle Buchungen eines Kontos die in einem gewissen Zeitraum getätigt wurden. Dateien die einen solchen logischen Zusammenhang besitzen, werden häufig gemeinsam angefragt. Daher sollen hier mehrere Buchungen in eine gemeinsame Datei eingefügt werden. Diese Datei ist vergleichbar mit einem Volume bei Haystack. Um Verwirrung zu vermeiden wird sie im Folgenden jedoch Bucket genannt. Ein Bucket hat immer eine feste Größe und wird durch das Einfügen von Daten gefüllt. Da das DFS mit sehr kleinen Daten umgehen soll, wird die Größe eines Buckets auf 16 Megabyte festgelegt. Geht man von etwa einem Kilobyte an Daten pro Buchung aus, so kann ein Bucket 16000 Buchungen halten. Um einen Bucket innerhalb eines Jahres zu füllen müssten über 43 Buchungen am Tag eingehen. Eine Vielzahl an Konten lässt sich also mit nur einem einzigen Bucket abdecken. Aber auch Konten mit sehr vielen Buchungen wie zum Beispiel von einem Online-Händler können auf nur wenige Buckets verteilt werden. Die Verwaltung von mehreren kleinen Einträgen in einer großen Datei ist zwingend erforderlich, um den Hauptspeicher des Masters zu entlasten. Dieser kennt immer nur die Adresse des Buckets und nicht dessen gesamten Inhalt. Ein weiterer Vorteil der Gruppierung von logisch zusammenhängenden Daten in einer großen Datei ist, dass Lesezugriffe nur einen einzigen Zugriff auf die Festplatte benötigen, um mehrere Daten in den Hauptspeicher zu laden und die Anfrage des Clients zu beantworten. Bei einer kleineren Bucket-Größe, werden mehr Buckets benötigt, um die gleiche Datenmenge zu verwalten und es wird mehr Hauptspeicher des Masters belagert. Wählt man eine größere Bucket-Größe, so wird der Master entlastet, aber die OSDs müssen mehr Daten durchsuchen um einen Eintrag zu finden. Die Bucket-Größe von 16 Megabyte liefert ein sinnvolles Verhältnis von Leseaufwand der OSDs und Verwaltungsaufwand des Masters. Nur wie kann der Master wissen in welchem Bucket sich welche Daten befinden? Alle abgelegten Daten erhalten einen streng aufsteigenden, acht Byte großen Schlüssel. Dieser Schlüssel gibt den Zeitpunkt der Erstellung bis auf Nanosekunden genau wieder. Die Adresse eines Buckets beinhaltet immer den Schlüssel und somit den Zeitpunkt der ältesten Daten die innerhalb des Buckets abgelegt wurden. Werden weitere Daten in diesen Bucket eingefügt, so werden diese streng aufsteigend eingegliedert. Bevor ein neuer Bucket angelegt wird, muss immer erst der vorherige komplett aufgefüllt werden. Fordert der Client nun Daten mit einem Schlüssel an, muss nur der Bucket gefunden werden, dessen Adresse entweder gleich dem gegebenen Schlüssel ist, oder am nächsten vor dem gegebenen Schlüssel liegt. Wenn bekannt ist, in welchem Bucket sich die benötigten Daten befinden, kann dieser gelesen und der entsprechende Abschnitt zurückgeliefert werden. Damit aber nicht bei jeder Anfrage der komplette Bucket durchlaufen werden muss, wird wie bei Haystack ein Bucket Index File angelegt. Im Gegensatz zu Haystack wird darin aber nicht jeder einzelne Eintrag festgehalten sondern nur einige Checkpoints abgespeichert. Dies geschieht immer wenn ein gewisser Füllstand erreicht wurde. Standardmäßig soll ein 16 Megabyte großer Bucket mindestens 16 mal unterteilt werden. Der Aufbau eines Buckets und des dazugehörigen Bucket Index Files ist schematisch in Abbildung \ref{bucket} zu sehen. Der Inhalt des Bucket Index Files wird von den OSDs im Hauptspeicher gehalten. Das Einfügen nur weniger Checkpoints ermöglicht entgegen der Indizierung jedes einzelnen Eintrags, dass sehr viele Buckets auf einem OSD gehalten und effizient verwaltet werden können. Ein Eintrag im Bucket Index File speichert den acht Byte großen Schlüssel, plus drei Byte für den Offset innerhalb des Buckets. Die Größe der Daten muss nicht mit abgelegt werden. Jeder Eintrag im Bucket beginnt mit seinem Schlüssel gefolgt von zwei Byte für die Größe der abgelegten Daten. Beim Suchen bestimmter Daten mittels eines Schlüssels wird immer von dem Checkpoint gestartet, der am nächsten vor dem Schlüssel liegt und von dort aus Eintrag für Eintrag gelesen. Für den Zugriff auf beliebige Einträge müssen bei einer durchschnittlichen Dateigröße von einem Kilobyte also bis zu 1000 Einträge gelesen werden, bis der richtige gefunden ist. Dieser Suchvorgang fällt aber beim Lesen mehrerer Einträge am Stück nicht mehr besonders ins Gewicht.

\begin{figure}
  \centering
  \includegraphics[width=10cm]{img/5/bucket.png}
  \caption[Aufbau eines Buckets und dem Bucket Index Files]{ Aufbau eines Buckets (links) und eines Bucket Index Files (rechts).}
  \label{bucket}
\end{figure}

Um Buckets effizient zu Nutzen muss das DFS aber auch in der Lage sein, logisch zusammenhängende Daten zu erkennen. Die einfachste Möglichkeit wäre die Verwendung von Ordnern und Pfaden. Alle Dateien die sich innerhalb eines Ordners befinden werden gemeinsam in einen Bucket abgelegt. Die Verwaltung von Ordnern und Pfaden beansprucht aber Logik und Hauptspeicher des Masters. Das Verschieben von Dateien innerhalb der Ordner, das Umbenennen von Ordnern oder das Anlegen neuer Unterordner ist aber vielleicht gar nicht nötig. Eine Technik die den logischen Zusammenhang in den Schlüssel von Daten fest kodiert nennt sich Namespace Flattening \cite{smallfilesObjectStorage}. Im Falle von Buchungen für Konten bietet es sich an, im ersten Teil des Schlüssels BIC und IBAN aneinanderzuhängen, gefolgt von dem zweiten Teil des Schlüssels, der einem acht Byte Timestamp entspricht. Alle Daten, die beim Ablegen einen identischen ersten Teil des Schlüssels besitzen, landen im gleichen Bucket. Zusätzlich kann nur nach dem ersten Teil des Schlüssels gefragt werden, um alle Buckets zu erhalten, die auf den Schlüssel bezogen logisch zusammenhängende Daten beinhalten. Durch das Namespace Flattening kann wichtiger Hauptspeicher auf dem Master gespart werden. Dieser benötigt mit diesem Ansatz ingesamt 64 Byte Hauptspeicher für einen Bucket. Für BIC und IBAN 44 Byte, für die Adresse der drei Replikas des Buckets jeweils vier Bytes und für den zweiten Teil des Schlüssels, dem Timestamp acht Bytes \cite{bic}\cite{iban}.

Die oben beschriebenen Ansätze ermöglichen das Abspeichern vieler kleiner Daten und das effiziente Auslesen mehrerer zusammenhängenden Daten. Ein weiteres Problem, dass sehr viele kleine Dateien mit sich bringen, ist die Last der Anfragen auf den Master. Für jeden Lesezugriff muss der Master nach der Position der Datei gefragt werden. Zum Großteil kann dieses Problem bereits durch die Gruppierung mehrerer Daten in eine große Datei mit streng aufsteigenden Schlüssel behoben werden. Wenn aber Lesezugriffe stattfinden, die über die Grenzen von Buckets hinausgehen, ist der Master aktuell immer ein weiteres mal involviert. Dieser Zugriff auf den Master kann jedoch durch das sogenannte Prefetching verhindert werden \cite{smallfilesObjectStorage} \cite{smallFilePrefetching}. Prefetching bezeichnet gewöhnlich das clientseitige Anfragen und Speichern von Daten, bevor sie eigentlich benötigt werden. Das setzt aber voraus, dass der Client genau weiß, welche Daten er überhaupt anfragen könnte. Das ist in einem Buchungssystem aber nicht der Fall. Der Client ist sich nicht bewusst, welche Buchungen in welchen Buckets liegen. Daher soll das hier entwickelte DFS ein serverseitiges Prefetching implementieren. Bei jeder Anfrage nach einzelnen Buchungen oder einem zeitlich zusammenhängenden Block an Buchungen, soll der Master nicht nur die Adresse des Buckets mit den angefragten Daten zurückliefern, sondern auch die Adresse der nächsten Buckets zur Verfügung stellen. Für weitere Lesezugriffe muss dann nicht mehr der Master gefragt werden. Da BIC und IBAN hart in den Schlüssel für Daten einkodiert werden, ändern sich die Adressen der Buckets nur, wenn diese korrupt werden sollten. Solange das nicht der Fall ist, kann der Client diese Information auch im Cache behalten.

Nachdem die generelle Struktur des DFS erklärt wurde, werden nun die Schritte die zum Schreiben und Lesen von Daten nötig sind aufgezeigt.
Das Schreiben von Daten erfolgt nahezu identisch wie bei dem GFS. Der Client fragt mit dem ersten Teil des Schlüssels also BIC und IBAN beim Master nach einem beschreibbaren Bucket. Jeder Bucket besitzt standardmäßig drei Replikas. Der Master gewährt einen Lease auf einen Bucket und sendet die Adressen der Replikas zum Client. Eine dieser Replikas wird Primary die anderen Secondaries. Der Client beginnt in beliebiger Reihenfolge auf die Replikas zu streamen und sendet erst danach den Schreibauftrag zum Primary. Es wird immer zunächst der zweite Teil des Schlüssels und die Größe der Datei in die Buckets geschrieben, gefolgt von den eigentlichen Nutzdaten. Beim Erreichen einer gewissen Füllmenge des Buckets, wird der geschriebene Eintrag im Bucket Index File als Checkpoint abgelegt und im Hauptspeicher des zuständigen OSDs hinzugefügt. Da es sich hier um kleine Datenmengen handelt, wird die Dauer eines Leases auf zehn Sekunden festgelegt. Durch diese Art des Schreibvorganges ergeben sich die gleichen Vorteile wie beim GFS. Die Reihenfolge in der Daten geschrieben werden, ist immer in allen Replikas der Buckets die gleiche, und der Zugriff durch einen zweiten Client stellt kein Problem für die Konsistenz dar.

Ein Lesezugriff läuft sehr ähnlich wie bei Haystack ab. Neben dem Lesen eines konkreten Eintrages über einen Schlüssel, soll aber auch das Lesen mehrerer Einträge die nach einem bestimmten Zeitpunkt abgelegt wurden möglich sein. Beim Lesen eines einzelnen Eintrags über einen Schlüssel, sendet der Client den Schlüssel an den Master. Dieser findet über den ersten Teil des Schlüssels heraus in welchen Buckets sich die Daten potentiell befinden können. Über den zweiten Teil des Schlüssels kann der genaue Bucket identifiziert werden. Der Master antwortet dem Client mit der Adresse des Buckets und seiner Replikas für die angefragten Daten und zusätzlich mit der Adresse der zwei am nächsten anliegenden Buckets. Der Client kann jetzt mit dem Schlüssel bei dem OSD anfragen, der den Bucket beheimatet. Das OSD sucht in seinem Hauptspeicher für den zuständigen Bucket nach dem Checkpoint, der gleich ist oder am nächsten vor dem gegebenen Schlüssel liegt. Von diesen Checkpoint aus beginnt die Suche nach dem Eintrag. Der maximale Suchraum für einen Eintrag liegt immer zwischen zwei Checkpoints. Die gefundenen Daten werden zum Client zurückgesendet.  
Um alle Einträge, die in einem gewissen Zeitraum passiert sind zu lesen, stellt der Client seine Anfrage an den Master mit einen bestimmten BIC und IBAN und einem Datum von und gegebenenfalls ein Datum bis zudem alle Einträge zurückgeliefert werden sollen. Der Master sucht auch hier wieder anhand der Schlüssel der Buckets die entsprechenden Adressen heraus und sendet sie gemeinsam mit den nächsten weiteren Buckets an den Client. Dieser fragt wieder mit BIC, IBAN und den Daten das zuständige OSD an. Der Suchvorgang wird in Gang gesetzt, doch anstelle beim passenden Eintrag zu stoppen, werden alle Einträge, deren Schlüssel nach dem gegeben Datum erstellt wurde als Block zurückgeliefert. Die Daten werden entweder bis zum Ende des Buckets oder bis zum gegebenen Enddatum zum Client gestreamt.
Hat ein Bucket nicht alle benötigten Daten enthalten, kennt der Client über das Prefetching bereits die anliegenden Buckets und kann weitere Daten anfragen.

Ein DFS nach dem hier beschrieben Design ermöglicht das Ablegen sehr vieler kleiner Datenschnippsel, sofern sie in irgendeiner Form einen logischen Zusammenhang erfüllen. Außerdem ist das Auslesen der so zusammenhängenden Daten sehr performant und kann häufig mit nur einem Festplattenzugriff realisiert werden. Ein OSD benötigt für einen 16 Megabyte großen Bucket gerade einmal 176 Byte Hauptspeicher. Diese kommen aus den elf Bytes pro Checkpoint und den 16 Einträgen pro Bucket zustande. Wenn die OSDs über 16 Gigabyte Hauptspeicher verfügen, können sie so über 90 Millionen Buckets verwalten, wobei jeder potentiell ein Konto komplett abbildet. Die Adresse der Buckets benötigt auf dem Master sogar nur 64 Byte Hauptspeicher. Wenn auch der Master über 16 Gigabyte Hauptspeicher verfügt, ermöglicht er den schnellen Zugriff auf 250 Millionen Buckets mit bis zu vier Billionen Buchungen. Das entspricht einem Festplattenspeicher von etwa vier Petabyte. Dieses DFS sollte in der Lage sein alle Buchungen die bei einer Bank eingehen effizient und schnell zu speichern und auszuliefern. Inkonsistenzen treten nur während der Dauer eines Leases auf, welches zunächst auf zehn Sekunden limitiert wurde. Ist Konsistenz dringend erforderlich, kann die Ausgabe von Leases gestoppt werden und nach zehn Sekunden tritt ein konsistenter Zustand ein. Generell wäre ein solches DFS für die Abbildung der Kontoarten die nach Kapitel \ref{konsistenzKonten} eine gelockerte Konsistenz ertragen könnten bereits völlig ausreichend. Problematisch ist jedoch, dass zum Feststellen des Kontostandes immer alle Buchungen eines Kontos gelesen und ausgewertet werden müssten. Dieses Problem und die noch zu schwache Konsistenz um alle Kontoarten zu realisieren lässt sich aber durch ein geschicktes Design der Anwendungsschicht des Buchungssystem lösen.

\section{Design des Buchungssystems}
Die Anwendungsschicht des Buchungssystems stellt eine Schnittstelle für Clients und Anwendungen, die das Buchungssystem nutzen zur Verfügung. Sie ist die einzige Möglichkeit um Daten in das DFS zu schreiben und Daten auszulesen. Daher ist die Anwendungsschicht des Buchungssystems für die Validierung von eingehenden Buchungen, sowie der Bereitstellung von erweiterten Datenmanipulationen zuständig. Für Termineinlagen und Ratenkredite, würde es ausreichen, wenn Anfragen an das Buchungssystem direkt vom DFS beantwortet werden. Die nur wenigen Kontobewegungen bei diesen Kontoarten erzeugen eine sehr geringe Menge an Daten und Konsistenz kann bei Auszahlung oder Ende des Kredites leicht sicher gestellt werden. Konten bei denen beliebig viele und zu beliebigen Zeitpunkten Buchungen eingehen, haben aber mit der Inkonsistenz des DFS noch ein Problem. Generell ist es dabei egal ob zum Zeitpunkt zu dem eine Buchung eingeht bereits alle anderen Buchungen korrekt ausgelesen werden können. Es ist nur relevant, dass der Kontostand bereits alle vorangegangenen Buchungen berücksichtigt. Die einzige Information die für Girokonto und Co. also immer Konsistent sein muss, ist der tatsächliche Kontostand. Dieser lässt sich mit einem einzigen Eintrag pro Konto effizient erfassen. Bei jeder Buchung die dem Konto Zahlungsmittel zu- oder abführt, wird dieser Eintrag entsprechend angepasst. Um die benötigte Konsistenz zu gewährleisten, eignet sich für die Verwaltung der Kontostände ein ACID basierte relationale Datenbank. Diese Datenbank hält nur eine einzige Tabelle die nur so viele Einträge hat, wie Konten im Buchungssystem. Diese Datenmenge ist für ein relationales Datenbanksystem leicht zu bewältigen. Ein Eintrag in dieser Tabelle besteht aus BIC, IBAN, dem aktuellen Kontostand und dem Datum der letzten schreibenden Buchung. Bei jeder neuen Buchung wird der Betrag und das Datum für das Konto entsprechend angepasst. Erst danach wird die Buchung in das DFS abgelegt. Schlägt der erste oder der zweite Schritt fehl, muss sichergestellt werden, dass weder im DFS noch in der Kontostanddatenbank die Buchung vermerkt wird und der Client einen Fehler vom Buchungssystem erhält. Die zusätzliche Kontostandsdatenbank ermöglicht es festzustellen ob ein Konto über die benötigten Zahlungsmittel für eine Buchung verfügt. Außerdem kann über das Datum ermittelt werden, ob bereits alle Buchungen korrekt geschrieben wurden oder noch Buchungen verarbeitet werden. Um den Kontostand auszulesen, müssen auch nicht mehr alle Buchungen verrechnet werden, sondern es genügt ein Zugriff auf die Kontostands Datenbank.

Kombiniert man diese Idee eines Buchungssystem mit dem in Kapitel \ref{oneOnlyDFS} entwickelten DFS so läuft das Schreiben einer Buchung nun wie folgt ab. Die Buchung wird über eine Schnittstelle an das Buchungssystem geliefert. Dieses führt über die Kontostanddatenbank einige Validierungen aus. Es wird geprüft ob das Konto existiert und ob es über die nötigen Zahlungsmittel verfügt um die Buchung zu erfüllen. Ist dem so, wird der Eintrag in der Kontostandsdatenbank entsprechend angepasst und die Buchung weiter an das DFS gegeben. Dieses führt ebenfalls die Schritte zum Abspeichern der Buchung durch. Erst wenn die Buchung sowohl von der Kontostands Datenbank als auch vom DFS korrekt erfasst wurden, wird dem Client der Erfolg der Operation übermittelt.

Beim Lesen von Buchungen für ein Konto wird zunächst geprüft ob auch der Kontostand übertragen werden soll. Wenn dem so ist, wird er aus der Kontostandsdatenbank ausgelesen. Darauf hin wird die Anfrage an das DFS weitergeleitet und dessen Antwort mit dem Kontostand kombiniert und zum Client ausgeliefert. Zusätzlich kann der Client informiert werden, ob aktuell noch Buchungen ausstehen, die sich noch in der Verarbeitung befinden.

\section{Skalierbarkeit}
Der Sinn dieses Konzepts für ein DFS und des darauf aufsetzenden Buchungssystem, war die Erfüllung einer besonders einfachen und beliebig großen Skalierbarkeit. Besonders Lesezugriffe sollen immer schnell beantwortet werden können. Da der Fokus vor allem auf diesen Punkten lag, hat das Konzept nicht den Anspruch in allen Aspekten vollständig zu sein. 

Das hier entwickelte Buchungssystem und das dazugehörige DFS lässt sich auf mehrere Arten skalieren. Die Anwendungsschicht ist zunächst einmal komplett unabhängig vom DFS und der Kontostanddatenbank. Demnach lassen sich beliebig viele Instanzen davon parallel hochfahren. Je nach Anzahl der Anfragen, kann dies sogar lastbezogen geschehen. Wenn sehr viele Anfragen auf das Buchungssystem einprasseln, lassen sich mehrere Anwendungsschichten hochfahren, wenn die Anzahl der Anfragen wieder sinkt, können die zusätzlichen Systeme einfach wieder entfernt werden. Die Skalierung der Kontostandsdatenbank kann entweder über Replikation erfolgen, also mehrere Systeme oder über eine klare Aufteilung der Zuständigkeit auf mehrere Datenbanksysteme. So dass ein Datenbanksystem immer nur für einen bestimmten BIC Bereich zuständig ist und nur Anfragen beantwortet, die diesen Bereich betreffen. Das DFS lässt sich wie bereits beschrieben, einfach durch das Hinzufügen weiterer OSDs oder das Aufstocken des Hauptspeichers des Masters skalieren. Dabei ist zu erwähnen, dass die Skalierung des DFS, sowie die Skalierung der Anwendungsschicht der Buchungssysteme komplett unabhängig von einander statt finden kann. Die Anwendungsschicht braucht keine Anpassungen vorzunehmen um die gesteigerte Kapazität des DFS zu nutzen. Demnach wäre es auch denkbar das DFS auf Systeme von Drittanbietern auszulagern. Sollte die Möglichkeiten der Skalierung des DFS an ihre Grenzen stoßen, kann ähnlich wie bei der Kontostands Datenbank die Zuständigkeit für die Buchungen anhand des BICs auf mehreren DFS aufgeteilt werden.

\chapter{Implementierung}
Ein DFS, so wie es im Kapitel \ref{oneOnlyDFS} beschrieben wurde, existiert aktuell nicht. Da die Entwicklung eines DFS aber äußert komplex und zeitaufwendig ist, wird für die beispielhafte Implementierung auf ein bereits bestehendes DFS, das am besten auf die erarbeiteten Anforderungen passt, zurückgegriffen. Hauptkriterium ist dabei das Speichern und Lesen vieler kleiner Dateien. Von den beiden betrachteten DFS bietet sich dafür Haystack an. Leider gibt es jedoch von Haystack keine öffentlich zugängliche Version. Das Open Source Projekt SeaweedFS wurde aber auf Basis des Haystack Artikels entwickelt und weist dementsprechend eine große Ähnlichkeit in Architektur und Leistung zu seinem Vorbild auf. Bevor in diesem Kapitel näher auf die Anwendung und Details von SeaweedFS eingegangen wird, soll die Programmiersprache Go kurz angerissen werden. In ihr wurde sowohl SeaweedFS, die extra für diese Arbeit angefertigte Client-Bibliothek für SeaweedFS und alle anderen Systemkomponenten implementiert. Auch der Aufbau des entwickelten Systems und der Ablauf beim Lesen und Schreiben einer Buchung wird in diesem Kapitel dargelegt. Bei der hier besprochenen Implementierung handelt es sich mehr um einen Proof of Concept als um eine allumfassende Lösung zur Umstrukturierung von Buchungssystemen. Der Schwerpunkt lag auf Skalierung und Ausfallsicherheit. Der Sicherheit der Daten und der Entwicklung einer Nutzerauthentifizierung wurden im Rahmen dieser Implementierung keine Aufmerksamkeit geschenkt.

\section{Golang}
Go ist eine von Robert Griesemer, Rob Pike und Ken Thompson bei Google entwickelte Programmiersprache. 2009 wurde der Quellcode Open Source. Der Grund, wieso überhaupt eine neue Sprache entwickelt wurde ist im offiziellen FAQ von Go beschrieben \cite{gofaq}. Demnach waren die Autoren frustriert über die bestehenden Sprachen zur Systemprogrammierung, bei denen immer die Entscheidung zwischen Leichtigkeit der Programmierung, schneller Kompilierung und schneller Ausführung fallen musste. Go soll das leichtgewichtige Programmieren, das von interpretierten dynamisch typisierten Programmiersprachen bekannt ist, mit den Vorteilen einer statischen Typisierung kombinieren und dabei auch große Programme schnell kompilieren und ausführen. Zusätzlich bringt Go schon von Haus aus viele Werkzeuge mit, die die Entwicklung erleichtern sollen. Neben dem Kompilieren der Programme für unterschiedliche Betriebssysteme, können Unit Tests sowie Benchmark Tests nur mit der Standard Runtime von Go entwickelt werden. Zusätzlich liefert Go eine Art Linter, eine vordefinierte Quellcodeformatierung, sowie eine automatisch generierte Projektdokumentation.

Bei Go handelt es sich um eine prozedurale Programmiersprache, die sich in ihrem Syntax stark an C orientiert. Der Code für Hello World in Go ist in Abbildung \ref{lst:helloworld} zu sehen.
\begin{lstlisting}[label=lst:helloworld,
           language=Java,
           firstnumber=1,
           caption=Hello World mit Go.]           
package main

import "fmt"

func main() {
  fmt.Println("Hello World")
}

\end{lstlisting}

Besonders aber eignet sich Go zur Entwicklung von Webservern \cite[3-22]{gowebprogramming}. Der Hauptgrund dafür dürfte der geschickte Umgang mit Nebenläufigkeit sein. Anstelle von Threads wird Parallelität in Go durch sogenannte Goroutines bewerkstelligt. Diese werden im Gegensatz zu Threads nicht vom Betriebssystem verwaltet sondern von der Go Runtime. Das ermöglicht ein schnelles Erstellen und Zerstören von Goroutines. Außerdem wird für das Anlegen einer Goroutine weniger Hauptspeicher als für einen Thread benötigt \cite{goroutines}. Um eine Goroutine zu starten, muss lediglich das Schlüsselwort \textit{go} vor den Funktionsaufruf geschrieben werden. Der in Go standardmäßig  enthaltene Webserver ist dadurch sehr leichtgewichtig und simpel anzuwenden. Ein Beispiel für einen einfachen Webserver und eine Goroutine ist in Listing \ref{lst:webserver} zu sehen. Nach einer im Jahr 2016 durchgeführten Umfrage nutzen 63 \% der Go Programmierer die Sprache zur Webentwicklung \cite{gosurvey}.

Für die Implementierung eines Buchungssystems fiel die Wahl auf Go zum einen, weil das verwendete DFS, Seaweedfs, in Go realisiert wurde, und zum anderen, weil die einfache parallele Programmierung mit bereits geringen Aufwand eine Leistungssteigerung der Systeme ermöglicht.

\begin{minipage}{\linewidth}
\begin{lstlisting}[label=lst:webserver,
           language=Java,
           firstnumber=1,
           caption=Einfacher Webserver in Go und Beispiel einer Goroutine.]           
package main

import (
  "fmt"
  "net/http"
)

// StartAsGoroutine is a simple example for a function that can be 
// called within a Goroutine
func StartAsGoroutine() {
  fmt.Println("This will be called asynchronous!")
}

func main() {
  http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
    go StartAsGoroutine()
    fmt.Fprintf(w, "Hi there it's me!")
  })

  http.ListenAndServe(":8080", nil)
}
\end{lstlisting}
\end{minipage}

\section{SeaweedFS}
SeaweedFS ist eine unter der Apache Lizenz veröffentlichte Open Source Implementierung eines DFS auf Basis des 2010 veröffentlichten Haystack Artikels \cite{seaweedfsRepo}. Die internen Schritte zum Schreiben und Lesen von Dateien laufen also wie beim Vorbild ab. Über eine HTTP-Schnittstelle lassen sich beim Master Schlüssel und Adresse der Store Machine zum Speichern von Dateien anfragen und dann direkt auf der Store Machine ablegen. Diese beiden Schritte werden in Listing \ref{lst:seaweedfs} mit Hilfe des Kommandozeilen-Programms \textit{curl} dargestellt. Das Directory ist hierbei unter der Adresse \url{http://localhost:9333} erreichbar und die Store Machine unter \url{http://localhost:8080}.

Mit den Schnittstellen, die SeaweedFS auf Grundlage von Haystack zur Verfügung stellt, lassen sich aber nicht alle Teile des in Kapitel \ref{concept} vorgestellten Konzepts realisieren. So können zum Beispiel keine eigenen Schlüssel in der Form BIC, IBAN gefolgt von einem Zeitstempel erzeugt werden. Außerdem gibt es keine Möglichkeit nach Dateien zu suchen. Das Lesen einer Datei benötigt immer die Kenntnis über deren Schlüssel. Auch die Gruppierung von logisch zusammenhängenden Daten ist so nicht möglich. Der Client kann nicht bestimmen in welchem Volume und in welcher Reihenfolge innerhalb des Volumes die Daten landen.

\begin{minipage}{\linewidth}
\begin{lstlisting}[label=lst:seaweedfs,
           language={},
           emphstyle=\color{black},
           extendedchars=true,
           literate={ü}{{\"u}}1,
           keywordstyle=\color{black},
           stringstyle=\color{black},
           firstnumber=1,
           caption=Schritte zum Ablegen einer Datei in SeaweedFS.]           
# Anfragen eines Schlüssels
curl http://localhost:9333/dir/assign
{"count":1,"fid":"3,01637037d6","url":"127.0.0.1:8080",
 "publicUrl":"localhost:8080"}

# Abspeichern der Datei mit dem gegebenen Schlüssel
curl -F file=@testData.txt http://localhost:8080/3,01637037d6
{"size": 43234}
\end{lstlisting}
\end{minipage}

Die ersten beiden Probleme können aber über eine Besonderheit von SeaweedFS gelöst werden. Neben dem Directory und den Store Machines verfügt SeaweedFS auch über einen sogenannten Filer. Über diesen können Dateien in Pfaden und Ordnern abgelegt werden. Da SeaweedFS ganz nach dem Vorbild von Haystack entwickelt wurde, werden diese Pfade und Ordner hocheffizient im Hauptspeicher des Filers gehalten. Stellt man beim Filer eine Anfrage nach einer Datei innerhalb eines Ordners, müssen drei Schritte unternommen werden. Zunächst muss die Ordner-ID für den der Datei übergeordneten Ordner gefunden werden. Gemeinsam mit der ermittelten Ordner-ID und dem Namen der Datei kann dann der Dateischlüssel gefunden werden. Zuletzt wird über den Dateischlüssel der entsprechende Datenblock aus den Volumes gesucht. Der letzte Schritt fällt bereits in den Aufgabenbereich des Directories. Der Filer ist demnach nur für die Zuordnung von Ordner zu Ordner-ID und Dateiname zu Dateischlüssel zuständig. Während die Ordner und Pfade im Hauptspeicher des Filers liegen, wird die Zuordnung von übergeordneter Ordner und Dateiname zu Dateischlüssel in einem Key-Value-Store gespeichert. In der Standardkonfiguration von SeaweedFS ist dieser eine LevelDB. Wenn der Dateischlüssel ermittelt wurde, leitet der Filer die Anfrage an das entsprechende Volume weiter. Dieses Antwortet dann dem Client mit den Daten.
Über den Filer kann jetzt ein eigener Schlüssel in Form von Ordner und Dateiname vergeben werden. Der erste Teil des Schlüssels also BIC und IBAN wird dabei in den Pfad kodiert, der stets aufsteigende  zweite Teil des Schlüssels, der Zeitstempel, in den Dateinamen. Der Schlüssel für eine Buchung hat demnach die Form \textit{[BIC/IBAN/Timestamp]}. Über den Filer kann auch nach Dateien innerhalb eines Ordners gesucht werden. Gibt man bei einer Anfrage einen \textit{lastFileName} über einen Query-Parameter mit, werden alle Dateien vom Filer zurückgeliefert, die in alphabetischer Reihenfolge nach dem \textit{lastFileName} liegen. Ein Beispiel zum Anlegen, Auslesen und dem Filtern nach Dateien ist in Listing \ref{lst:filer} zu sehen.

In der Standardkonfiguration stellt der Filer jedoch einen Single Point of Failure da. Durch den Einsatz von Redis oder Apache Casandra anstelle der LevelDB als Persistenzschicht für den Filer, kann ein Verteilter Filer realisiert werden. Alternativ könnten auch mehrere Filer gestartet werden, die jeweils einen anderen Namensraum verwalten. Für den Rahmen dieser Arbeit reicht die Leistung eines einzigen Filers jedoch aus.

\begin{minipage}{\linewidth}
\begin{lstlisting}[label=lst:filer,
           language={},
           emphstyle=\color{black},
           extendedchars=true,
           literate={ü}{{\"u}}1,
           keywordstyle=\color{black},
           stringstyle=\color{black},
           firstnumber=1,
           caption=Schritte zum Ablegen einer Datei in SeaweedFS.]           
# Ablegen einer Datei über den Filer
curl -F "file=@Buchung" "http://localhost:8888/book/BIC/IBAN/Timestamp"

# Anfragen einer Datei über den Filer
curl "http://localhost:8888/book/BIC/IBAN/Timestamp"
{"Daten":1,"MehrDaten":2}

# Filtern aller Dateien in einem Ordner über den Dateinamen
curl "http://localhost:8888/book/BIC/IBAN/?lastFileName=Timestamp"
{
  "Path": "/BIC/IBAN/",
  "Files": [
    {
      "name": "Timestamp1",
      "fid": "6,0625473512"
    },
    {
      "name": "Timestamp2",
      "fid": "6,106f19db84"
    }
  ],
  "Directories": null,
  "Limit": 100,
  "LastFileName": "Timestamp2",
  "ShouldDisplayLoadMore": false
}
\end{lstlisting}
\end{minipage}

Für die Kommunikation mit SeaweedFS wurde extra eine eigene Client-Bibliothek entwickelt. Diese trägt den Namen \textit{Weedharvester} und ist unter der MIT-Lizenz auf GitHub öffentlich zugänglich. Weedharvester abstrahiert den Zugriff auf SeaweedFS und kann sowohl direkt über das Directory, als auch über den Filer Anfragen an SeaweedFS senden. Alle wichtigen Operationen wie das Hochladen, Anfragen, Filtern und Löschen von Dateien werden unterstützt. Dabei ist das Hochladen nicht auf Dateien beschränkt. Mit Weedharvester kann jede beliebige Bytefolge in SeaweedFS abgelegt werden. Ein Beispiel für die Nutzung des Standard-Clients und des Filer-Clients ist in Listing \ref{lst:weedharvester} dargestellt.

\begin{minipage}{\linewidth}
\begin{lstlisting}[label=lst:weedharvester,
           language={},
           emphstyle=\color{black},
           extendedchars=true,
           literate={ü}{{\"u}}1,
           keywordstyle=\color{black},
           stringstyle=\color{black},
           firstnumber=1,
           caption=Beispiele zur Verwendung von Weedharvester.]          
# Client
client := NewClient("http://masterurl.com")

fid, err := client.Create(reader)
reader := client.Read(fid)

# Filer
filer := NewFiler("http://filerurl.com")

err := filer.Create(reader, filename, path)
reader := filer.Read(filename, path)
\end{lstlisting}
\end{minipage}

Die Datenreplikation kann bei SeaweedFS entweder beim Anlegen jeder Datei separat mitgegeben werden, oder, wie hier der Fall, beim Starten des Directories allgemeingültig festgelegt werden. Über ein Schema mit drei Ziffern kann bestimmt werden, wie viele Replikas auf unterschiedlichen Datenzentren, unterschiedlichen Racks und gleichen Datenzentren oder dem gleichen Rack und unterschiedlichen Servern angelegt werden sollen. Die Kennung für Datenzentrum und Rack wird den Store Machines beim Start mitgegeben.

Nicht nur die Fähigkeit relativ kleine Dateien zu verwalten, macht SeaweedFS zu einer guten Wahl für die beispielhafte Implementierung eines Buchungssystems sondern auch die einfache Nutzung. Das Starten und Konfigurieren ist in wenigen Schritten getan. Außerdem hat der Autor auch ein Docker Image für SeaweedFS veröffentlicht, mit dem die Konfiguration und Verwaltung noch einfacher ist. Ein Beispiel für eine docker-compose Datei, die ein Directory, einen Filer und einen Store Machine startet und entsprechend konfiguriert, ist in Listing \ref{lst:docker} zu sehen. Auf diese Weise wurde SeaweedFS auch in der folgenden Implementierung eingesetzt.

\begin{lstlisting}[label=lst:docker,
           language={},
           emphstyle=\color{black},
           extendedchars=true,
           literate={ü}{{\"u}}1,
           keywordstyle=\color{black},
           stringstyle=\color{black},
           firstnumber=1,
           caption=docker-compose Datei zum Starten eines Filers{,} eines Directories (master) und einer Store Machine (volume) in SeaweedFS.]          
version: '2.1'
services:
  seaweed:
    container_name: seaweedf
    image: chrislusf/seaweedfs
    ports:
      - 8888:8888
      - 9333:9333
      - 8080:8080
    command: 'server -master.port=9333 -volume.port=8080 -filer=true -volume.publicUrl http://docker:8080'
\end{lstlisting}

\section{Aufbau der Implementierung}
Das komplett implementierte System ist auf Github unter der MIT-Lizenz öffentlich zugänglich. Die Grundbausteine sind ein Backend, mehrere Account-Updater, und SeaweedFS als das zugrundeliegende DFS. Das Backend stellt dabei die Schnittstelle zum Client und somit die einzige Möglichkeit um mit SeaweedFS zu kommunizieren dar. Zu seinen Aufgaben gehört genauso das Anlegen und Auslesen von Konten, wie das persistente Speichern einer jeden eingehenden Buchung. Zusätzlich informiert das Backend den Account-Updater über das Eintreffen neuer Daten. Auch für das Lesen von Buchungen ist das Backend zuständig. Der Account-Updater hingegen ist für das Zusammenfassen mehrerer Buchungen in eine Datei verantwortlich und löst damit das Problem, das SeaweedFS alleine nicht bewältigen konnte. Über das Backend erfährt der Account-Updater, für welche Konten neue Buchungen vorliegen und verarbeitet diese. SeaweedFS speichert zum einen jede einzelne Buchung in einem Ordner \url{books} und zum anderen für jedes Konto eine Zusammenfassung von Buchungen im Ordner \url{accounts}. Alle Daten die das System über das Backend betreten oder verlassen, sind in JSON formatiert. Ein in JSON formatiertes Konto ist in Listing \ref{lst:accountJson} zu sehen. Jedes Konto ist durch \textit{bic} und \textit{iban} eindeutig identifizierbar und kann mit einem beliebigen Kontostand initiiert werden.

\begin{lstlisting}[label=lst:accountJson,
           language={},
           firstnumber=1,
           caption=Kontoobjekt in JSON-Format.]           
{
  "name": "RandomAccount",
  "bic": "BQLM8NINHQ4",
  "iban": "DE627131848139984060",
  "balance": 2481
}
\end{lstlisting}

Für jedes Konto können beliebig viele Buchungen abgespeichert werden. Eine einzelne JSON codierte Buchung wird in Listing \ref{lst:transactionJson} dargestellt. Eine Buchung ist eindeutig über ihr \textit{bookingDate} in Kombination mit \textit{bic} und \textit{iban} des \textit{recipients} also dem Empfänger der Buchung. Nur anhand der Daten einer Buchung kann nicht herausgefunden werden, ob sie durch Online-Banking, eine Lastschrift oder eine Überweisung entstanden ist. Lediglich die Beschreibung im Feld \textit{intendedUse} könnte darüber Auskunft geben. Bei jeder Bewegung von bargeldlosen Zahlungsmitteln werden immer zwei solcher Buchungen erstellt. Diese beiden Buchungen unterscheiden sich lediglich darin, dass der \textit{recipient} und der \textit{sender} getauscht und der Betrag in \textit{valueInSmallestUnit} das Komplementär zur ursprünglichen Buchung ist. Jede dieser beiden Buchungen werden dann für den entsprechenden Empfänger abgelegt. Dieses Vorgehen entspricht der zuvor erklärten doppelten Buchführung. Da die beiden Konten, die an einer Zahlungsmittelbewegung beteiligt sind, nicht immer bei der gleichen Bank liegen müssen, kümmert sich ein sogenannter Zahlungsdienstleister um die Verteilung der beiden Buchungen an die zuständigen Banken \cite{wiki:zahlungsdienstleister}. Das Buchungssystem selbst kann dadurch jede Buchung unabhängig voneinander verarbeiten.

\begin{lstlisting}[label=lst:transactionJson,
           language={},
           firstnumber=1,
           caption=Buchung in JSON-Format.]           
{
  "recipient": {
    "name": "RandomAccount",
    "bic": "BQLM8NINHQ4",
    "iban": "DE627131848139984060"
  },
  "sender": {
    "name": "TestUser",
    "bic": "TESTBIC",
    "iban": "TESTIBAN"
  },
  "bookingDate": "2017-03-20T15:30:45.027280618Z",
  "currency": "EUR",
  "valueInSmallestUnit": 1913,
  "intendedUse": "TestCreateTransaction"
}
\end{lstlisting}

Die grundlegende Architektur der Anwendung ist in Abbildung \ref{architectureImpl} dargestellt. Das Backend verfügt hierbei über alle Schnittstellen um Konten oder Buchungen anzulegen und auszulesen.  Das Senden einer POST-Anfrage mit einem JSON-formatierten Konto im Body-Teil an die Adresse \url{/accounts} erzeugt zum einen einen einen Eintrag in der Kontostandsdatenbank, hier eine PostgreSQL, mit den Attributen \textit{bic}, \textit{iban} und \textit{balance} als auch eine sogenannte \textit{AccountInfo} im DFS unter dem Verzeichnis \url{accounts/[BIC]/[IBAN]} mit dem Zeitpunkt der Erstellung als Dateinamen. Eine \textit{AccountInfo} ist vergleichbar mit dem im Konzept beschriebenen Bucket, nur das keine Index File zum schnellen Zugriff innerhalb der \textit{AccountInfo} angelegt werden kann. Der Pfad in Kombination mit dem Dateinamen entspricht hierbei wieder dem im Konzept vorgestellten Schlüssel. Sowohl das Speichern des Eintrages in der Kontodatenbank als auch das Erstellen der \textit{AccountInfo} im DFS muss gelingen, damit das Anlegen eines Kontos als erfolgreich gilt. Eine \textit{AccountInfo} wie sie im DFS abgelegt wird, ist in Listing \ref{lst:accountinfoJson} dargestellt. Neben den Informationen für ein Konto, speichert die \textit{AccountInfo} noch Buchungen. Über die Felder \textit{oldestTransaction} und \textit{latestTransaction} ist auch das Datum der ältesten und der neusten Buchung innerhalb der \textit{AccountInfo} einfach zugänglich. Das Feld \textit{predeccessor} gibt den Namen der vorangegangenen \textit{AccountInfo} an.

\begin{lstlisting}[label=lst:accountinfoJson,
           language={},
           firstnumber=1,
           caption=AccountInfo in JSON-Format.]           
{
  "name": "RandomAccount",
  "bic": "2B9PX8YTFLX",
  "iban": "DE440007387504066832",
  "balance": 8542,
  "predeccessor": "2017-03-20T15:30:42.027280124Z",
  "oldestTransaction": "2017-03-20T15:30:45.027280618Z",
  "latestTransaction": "2017-03-20T15:30:45.027280618Z",
  "transactions": [
    {
      "receipient": {
        "name": "RandomAccount8",
        "bic": "2B9PX8YTFLX",
        "iban": "DE440007387504066832"
      },
      "sender": {
        "name": "TestUser",
        "bic": "TESTBIC",
        "iban": "TESTIBAN"
      },
      "bookingDate": "2017-03-20T15:30:45.027280618Z",
      "currency": "EUR",
      "valueInSmallestUnit": 1913,
      "intendedUse": "TestCreateTransaction"
    }
}
\end{lstlisting}

Die \textit{AccountInfo} dient der Bündelung von mehreren Buchungen eines Kontos in eine größere Datei. Wie viele Buchungen in in einer \textit{AccountInfo} abgelegt werden, kann konfiguriert werden. Wurde ein Konto angelegt kann er direkt über eine GET-Anfrage an die Adresse \url{accounts/[BIC]/[IBAN]} beim Backend ausgelesen werden, oder eine Liste aller Konten über \url{/accounts}. Buchungen können nur für bereits bestehende Konten empfangen werden. Zum Speichern einer Buchung für ein Konto muss eine POST-Anfrage mit einer JSON-formatierten Buchung im Body-Teil an die Adresse \url{accounts/[BIC]/[IBAN]/transactions} gestellt werden. Jede eingehende Buchung wirkt sich zum Einen auf den entsprechenden Eintrag des Empfängers in der Kontostanddatenbank aus, zum anderen wird sie in SeaweedFS unter dem Pfad \url{books/[BIC]/[IBAN]} mit ihrem \textit{bookingDate} als Dateiname abgelegt. Als nächsten Schritt teilt das Backend einem Account-Updater mit, für welches Konto eine neue Buchung eingegangen ist. Hier endet der Aufgabenbereich des Backends.

\begin{figure}
  \centering
  \includegraphics[width=15cm]{img/6/architecture.png}
  \caption[Grundlegende Architektur des implementierten Systems]{ Grundlegende Architektur des implementierten Systems.}
  \label{architectureImpl}
\end{figure}

Die weiteren Schritte zum Verarbeiten der Buchung übernehmen die Account-Updater. Diese sind in einem Master und mehrere Slaves aufgeteilt. Welcher Account-Updater Master und welcher Slave ist, wird beim Start festgelegt. Während der Master keinerlei Kenntnis über mögliche Slaves beim hochfahren besitzt, wird Slaves die Adresse des Master-Account-Updaters mitgeteilt. Die Slaves melden sich dann selbstständig beim Master mit ihrer Adresse an. Für das Backend spielt es jedoch keine Rolle, welcher Account-Updater Master oder Slave ist. Jeder kann die Information, für welches Konto es neue Buchungen gibt empfangen und abspeichern. Dabei wird die BIC, der IBAN sowie das Erstellungsdatum der Buchung in einer Update-Datenbank, hier eine MongoDB, abgelegt, die sich alle Account-Updater teilen. Der Master ist jedoch als einziger berechtigt in regelmäßigen Zeitabschnitten allen Slaves den Auftrag zu erteilen neuen Buchungen zu verarbeiten. Dabei erhält jeder Slave seinen eigenen Satz an Kontonummern, deren Buchungen noch in eine \textit{AccountInfo} zusammengefasst werden müssen. Die Account-Updater beginnen dann, mit dem gegebenen BICs und IBANs die letzte \textit{AccountInfo} für jedes Konto auszulesen. Über das Feld \textit{latestTransaction} ist ein schneller Vergleich mit dem Erstellungsdatum der neuesten Buchung der Update-Datenbank möglich. 
Liegt dieses Datum nach dem Datum der letzten Buchung innerhalb der \textit{AccountInfo}, fordert der Account-Updater alle Buchungen die sich unter dem Pfad \url{books/[BIC]/[IBAN]/} befinden und nach dem Erstellungsdatum der neuesten Buchung innerhalb der \textit{AccountInfo} liegen, bei SeaweedFS an. Diese Buchungen werden an die bestehende neueste \textit{AccountInfo} angehängt und das Feld \textit{latestTransaction} entsprechend angepasst. Wird während dieses Vorgangs der Schwellenwert für die maximale Anzahl an Buchungen innerhalb einer \textit{AccountInfo} überschritten, wird die volle abgespeichert und das befüllen einer neuen begonnen. Auch diese wird wieder unter dem Pfad \url{accounts/[BIC]/[IBAN]/} mit dem Zeitpunkt der Erstellung der ältesten Buchung als Dateiname in SeaweedFS angelegt. Die Entscheidung das Gruppieren von ausstehenden Buchungen in regelmäßigen Abständen, anstelle nach jeder eingehenden Buchung durchzuführen hat einige Vorteile. Da SeaweedFS wie Haystack nicht in der Lage ist Daten an eine bestehende Datei anzuhängen, wird bei jedem Einfügen einer Buchung in eine bestehende \textit{AccountInfo} eine neue Needle im Volume angelegt und die ursprüngliche als ungültig erklärt. Das hat wiederum für jede eingehende Buchung ein Anpassen des Volume Index Files sowie der dazugehörigen Repräsentation im Hauptspeicher zur Folge. Fasst man Buchungen aber nur in gewissen Abständen zusammen, können manchmal mehrere Buchungen eines Kontos mit nur einem Update-Vorgang berücksichtigt werden. Außerdem muss so im besten Fall eine \textit{AccountInfo} für das Hinzufügen mehrere Buchungen nur ein einziges mal ausgelesen werden. Die Last die dadurch entsteht wird besser berechenbar und handhabbar.

Nach erfolgreicher Verarbeitung und Gruppierung der ausstehenden Buchungen in die \textit{AcountInfos}, wird jedoch der Eintrag in der Update-Datenbank und die einzelnen Buchungen im Pfad \url{books/[BIC]/[IBAN]/} noch nicht gelöscht. Dies geschieht erst, wenn bei einem zweiten Update-Durchgang festgestellt wird, dass das Datum in der Update-Datenbank das gleiche ist wie das Datum der letzten Buchung in der \textit{AccountInfo}. Dieses Vorgehen stellt gerade bei stark frequentierten Konten sicher, dass immer alle Buchungen berücksichtigt werden. Wenn zwei Account-Updater gleichzeitig Buchungen für das gleiche Konto verarbeiten, kann es zu einem Lost-Update-Problem kommen. Dieses wird in Abbildung \ref{lostUpdate} aufgezeigt. Wenn Account-Updater beginnt die Buchungen zu verarbeiten, berücksichtigt er dabei nur Buchung1 und Buchung2. Aus irgendwelchen Gründen dauert die Verarbeitung dieser beiden Buchungen so lange, dass in einer zweiten Runde ein zweiter Account-Updater beginnt das Update durchzuführen. Dieser berücksichtigt nun Buchung1, Buchung2 und Buchung3. Account-Updater1 schließt seine Aufgabe nun erst nach Account-Updater2 ab und überschreibt die \textit{AccountInfo}. Würden nun einzelnen Buchungen sowie der Eintrag in der Update-Datenbank gelöscht werden, so wäre Buchung3 nicht berücksichtigt worden. Durch das erzwingen des gleichen Datums in der neuesten \textit{AccountInfo} sowie im Eintrag der Update-Datenbank, kann dieses Problem nicht auftreten.
\begin{figure}
  \centering
  \includegraphics[width=15cm]{img/6/race.png}
  \caption[Lost-Update-Problem beim Zusammenfassen der Buchungen]{ Lost-Update-Problem beim Zusammenfassen der Buchungen.}
  \label{lostUpdate}
\end{figure}
Kann der Master einen Slave während der Verteilung der Update-Vorgänge nicht erreichen, entfernt der Master diesen aus seiner Liste. Auch regelmäßige Heartbeat-Nachrichten der Slaves an den Master stellen sicher, dass dieser immer über eine aktuelle Liste der Slaves verfügt.

Sind alle Buchungen in \textit{AccountInfos} zusammengefasst, wurde ein Zustand hergestellt, der dem im Konzept besprochenen System sehr nahe ist. Mehrere Buchungen eines Kontos wurden in \textit{AccountInfos} zusammengefasst, die über BIC und IBAN und einem Zeitstempel erreichbar sind. Auch das Suchen der \textit{AccountInfos}, die nach einem bestimmten Zeitpunkt angelegt wurden ist über den SeaweedFS Filer und der Angabe eines \textit{lastFileName} möglich. Jedoch gibt es keine Datei, welche den schnellen Zugriff innerhalb einer \textit{AccountInfo} ermöglicht. Wann immer eine oder mehrere Buchungen gelesen werden sollen, muss immer die ganze \textit{AccountInfo} verarbeitet werden. Daher ist die Auswahl der maximalen Anzahl an Buchungen in einer \textit{AccountInfo} wichtig. Ist sie zu groß, muss für jede Anfrage ein sehr großer Datenblock gelesen werden, ist sie zu klein, müssen möglicherweise viele \textit{AccountInfos} gelesen werden um mehrere Buchungen zu erhalten. In der hier besprochenen Implementierung liegen 50 Buchungen in einer \textit{AccountInfo}. Das Lesen aller Buchungen, die in einem Zeitraum getätigt wurden ist über die Schnittstelle \url{account/BIC/IBAN/transactions?from=2017-02-16_13:05:00} des Backends möglich. Es kann auch ein Query-Parameter mitgegeben werden, der das Ende des gesuchten Zeitraums angibt. Mit dem gegebenen Startdatum wird der Filer nach allen \textit{AccountInfos} die danach erstellt wurden befragt. Es kann vorkommen, dass obwohl Buchungen für den angegebenen Zeitraum vorliegen der Filer keine Daten zurückliefert. Das ist immer dann der Fall, wenn der Dateiname der neuesten \textit{AccountInfo} einen Zeitstempel beinhaltet, der älter als das angefragte Datum ist. In diesem Fall muss explizit nach der letzten \textit{AccountInfo} für das Konto gefragt und nach Buchungen im gewünschten Zeitraum durchsucht werden. Liefert der Filer Daten zu mehreren \textit{AccountInfos}, die Buchungen nach dem gewünschten Datum speichern, so werden diese alle nacheinander geladen und zu einer großen \textit{AccountInfo} zusammengeführt. Das geschieht so lange, bis alle \textit{AccountInfos} berücksichtigt wurden, oder das angegebene Enddatum erreicht wurde. Als letzter Schritt wird dann noch die \textit{AccountInfo} angefordert, die der ältesten berücksichtigten vorausging. Über das Feld \textit{predeccessor} ist dies leicht möglich. Auch diese wird noch mit zu einer großen \textit{AccountInfo} hinzugefügt. Die so kombinierten Daten, werden dann dem Client ausgeliefert. 
Durch die Verzögerung beim Verarbeiten der ausstehenden Buchungen, kann es vorkommen, dass neue Buchungen bei einer Anfrage der \textit{AccountInfo} noch nicht berücksichtigt werden. Über die Kontostandsdatenbank ist jedoch zu jedem Zeitpunkt das Auslesen des korrekten Kontostands möglich.

Zum Abschluss des Kapitels soll noch ein letztes mal der Schreib- und Lesevorgang einer oder mehrerer Buchungen zusammengefasst werden. Nachdem die Buchung das Backend erreicht hat, wir ihr Betrag zunächst in der Kontostandsdatenbank vermerkt, bevor sie in SeaweedFS abgelegt wird. Nachdem einem Account-Updater das Vorhandensein neuer Buchungen mitgeteilt wurde, wird der Auftrag diese zu Verarbeiten gegeben. Dabei werden alle Buchungen eines Kontos in einer \textit{AccountInfo} zusammengefasst.

Möchte der Client nun Buchungen für einen bestimmten Zeitraum lesen, stellt er diese Anfrage an die entsprechende Schnittstelle des Backends. Dieses sammelt über den Filer von SeaweedFS alle \textit{AccountInfos} zusammen, die möglicherweise Buchungen für den gewünschten Zeitraum halten und fasst diese in eine große \textit{AccountInfo} zusammen. Diese wird dem Client als Antwort zurückgeliefert.

\section{Schnittstelle zu SeaweedFS}
Wieso wurde eine neue Schnittstelle geschrieben?
Worauf war zu achten? Nutzung des Filers (Distributed Filer)
\section{Bibliothek zur Abbildung von Buchungen}
Einführen der fehlenden Abstraktionsschicht für die spätere Anwendung
\section{RESTful Webservice}
Implementierung einer API zum leichten Anlegen und Lesen von Buchungen. Besonderes Augenmerk auf Modularisierung.

\chapter{Evaluierung}
Mal sehen was hier später steht.

\chapter{Ausblick}
Ich bin sehr gespannt.

\begin{lstlisting}[label=lst:java,
				   language=java,
           keywordstyle=\color{black},
				   firstnumber=1,
				   caption=Beispiel für einen Quelltext]				   

public void foo() {				   
	// Kommentar
}
\end{lstlisting}

\chapter{Zusammenfassung}


\backmatter
%%%%%%%%%%%%%%%%%%%
%% create figure list
%%%%%%%%%%%%%%%%%%%

\listoffigures
\addcontentsline{toc}{chapter}{Verzeichnisse}			

%%%%%%%%%%%%%%%%%%%
%% create tables list
%%%%%%%%%%%%%%%%%%%
%\listoftables

%%%%%%%%%%%%%%%%%%%
%% create listings list
%%%%%%%%%%%%%%%%%%%
\lstlistoflistings
\addcontentsline{toc}{chapter}{Listings}				

\printbibliography
\addcontentsline{toc}{chapter}{Literatur}		

%%%%%%%%%%%%%%%%%%%
%% declaration on oath
%%%%%%%%%%%%%%%%%%%

\addchap{Eidesstattliche Erklärung}

Hiermit versichere ich, dass ich die vorgelegte Bachelorarbeit selbstständig verfasst und noch nicht anderweitig zu Prüfungszwecken vorgelegt habe. Alle benutzten Quellen und Hilfsmittel sind angegeben, wörtliche und sinngemäße Zitate wurden als solche gekennzeichnet.

\vspace{20pt}
\begin{flushright}
$\overline{~~~~~~~~~~~~~~~~~\mbox{\BaAuthor, am \today}~~~~~~~~~~~~~~~~~}$
\end{flushright}
\end{document}

